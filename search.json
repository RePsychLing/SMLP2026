[
  {
    "objectID": "useful_packages.html",
    "href": "useful_packages.html",
    "title": "Useful packages",
    "section": "",
    "text": "Unlike R, Julia does not immediately expose a huge number of functions, but instead requires loading packages (whether from the standard library or from the broader package ecosystem) for a lot of relevant functionality for statistical analysis. There are technical reasons for this, such as the ease of using the Julia package system. One further motivation is that Julia is aimed at a broader ‚Äútechnical computing‚Äù audience (like MATLAB or perhaps Python) and less at a ‚Äústatistical analysis‚Äù audience.\nThis has two important implications:\nThis notebook is not intended to be an exhaustive list of packages, but rather to highlight a few packages that I suspect will be particularly useful. Before getting onto the packages, I have one final hint: take advantage of how easy and first-class package management in Julia is. Having good package management makes reproducible analyses much easier and avoids breaking old analyses when you start a new one. The package-manager REPL mode (activated by typing ] at the julia&gt; prompt) is very useful.",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "useful_packages.html#reading-data",
    "href": "useful_packages.html#reading-data",
    "title": "Useful packages",
    "section": "1.1 Reading data",
    "text": "1.1 Reading data\n\nArrow.jl a high performance format for data storage, accessible in R via the arrow package and in Python via pyarrow. (Confusingly, the function for reading and writing Arrow format files in R is called read_feather and write_feather, but the modern Arrow format is distinct from the older Feather format provided by the feather package.) This is the format that we store the example and test datasets in for MixedModels.jl.\nCSV.jl useful for reading comma-separated values, tab-separated values and basically everything handled by the read.csv and read.table family of functions in R.\n\nNote that by default both Arrow.jl and CSV.jl do not return a DataFrame, but rather ‚Äúcolumn tables‚Äù ‚Äì named tuples of column vectors.",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "useful_packages.html#dataframes",
    "href": "useful_packages.html#dataframes",
    "title": "Useful packages",
    "section": "1.2 DataFrames",
    "text": "1.2 DataFrames\nUnlike in R, DataFrames are not part of the base language, nor the standard library.\nDataFrames.jl provides the basic infrastructure around DataFrames, as well as its own mini language for doing the split-apply-combine approach that underlies R‚Äôs dplyr and much of the tidyverse. The DataFrames.jl documentation is the place for looking at how to e.g.¬†read in a CSV or Arrow file as a DataFrame. Note that DataFrames.jl by default depends on CategoricalArrays.jl to handle the equivalent of factor in the R world, but there is an alternative package for factor-like array type in Julia, PooledArrays.jl. PooledArrays are simpler, but more limited than CategoricalArrays and we (Phillip and Doug) sometimes use them in our examples and simulations. The tables produced by reading an Arrow file have their own representation of factor-like data as DictEncoded arrays.\nDataFrame.jl‚Äôs mini language can be a bit daunting, if you‚Äôre used to manipulations in the style of base R or the tidyverse. For that, there are several options; recently, we‚Äôve had particularly nice experiences with DataFrameMacros.jl and Chain.jl for a convenient syntax to connect or ‚Äúpipe‚Äù together successive operations. It‚Äôs your choice whether and which of these add-ons you want to use! Phillip tends to write his code using raw DataFrames.jl, but Doug really enjoys DataFrameMacros.jl.\nThe recently added Tidier collection of Julia packages is popular with those coming from the tidyverse.",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "useful_packages.html#regression",
    "href": "useful_packages.html#regression",
    "title": "Useful packages",
    "section": "1.3 Regression",
    "text": "1.3 Regression\nUnlike in R, neither formula processing nor basic regression are part of the base language or the standard library.\nThe formula syntax and basic contrast-coding schemes in Julia is provided by StatsModels.jl. By default, MixedModels.jl re-exports the @formula macro and most commonly used contrast schemes from StatsModels.jl, so you often don‚Äôt have to worry about loading StatsModels.jl directly. The same is true for GLM.jl, which provides basic linear and generalized linear models, such as ordinary least squares (OLS) regression and logistic regression, i.e.¬†the classical, non mixed regression models.\nThe basic functionality looks quite similar to R, e.g.\njulia &gt; lm(@formula(y ~ 1 + x), data)\njulia &gt; glm(@formula(y ~ 1 + x), data, Binomial(), LogitLink())\nbut the more general modelling API (also used by MixedModels.jl) is also supported:\njulia &gt; fit(LinearModel, @formula(y ~ 1 + x), mydata)\njulia &gt; fit(\n  GeneralizedLinearModel,\n  @formula(y ~ 1 + x),\n  data,\n  Binomial(),\n  LogitLink(),\n)\n(You can also specify your model matrices directly and skip the formula interface, but we don‚Äôt recommend this as it‚Äôs easy to mess up in really subtle but very problematic ways.)",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "useful_packages.html#formula-macros-and-domain-specific-languages",
    "href": "useful_packages.html#formula-macros-and-domain-specific-languages",
    "title": "Useful packages",
    "section": "1.4 @formula, macros and domain-specific languages",
    "text": "1.4 @formula, macros and domain-specific languages\nAs a sidebar: why is @formula a macro and not a normal function? Well, that‚Äôs because formulas are essentially their own domain-specific language (a variant of Wilkinson-Rogers notation) and macros are used for manipulating the language itself ‚Äì or in this case, handling an entirely new, embedded language! This is also why macros are used by packages like Turing.jl and Soss.jl that define a language for Bayesian probabilistic programming like PyMC3 or Stan.",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "useful_packages.html#extensions-to-the-formula-syntax",
    "href": "useful_packages.html#extensions-to-the-formula-syntax",
    "title": "Useful packages",
    "section": "1.5 Extensions to the formula syntax",
    "text": "1.5 Extensions to the formula syntax\nThere are several ongoing efforts to extend the formula syntax to include some of the ‚Äúextras‚Äù available in R, e.g.¬†RegressionFormulae.jl to use the caret (^) notation to limit interactions to a certain order ((a+b+c)^2 generates a + b + c + a&b + a&c + b&c, but not a&b&c). Note also that Julia uses & to express interactions, not : like in R.",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "useful_packages.html#standardizing-predictors",
    "href": "useful_packages.html#standardizing-predictors",
    "title": "Useful packages",
    "section": "1.6 Standardizing Predictors",
    "text": "1.6 Standardizing Predictors\nAlthough function calls such as log can be used within Julia formulae, they must act on a rowwise basis, i.e.¬†on observations. Transformations such as z-scoring or centering (often done with scale in R) require knowledge of the entire column. StandardizedPredictors.jl provides functions for centering, scaling, and z-scoring within the formula. These are treated as pseudo-contrasts and computed on demand, meaning that predict and effects (see next) computations will handle these transformations on new data (e.g.¬†centering new data around the mean computed during fitting the original data) correctly and automatically.",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "useful_packages.html#effects",
    "href": "useful_packages.html#effects",
    "title": "Useful packages",
    "section": "1.7 Effects",
    "text": "1.7 Effects\nJohn Fox‚Äôs effects package in R (and the related ggeffects package for plotting these using ggplot2) provides a nice way to visualize a model‚Äôs overall view of the data. This functionality is provided by Effects.jl and works out-of-the-box with most regression model packages in Julia (including MixedModels.jl). Support for formulae with embedded functions (such as log) is not yet complete, but we‚Äôre working on it!",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "useful_packages.html#estimated-marginal-least-square-means",
    "href": "useful_packages.html#estimated-marginal-least-square-means",
    "title": "Useful packages",
    "section": "1.8 Estimated Marginal / Least Square Means",
    "text": "1.8 Estimated Marginal / Least Square Means\nEffects.jl provides a subset of the functionality (basic estimated-marginal means and exhaustive pairwise comparisons) of the R package emmeans package. However, it is often better to use sensible, hypothesis-driven contrast coding than to compute all pairwise comparisons after the fact. üòÉ",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "useful_packages.html#makie",
    "href": "useful_packages.html#makie",
    "title": "Useful packages",
    "section": "3.1 Makie",
    "text": "3.1 Makie\nThe Makie ecosystem is a relatively new take on graphics that aims to be both powerful and easy to use. Makie.jl itself only provides abstract definitions for many components (and is used in e.g.¬†MixedModelsMakie.jl to define plot types for MixedModels.jl). The actual plotting and rendering is handled by a backend package such as CairoMakie.jl (good for Quarto notebooks or rending static 2D images) and GLMakie.jl (good for dynamic, interactive visuals and 3D images). AlgebraOfGraphics.jl builds a grammar of graphics upon the Makie framework. It‚Äôs a great way to get good plots very quickly, but extensive customization is still best achieved by using Makie directly. PumasAI has a collection of nice tutorials for AlgebraOfGraphics.",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "useful_packages.html#plots.jl",
    "href": "useful_packages.html#plots.jl",
    "title": "Useful packages",
    "section": "3.2 Plots.jl",
    "text": "3.2 Plots.jl\nPlots.jl is the original plotting package in Julia, but we often find it difficult to work with compared to some of the other alternatives. StatsPlots.jl builds on this, adding common statistical plots, while UnicodePlots.jl renders plots as Unicode characters directly in the REPL.\nPGFPlotsX.jl is a very new package that writes directly to PGF (the format used by LaTeX‚Äôs tikz framework) and can stand alone or be used as a rendering backend for the Plots.jl ecosystem.",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "useful_packages.html#gadfly",
    "href": "useful_packages.html#gadfly",
    "title": "Useful packages",
    "section": "3.3 Gadfly",
    "text": "3.3 Gadfly\nGadfly.jl was the original attempt to create a plotting system in Julia based on the grammar of graphics (the ‚Äúgg‚Äù in ggplot2). Development has largely stalled, but some functionality still exceeds AlgebraOfGraphics.jl, which has taken up the grammar of graphics mantle. Notably, the MixedModels.jl documentation still uses Gadfly as of this writing (early September 2021).",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "useful_packages.html#others",
    "href": "useful_packages.html#others",
    "title": "Useful packages",
    "section": "3.4 Others",
    "text": "3.4 Others\nThere are many other graphics packages available in Julia, often wrapping well-established frameworks such as VegaLite.",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "sleepstudy_speed.html",
    "href": "sleepstudy_speed.html",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "",
    "text": "Belenky et al. (2003) reported effects of sleep deprivation across a 14-day study of 30-to-40-year old men and women holding commercial vehicle driving licenses. Their analyses are based on a subset of tasks and ratings from very large and comprehensive test and questionnaire battery (Balkin et al., 2000).\nInitially 66 subjects were assigned to one of four time-in-bed (TIB) groups with 9 hours (22:00-07:00) of sleep augmentation or 7 hours (24:00-07:00), 5 hours (02:00-07:00), and 3 hours (04:00-0:00) of sleep restrictions per night, respectively. The final sample comprised 56 subjects. The Psychomotor Vigilance Test (PVT) measures simple reaction time to a visual stimulus, presented approximately 10 times ‚ÅÑ minute (interstimulus interval varied from 2 to 10 s in 2-s increments) for 10 min and implemented in a thumb-operated, hand-held device (Dinges & Powell, 1985).\n\n\nThe study comprised 2 training days (T1, T2), one day with baseline measures (B), seven days with sleep deprivation (E1 to E7), and four recovery days (R1 to R4). T1 and T2 were devoted to training on the performance tests and familiarization with study procedures. PVT baseline testing commenced on the morning of the third day (B) and testing continued for the duration of the study (E1‚ÄìE7, R1‚ÄìR3; no measures were taken on R4). Bed times during T, B, and R days were 8 hours (23:00-07:00).\n\n\n\nThe PVT (along with the Stanford Sleepiness Scale) was administered as a battery four times per day (09:00, 12:00, 15:00, and 21:00 h); the battery included other tests not reported here (see Balkin et al., 2000). The sleep latency test was administered at 09:40 and 15:30 h for all groups. Subjects in the 3- and 5-h TIB groups performed an additional battery at 00:00 h and 02:00 h to occupy their additional time awake. The PVT and SSS were administered in this battery; however, as data from the 00:00 and 02:00 h sessions were not common to all TIB groups, these data were not included in the statistical analyses reported in the paper.\n\n\n\nThe authors analyzed response speed, that is (1/RT)*1000 ‚Äì completely warranted according to a Box-Cox check of the current data ‚Äì with mixed-model ANOVAs using group as between- and day as within-subject factors. The ANOVA was followed up with simple tests of the design effects implemented over days for each of the four groups.\n\n\n\nThe current data distributed with the RData collection is attributed to the 3-hour TIB group, but the means do not agree at all with those reported for this group in (Belenky et al., 2003, fig. 3) where the 3-hour TIB group is also based on only 13 (not 18) subjects. Specifically, the current data show a much smaller slow-down of response speed across E1 to E7 and do not reflect the recovery during R1 to R3. The current data also cover only 10 not 11 days, but it looks like only R3 is missing. The closest match of the current means was with the average of the 3-hour and 7-hour TIB groups; if only males were included, this would amount to 18 subjects. (This conjecture is based only on visual inspection of graphs.)",
    "crumbs": [
      "Worked examples",
      "The sleepstudy: Speed - for a change ..."
    ]
  },
  {
    "objectID": "sleepstudy_speed.html#design",
    "href": "sleepstudy_speed.html#design",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "",
    "text": "The study comprised 2 training days (T1, T2), one day with baseline measures (B), seven days with sleep deprivation (E1 to E7), and four recovery days (R1 to R4). T1 and T2 were devoted to training on the performance tests and familiarization with study procedures. PVT baseline testing commenced on the morning of the third day (B) and testing continued for the duration of the study (E1‚ÄìE7, R1‚ÄìR3; no measures were taken on R4). Bed times during T, B, and R days were 8 hours (23:00-07:00).",
    "crumbs": [
      "Worked examples",
      "The sleepstudy: Speed - for a change ..."
    ]
  },
  {
    "objectID": "sleepstudy_speed.html#test-schedule-within-days",
    "href": "sleepstudy_speed.html#test-schedule-within-days",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "",
    "text": "The PVT (along with the Stanford Sleepiness Scale) was administered as a battery four times per day (09:00, 12:00, 15:00, and 21:00 h); the battery included other tests not reported here (see Balkin et al., 2000). The sleep latency test was administered at 09:40 and 15:30 h for all groups. Subjects in the 3- and 5-h TIB groups performed an additional battery at 00:00 h and 02:00 h to occupy their additional time awake. The PVT and SSS were administered in this battery; however, as data from the 00:00 and 02:00 h sessions were not common to all TIB groups, these data were not included in the statistical analyses reported in the paper.",
    "crumbs": [
      "Worked examples",
      "The sleepstudy: Speed - for a change ..."
    ]
  },
  {
    "objectID": "sleepstudy_speed.html#statistical-analyses",
    "href": "sleepstudy_speed.html#statistical-analyses",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "",
    "text": "The authors analyzed response speed, that is (1/RT)*1000 ‚Äì completely warranted according to a Box-Cox check of the current data ‚Äì with mixed-model ANOVAs using group as between- and day as within-subject factors. The ANOVA was followed up with simple tests of the design effects implemented over days for each of the four groups.",
    "crumbs": [
      "Worked examples",
      "The sleepstudy: Speed - for a change ..."
    ]
  },
  {
    "objectID": "sleepstudy_speed.html#current-data",
    "href": "sleepstudy_speed.html#current-data",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "",
    "text": "The current data distributed with the RData collection is attributed to the 3-hour TIB group, but the means do not agree at all with those reported for this group in (Belenky et al., 2003, fig. 3) where the 3-hour TIB group is also based on only 13 (not 18) subjects. Specifically, the current data show a much smaller slow-down of response speed across E1 to E7 and do not reflect the recovery during R1 to R3. The current data also cover only 10 not 11 days, but it looks like only R3 is missing. The closest match of the current means was with the average of the 3-hour and 7-hour TIB groups; if only males were included, this would amount to 18 subjects. (This conjecture is based only on visual inspection of graphs.)",
    "crumbs": [
      "Worked examples",
      "The sleepstudy: Speed - for a change ..."
    ]
  },
  {
    "objectID": "sleepstudy_speed.html#within-subject-simple-regressions",
    "href": "sleepstudy_speed.html#within-subject-simple-regressions",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "5.1 Within-subject simple regressions",
    "text": "5.1 Within-subject simple regressions\nApplying combine to a grouped data frame like gdf produces a DataFrame with a row for each group. The permutation ord provides an ordering for the groups by increasing intercept (predicted response at day 0).\n\nwithin = combine(gdf, [:day, :speed] =&gt; simplelinreg =&gt; :coef)\n\n18√ó2 DataFrame\n\n\n\nRow\nSubj\ncoef\n\n\n\nString\nTuple‚Ä¶\n\n\n\n\n1\nS308\n(3.94806, -0.194812)\n\n\n2\nS309\n(4.87022, -0.0475185)\n\n\n3\nS310\n(4.90606, -0.120054)\n\n\n4\nS330\n(3.4449, -0.0291309)\n\n\n5\nS331\n(3.47647, -0.0498047)\n\n\n6\nS332\n(3.84436, -0.105511)\n\n\n7\nS333\n(3.60159, -0.0917378)\n\n\n8\nS334\n(4.04528, -0.133527)\n\n\n9\nS335\n(3.80451, 0.0455771)\n\n\n10\nS337\n(3.34374, -0.137744)\n\n\n11\nS349\n(4.46855, -0.170885)\n\n\n12\nS350\n(4.21414, -0.20151)\n\n\n13\nS351\n(3.80469, -0.0728582)\n\n\n14\nS352\n(3.68634, -0.144957)\n\n\n15\nS369\n(3.85384, -0.120531)\n\n\n16\nS370\n(4.52679, -0.215965)\n\n\n17\nS371\n(3.853, -0.0936243)\n\n\n18\nS372\n(3.69208, -0.113292)\n\n\n\n\n\n\nFigure¬†1 shows the reaction speed versus days of sleep deprivation by subject. The panels are arranged by increasing initial reaction speed starting at the lower left and proceeding across rows.\n\n\nCode\nlet\n  ord = sortperm(first.(within.coef))\n  labs = values(only.(keys(gdf)))[ord]       # labels for panels\n  f = clevelandaxes!(Figure(; resolution=(1000, 750)), labs, (2, 9))\n  for (axs, sdf) in zip(f.content, gdf[ord]) # iterate over the panels and groups\n    scatter!(axs, sdf.day, sdf.speed)      # add the points\n    coef = simplelinreg(sdf.day, sdf.speed)\n    abline!(axs, first(coef), last(coef))  # add the regression line\n  end\n  f\nend\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n‚îå Warning: abline! is deprecated and will be removed in the future. Use ablines / ablines! instead.\n‚îÇ   caller = top-level scope at sleepstudy_speed.qmd:102\n‚îî @ Core ~/Work/SMLP2026/sleepstudy_speed.qmd:102\n\n\n\n\n\n\n\n\n\nFigure¬†1: Reaction speed (s‚Åª¬π) versus days of sleep deprivation by subject",
    "crumbs": [
      "Worked examples",
      "The sleepstudy: Speed - for a change ..."
    ]
  },
  {
    "objectID": "singularity.html",
    "href": "singularity.html",
    "title": "Convergence, singularity and all that",
    "section": "",
    "text": "Add the packages to be used\n\n\nCode\nusing CairoMakie\nusing DataFrames\nusing LinearAlgebra\nusing MixedModels\nusing MixedModelsMakie\n\nconst progress = isinteractive()\n\n\nFit a model for reaction time in the sleepstudy example.\n\nm01 = lmm(@formula(reaction ~ 1 + days + (1 + days|subj)), MixedModels.dataset(:sleepstudy); progress)\n\nprint(m01)\n\nLinear mixed model fit by maximum likelihood\n reaction ~ 1 + days + (1 + days | subj)\n   logLik   -2 logLik     AIC       AICc        BIC    \n  -875.9697  1751.9393  1763.9393  1764.4249  1783.0971\n\nVariance components:\n            Column    Variance Std.Dev.   Corr.\nsubj     (Intercept)  565.52071 23.78068\n         days          32.68242  5.71685 +0.08\nResidual              654.94015 25.59180\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                Coef.  Std. Error      z  Pr(&gt;|z|)\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n(Intercept)  251.405      6.6323   37.91    &lt;1e-99\ndays          10.4673     1.50224   6.97    &lt;1e-11\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\nThe covariance matrix for each subject‚Äôs random effects is evaluated from its ‚Äúmatrix square root‚Äù, called the Cholesky factor.\n\nŒª = only(m01.Œª)\n\n2√ó2 LowerTriangular{Float64, Matrix{Float64}}:\n 0.92923     ‚ãÖ \n 0.0181644  0.222646\n\n\nThe transpose of \\(\\lambda\\), written \\(\\lambda'\\), is an upper triangular matrix generated by ‚Äúflipping‚Äù \\(\\lambda\\) about the main diagonal.\n\nŒª'\n\n2√ó2 UpperTriangular{Float64, Adjoint{Float64, Matrix{Float64}}}:\n 0.92923  0.0181644\n  ‚ãÖ       0.222646\n\n\nThe product \\(\\lambda * \\lambda'\\) will be symmetric. The covariance matrix of the random effects, \\(\\Sigma\\), is this symmetric matrix scaled by \\(\\sigma^2\\)\n\nŒ£ = m01.œÉ^2 * Œª * Œª'\n\n2√ó2 Matrix{Float64}:\n 565.521   11.0547\n  11.0547  32.6824\n\n\nThe estimated variances of the random effects, which are the diagonal elements of \\(\\Sigma\\), correspond to the values shown in the table. To evaluate the covariance, isolate the correlation\n\n# m01.œÉœÅs extracts the `œÉœÅs` property of the model.\n# This property is a NamedTuple where the names\n# correspond to grouping factors - in this case, `subj`.\n# So `m01.œÉœÅs.subj.œÅ` is the estimated correlation(s) for\n# this grouping factor.  Because there is only one such correlation\n# we can extract it with `only()`, which also verifies that\n# there is exactly one.\nœÅ = only(m01.œÉœÅs.subj.œÅ)\n\n0.08131376654227451\n\n\nand multiply by the standard deviations\n\nœÅ * sqrt(first(Œ£) * last(Œ£))\n\n11.054661054378572",
    "crumbs": [
      "Visualizations and diagnostics",
      "Convergence, singularity and all that"
    ]
  },
  {
    "objectID": "singularity.html#evaluating-the-random-effects-correlation-from-theta",
    "href": "singularity.html#evaluating-the-random-effects-correlation-from-theta",
    "title": "Convergence, singularity and all that",
    "section": "5.1 Evaluating the random effects correlation from \\(\\theta\\)",
    "text": "5.1 Evaluating the random effects correlation from \\(\\theta\\)\nThere is a short-cut for evaluating the correlation which is to ‚Äúnormalize‚Äù the second row of \\(\\lambda\\), in the sense that the row is scaled so that it has unit length.\n\nnormed = normalize!(Œª[2, :])\n\n2-element Vector{Float64}:\n 0.08131376654227453\n 0.9966885528442215\n\n\nproviding the correlation as\n\nfirst(normed)\n\n0.08131376654227453",
    "crumbs": [
      "Visualizations and diagnostics",
      "Convergence, singularity and all that"
    ]
  },
  {
    "objectID": "singularity.html#optimizing-with-a-fixed-correlation",
    "href": "singularity.html#optimizing-with-a-fixed-correlation",
    "title": "Convergence, singularity and all that",
    "section": "5.2 Optimizing with a fixed correlation",
    "text": "5.2 Optimizing with a fixed correlation\nTo profile the correlation we need optimize the objective while fixing a value of the correlation. The way we will do this is to determine \\(\\theta_2\\) as a function of the fixed \\(\\rho\\) and \\(\\theta_3\\).\nWe need to solve \\[\n\\rho = \\frac{\\theta_2}{\\sqrt{\\theta_2^2 + \\theta_3^2}}\n\\]\nfor \\(\\theta_2\\) as a function of \\(\\rho\\) and \\(\\theta_3\\).\nNotice that \\(\\theta_2\\) and \\(\\rho\\) have the same sign. Thus it is sufficient to determine the absolute value of \\(\\theta_2\\) then transfer the sign from \\(\\rho\\). \\[\n\\theta_2^2=\\rho^2(\\theta_2^2 + \\theta_3^2)\n\\] which implies \\[\n\\theta_2^2 = \\frac{\\rho^2}{1-\\rho^2}\\theta_3^2, \\quad \\theta_3\\ge 0\n\\] and thus \\[\n\\theta_2=\\frac{\\rho}{\\sqrt{1-\\rho^2}}\\theta_3\n\\]\nThis page was rendered from git revision fb99b3c\n using Quarto 1.7.33.",
    "crumbs": [
      "Visualizations and diagnostics",
      "Convergence, singularity and all that"
    ]
  },
  {
    "objectID": "selection.html",
    "href": "selection.html",
    "title": "SMLP2026: Advanced Frequentist Track",
    "section": "",
    "text": "Code\nusing Arrow\nusing CairoMakie\nusing DataFrames\ntbl = Arrow.Table(\"./data/fggk21.arrow\")\n\n\n‚îå Warning: unsupported ARROW:extension:name type: \"JuliaLang.CategoricalArrays.CategoricalArray\", arrow type = String\n‚îî @ Arrow ~/.julia/packages/Arrow/3GbnS/src/eltypes.jl:53\n\n\n\n\nArrow.Table with 525126 rows, 7 columns, and schema:\n :Cohort  String\n :School  String\n :Child   String\n :Sex     String\n :age     Float64\n :Test    String\n :score   Float64\ndf = DataFrame(tbl)\ndescribe(df)\n\n7√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nDataType\n\n\n\n\n1\nCohort\n\n2011\n\n2019\n0\nString\n\n\n2\nSchool\n\nS100043\n\nS800200\n0\nString\n\n\n3\nChild\n\nC002352\n\nC117966\n0\nString\n\n\n4\nSex\n\nfemale\n\nmale\n0\nString\n\n\n5\nage\n8.56073\n7.99452\n8.55852\n9.10609\n0\nFloat64\n\n\n6\nTest\n\nBPT\n\nStar_r\n0\nString\n\n\n7\nscore\n226.141\n1.14152\n4.65116\n1530.0\n0\nFloat64"
  },
  {
    "objectID": "selection.html#raw-score-density",
    "href": "selection.html#raw-score-density",
    "title": "SMLP2026: Advanced Frequentist Track",
    "section": "1 Raw score density",
    "text": "1 Raw score density\n\nlet\n  fdensity = Figure(; resolution=(1000, 500))\n  axs = Axis(fdensity[1, 1])\n  tdf = filter(:Test =&gt; ==(test), df)\n  colors = Makie.cgrad(:PuOr_4, 2; categorical=true, alpha=0.6)\n  if by_sex\n    density!(\n      axs,\n      filter(:Sex =&gt; ==(\"female\"), tdf).score;\n      color=colors[1],\n      label=\"Girls\",\n    )\n    density!(\n      axs,\n      filter(:Sex =&gt; ==(\"male\"), tdf).score;\n      color=colors[2],\n      label=\"Boys\",\n    )\n    axislegend(axs; position=:lt)\n  else\n    density!(axs, tdf.score)\n  end\n  fdensity\nend\n\nThis page was rendered from git revision fb99b3c\n using Quarto 1.7.33."
  },
  {
    "objectID": "pkg.html",
    "href": "pkg.html",
    "title": "Package management and reproducible environments",
    "section": "",
    "text": "Julius Krumbiegel also has a great blog post with more details on Julia environments.\nJulia packages can be configured (in a file called Project.toml) on a per-project basis. The packaged sources and compiled versions are stored in a central location, e.g.¬†~/.julia/packages and ~/.julia/compiled on Linux systems, but the configuration of packages to be used can be local to a project. The Pkg package is used to modify the local project‚Äôs configuration. (An alternative is ‚Äúpackage mode‚Äù in the read-eval-print-loop or REPL, which we will show at the summer school.) Start julia in the directory of the cloned SMLP2026 repository\n\nusing Pkg        # there's a package called 'Pkg' to manipulate package configs\nPkg.activate(\".\")# activate the current directory as the project\n\nIf you‚Äôve received an environment from someone/somewhere else ‚Äì such as this course repository ‚Äì then you‚Äôll need to first ‚Äúinstantiate‚Äù it (i.e., install all the dependencies).\n\nPkg.instantiate()# only needed the first time you work in a project\nPkg.update()     # get the latest package versions compatible with the project\n\n\nPkg.status()\n\nOccasionally the Pkg.status function call will give info about new versions being available but blocked by requirements of other packages. This is to be expected - the package system is large and the web of dependencies are complex. Generally the Julia package system is very good at resolving dependencies.\nThis page was rendered from git revision fb99b3c\n using Quarto 1.7.33.\n\n\n\n Back to top",
    "crumbs": [
      "Getting started with Julia",
      "Package management and reproducible environments"
    ]
  },
  {
    "objectID": "mrk17.html",
    "href": "mrk17.html",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "",
    "text": "Packages we (might) use.\n\nusing DataFrames\nusing MixedModels\nusing MixedModelsMakie\nusing StatsBase\n\nusing SMLP2026: dataset\nusing Statistics: mean, std\n\nconst progress = isinteractive()\n\nfalse\n\n\n\ndat = DataFrame(dataset(:mrk17_exp1))\ndescribe(dat)\n\n9√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nDataType\n\n\n\n\n1\nsubj\n\nS01\n\nS73\n0\nString\n\n\n2\nitem\n\nACHE\n\nYEAR\n0\nString\n\n\n3\ntrial\n239.958\n2\n239.0\n480\n0\nInt16\n\n\n4\nF\n\nHF\n\nLF\n0\nString\n\n\n5\nP\n\nrel\n\nunr\n0\nString\n\n\n6\nQ\n\nclr\n\ndeg\n0\nString\n\n\n7\nlQ\n\nclr\n\ndeg\n0\nString\n\n\n8\nlT\n\nNW\n\nWD\n0\nString\n\n\n9\nrt\n647.173\n301\n601.0\n2994\n0\nInt16",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#response-covariates-and-factors",
    "href": "mrk17.html#response-covariates-and-factors",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "2.1 Response, covariates, and factors",
    "text": "2.1 Response, covariates, and factors\nLinear mixed models (LMMs), like many other types of statistical models, describe a relationship between a response variable and covariates that have been measured or observed along with the response. The statistical model assumes that the residuals of the fitted response (i.e., not the responses) are normally ‚Äì also identically and independently ‚Äì distributed. This is the first assumption of normality in the LMM. It is standard practice that model residuals are inspected and, if serious skew is indicated, that the response is Box-Cox transformed (unless not justified for theoretical reasons) to fulfill this model assumption.\nIn the following we distinguish between categorical covariates and numerical covariates. Categorical covariates are factors. The important characteristic of a factor is that, for each observed value of the response, the factor takes on the value of one of a set of discrete levels. The levels can be unordered (nominal) or ordered (ordinal). We use the term covariate when we refer to numerical covariates, that is to continuous measures with some distribution. In principle, statistical models are not constrained by the distribution of observations across levels of factors and covariates, but the distribution may lead to problems of model identification and it does implications for statistical power.\nStatistical power, especially for the detection of interactions, is best when observations are uniformly distributed across levels of factors or uniform across the values of covariates. In experimental designs, uniform distributions may be achieved by balanced assignment of subjects (or other carriers of responses) to the levels of factors or combinations of factor levels. In observational contexts, we achieve uniform distributions by stratification (e..g., on age, gender, or IQ scores). Statistical power is worse for skewed than normal distributions (I think ‚Ä¶). Therefore, although it is not required to meet an assumption of the statistical model, it may be useful to consider Box-Cox transformations of covariates.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#nested-and-crossed-random-grouping-factors",
    "href": "mrk17.html#nested-and-crossed-random-grouping-factors",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "2.2 Nested and crossed random (grouping) factors",
    "text": "2.2 Nested and crossed random (grouping) factors\nIn LMMs the levels of at least one of the factors represents units in the data set that are assumed to be sampled, ideally randomly, from a population that is normally distributed with respect to the response. This is the second assumption of normal distribution in LMMs. In psychology and linguistics the observational units are often the subjects or items (e..g., texts, sentences, words, pictures) in the study. We may use numbers, such as subject identifiers, to designate the particular levels that we observed; we recommend to prepend these numbers with ‚ÄúS‚Äù or ‚ÄúI‚Äù to avoid confusion with numeric variables.\nRandom sampling is the basis of generalization from the sample to the population. The core statistics we will estimate in this context are variances and correlations of grand means and (quasi-)experimental effects. These terms will be explained below. What we want to stress here is that the estimation of (co-)variances / correlations requires a larger number of units (levels) than the estimation of means. Therefore, from a practical perspective, it is important that random factors are represented with many units.\nWhen there is more than one random factor, we must be clear about their relation. The two prototypical cases are that the factors are nested or crossed. In multilevel models, a special case of mixed models, the levels of the random factors are strictly nested. For example, at a given time, every student attends a specific class in a specific school. Students, classes, and schools could be three random factors. As soon as we look at this scenario across several school years, the nesting quickly falls apart because students may move between classes and between schools.\nIn psychology and linguistics, random factors are often crossed, for example, when every subject reads every word in every sentence in a word-by-word self-paced reading experiment (or alternatively: when every word in every sentence elicits a response from every subject). However, in an eye-movement experiment (for example), the perfect crossing on a measure like fixation duration is not attainable because of blinks or skipping of words.\nIn summary, the typical situation in experimental and observational studies with more than one random factor is partial crossing or partial nesting of levels of the random factors. Linear mixed models handle these situations very well.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#experimental-and-quasi-experimental-fixed-factors-covariates",
    "href": "mrk17.html#experimental-and-quasi-experimental-fixed-factors-covariates",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "2.3 Experimental and quasi-experimental fixed factors / covariates",
    "text": "2.3 Experimental and quasi-experimental fixed factors / covariates\nFixed experimental factor or covariate. In experiments the units (or levels) of the random factor(s) are assigned to manipulations implemented in their design. The researcher controls the assignment of units of the random factor(s) (e.g., subjects, items) to experimental manipulations. These manipulations are represented as factors with a fixed and discrete set of levels (e.g., training vs.¬†control group) or as covariates associated with continuous numeric values (e.g., presentation times).\nFixed quasi-experimental factor or covariate. In observational studies (which can also be experiments) the units (or levels) of random factors may ‚Äúbring along‚Äù characteristics that represent the levels of quasi-experimental factors or covariates beyond the control of the researcher. Whether a a subject is female, male, or diverse or whether a word is a noun, a verb, or an adjective are examples of quasi-experimental factors of gender or word type, respectively. Subject-related covariates are body height, body mass, and IQ scores; word-related covariates are their lengths, frequency, and cloze predictability.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#between-unit-and-within-unit-factors-covariates",
    "href": "mrk17.html#between-unit-and-within-unit-factors-covariates",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "2.4 Between-unit and within-unit factors / covariates",
    "text": "2.4 Between-unit and within-unit factors / covariates\nThe distinction between between-unit and within-unit factors is always relative to a random (grouping) factor of an experimental design. A between-unit factor / covariate is a factor for which every unit of the random factor is assigned to or characterized by only one level of the factor. A within-unit factor is a factor for which units of the random factor appear at every level of the factor.\nFor the typical random factor, say Subject, there is little ambiguity because we are used to the between-within distinction from ANOVAs, more specifically the F1-ANOVA. In psycholinguistics, there is the tradition to test effects also for the second random factor Item in an F2-ANOVA. Importantly, for a given fixed factor all four combinations are possible. For example, Gender is a fixed quasi-experimental between-subject / within-item factor; word frequency is a fixed quasi-experimental within-subject / between-item covariate; Prime-target relation is a fixed experimental within-subject / within-item factor (assuming that targets are presented both in a primed and in an unprimed situation); and when a training manipulation is defined by the items used in the training, then in a training-control group design, the fixed factor Group is a fixed experimental between-subject / between-item factor.\nThese distinctions are critical for setting up LMMs because variance components for (quasi-)experimental effects can only be specified for within-unit effects. Note also that loss of data (within limits), counterbalancing or blocking of items are irrelevant for these definitions.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#factor-based-contrasts-and-covariate-based-trends",
    "href": "mrk17.html#factor-based-contrasts-and-covariate-based-trends",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "2.5 Factor-based contrasts and covariate-based trends",
    "text": "2.5 Factor-based contrasts and covariate-based trends\nThe simplest fixed factor has two levels and the model estimates the difference between them. When we move to factors with k levels, we must decide on how we spend the k-1 degrees of freedom, that is we must specify a set of contrasts. (If we don‚Äôt do it, the program chooses DummyCoding contrasts for us.)\nThe simplest specification of a covariate is to include its linear trend, that is its slope. The slope (like a contrast) represents a difference score, that is the change in response to a one-unit change on the covariate. For covariates we must decide on the order of the trend we want to model.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#contrast--and-trend-based-fixed-effect-model-parameters",
    "href": "mrk17.html#contrast--and-trend-based-fixed-effect-model-parameters",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "2.6 Contrast- and trend-based fixed-effect model parameters",
    "text": "2.6 Contrast- and trend-based fixed-effect model parameters\nFixed factors and covariates are expected to have effects on the response. Fixed-effect model parameters estimate the hypothesized main and interaction effects of the study. The estimates of factors are based on contrasts; the estimates of covariates are based on trends. Conceptually, they correspond to unstandardized regression coefficients in multiple regression.\nThe intercept is a special regression coefficient; it estimates the value of the dependent variable when all fixed effects associated with factors and trends associated with covariates are zero. In experimental designs with higher-order interactions there is an advantage of specifying the LMM in such a way that the intercept estimates the grand mean (GM; mean of the means of design cells). This happens if (a) contrasts for factors are chosen such that the intercept estimates the GM (positive: EffectsCoding, SeqDifferenceCoding, or HelmertCoding contrasts; negative: DummyCoding), (b) orthogonal polynomial trends are used (Helmert, anova-based), and (c) covariates are centered on their mean before inclusion in the model. As always, there may be good theoretical reasons to depart from the default recommendation.\nThe specification of contrasts / trends does not depend on the status of the fixed factor / covariate. It does not matter whether a factor varies between or within the units of a random factor or whether it is an experimental or quasi-experimental factor. Contrasts are not specified for random (grouping) factors.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#variance-components-vcs-and-correlation-parameters-cps",
    "href": "mrk17.html#variance-components-vcs-and-correlation-parameters-cps",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "2.7 Variance components (VCs) and correlation parameters (CPs)",
    "text": "2.7 Variance components (VCs) and correlation parameters (CPs)\nVariance components (VCs) and correlation parameters (CPs) are within-group model parameters; they correspond to (some of the) within-unit (quasi-)experimental fixed-effect model parameters. Thus, we may be able to estimate a subject-related VC for word frequency. If we included a linear trend for word frequency, the VC estimates the subject-related variance in these slopes. We cannot estimate an item-related VC for the word-frequency slopes because there is only one frequency associated with words. Analogously, we may able to estimate an item-related VC for the effect of Group (training vs. control), but we cannot estimate a subject-related VC for this effect.\nThe within-between characteristics of fixed factors and covariates relative to the random factor(s) are features of the design of the experiment or observational study. They fundamentally constrain the specification of the LMM. That‚Äôs why it is of upmost importance to be absolutely clear about their status.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#conditional-modes-of-random-effects",
    "href": "mrk17.html#conditional-modes-of-random-effects",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "2.8 Conditional modes of random effects",
    "text": "2.8 Conditional modes of random effects\nIn this outline of the dimensions underlying the specification of an LMM, we have said nothing so far about the conditional modes of random effects (i.e., the results shown in caterpillar and shrinkage plots). They are not needed for model specification or model selection.\nThe VC is the prior variance of the random effects, whereas var(ranef(model)) is the variance of the posterior means/modes of the random effects. See Kliegl et al.¬†(2010, VisualCognition); Rizopoulos (2019, stackexchange.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#abstract",
    "href": "mrk17.html#abstract",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "3.1 Abstract",
    "text": "3.1 Abstract\nThis semantic-priming experiment was reported in Masson, Rabe, & Kliegl (2017, Exp. 1, Memory & Cognition). It is a direct replication of an experiment reported in Masson & Kliegl (2013, Exp. 1, JEPLMC). Following a prime word a related or unrelated high- or low-frequency target word or a nonword was presented in clear or dim font. The subject‚Äôs task was to decide as quickly as possible whether the target was a word or a nonword, that is subjects performed a lexical decision task (LDT). The reaction time and the accuracy of the response were recorded. Only correct reaction times to words are included. After filtering there were 16,409 observations recorded from 73 subjects and 240 items.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#codebook",
    "href": "mrk17.html#codebook",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "3.2 Codebook",
    "text": "3.2 Codebook\nThe data (variables and observations) used by Masson et al.¬†(2017) are available in file MRK17_Exp1.RDS\n\n\n\nVariable\nDescription\n\n\n\n\nSubj\nSubject identifier\n\n\nItem\nTarget (non-)word\n\n\ntrial\nTrial number\n\n\nF\nTarget frequency is high or low\n\n\nP\nPrime is related or unrelated to target\n\n\nQ\nTarget quality is clear or degraded\n\n\nlQ\nLast-trial target quality is clear or degraded\n\n\nlT\nLast-trail target requires word or nonword response\n\n\nrt\nReaction time [ms]\n\n\n\nlagQlty and lagTrgt refer to experimental conditions in the last trial.\nCorresponding indicator variables (-1/+1):\n\ncells = combine(\n  groupby(dat, [:F, :P, :Q, :lQ, :lT]),\n  nrow =&gt; :n,\n  :rt =&gt; mean =&gt; :rt_m,\n  :rt =&gt; std =&gt; :rt_sd\n)\ncells\n\n32√ó8 DataFrame7 rows omitted\n\n\n\nRow\nF\nP\nQ\nlQ\nlT\nn\nrt_m\nrt_sd\n\n\n\nString\nString\nString\nString\nString\nInt64\nFloat64\nFloat64\n\n\n\n\n1\nLF\nunr\ndeg\ndeg\nNW\n491\n683.831\n197.074\n\n\n2\nLF\nunr\ndeg\ndeg\nWD\n488\n676.848\n198.409\n\n\n3\nLF\nunr\ndeg\nclr\nNW\n499\n685.265\n186.73\n\n\n4\nLF\nunr\ndeg\nclr\nWD\n519\n676.711\n167.794\n\n\n5\nLF\nunr\nclr\ndeg\nNW\n528\n658.225\n213.144\n\n\n6\nLF\nunr\nclr\ndeg\nWD\n494\n653.03\n196.71\n\n\n7\nLF\nunr\nclr\nclr\nNW\n506\n648.057\n209.625\n\n\n8\nLF\nunr\nclr\nclr\nWD\n499\n641.669\n183.413\n\n\n9\nLF\nrel\ndeg\ndeg\nNW\n492\n667.872\n175.869\n\n\n10\nLF\nrel\ndeg\ndeg\nWD\n498\n653.43\n180.58\n\n\n11\nLF\nrel\ndeg\nclr\nNW\n539\n650.16\n167.51\n\n\n12\nLF\nrel\ndeg\nclr\nWD\n512\n678.119\n217.209\n\n\n13\nLF\nrel\nclr\ndeg\nNW\n499\n635.076\n192.738\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n21\nHF\nunr\nclr\ndeg\nNW\n553\n644.6\n203.261\n\n\n22\nHF\nunr\nclr\ndeg\nWD\n495\n633.046\n172.604\n\n\n23\nHF\nunr\nclr\nclr\nNW\n495\n619.626\n176.55\n\n\n24\nHF\nunr\nclr\nclr\nWD\n532\n620.838\n192.689\n\n\n25\nHF\nrel\ndeg\ndeg\nNW\n508\n659.266\n192.637\n\n\n26\nHF\nrel\ndeg\ndeg\nWD\n546\n637.793\n177.496\n\n\n27\nHF\nrel\ndeg\nclr\nNW\n507\n645.493\n179.962\n\n\n28\nHF\nrel\ndeg\nclr\nWD\n517\n667.685\n231.145\n\n\n29\nHF\nrel\nclr\ndeg\nNW\n527\n615.647\n176.826\n\n\n30\nHF\nrel\nclr\ndeg\nWD\n504\n620.569\n173.764\n\n\n31\nHF\nrel\nclr\nclr\nNW\n546\n635.319\n205.19\n\n\n32\nHF\nrel\nclr\nclr\nWD\n499\n613.094\n192.13",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#model-fit",
    "href": "mrk17.html#model-fit",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "4.1 Model fit",
    "text": "4.1 Model fit\n\ncontrasts =\n    Dict( :F =&gt; EffectsCoding(; levels=[\"LF\", \"HF\"]) ,\n          :P =&gt; EffectsCoding(; levels=[\"unr\", \"rel\"]),\n          :Q =&gt; EffectsCoding(; levels=[\"deg\", \"clr\"]),\n          :lQ =&gt;EffectsCoding(; levels=[\"deg\", \"clr\"]),\n          :lT =&gt;EffectsCoding(; levels=[\"NW\", \"WD\"])\n          );\n\nm_cpx = let\n    form = @formula (1000/rt) ~ 1+F*P*Q*lQ*lT +\n                               (1+F+P+Q+lQ+lT | subj) +\n                               (1  +P+Q+lQ+lT | item);\n    fit(MixedModel, form, dat; contrasts, progress)\nend\n\nVarCorr(m_cpx)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nitem\n(Intercept)\n0.00320197\n0.05658591\n\n\n\n\n\n\n\n\nP: rel\n0.00012890\n0.01135339\n+0.05\n\n\n\n\n\n\n\nQ: clr\n0.00016010\n0.01265320\n+0.36\n+0.38\n\n\n\n\n\n\nlQ: clr\n0.00003528\n0.00593999\n+0.37\n+0.03\n+0.03\n\n\n\n\n\nlT: WD\n0.00015722\n0.01253868\n-0.11\n-0.87\n-0.01\n-0.35\n\n\n\nsubj\n(Intercept)\n0.03062637\n0.17500392\n\n\n\n\n\n\n\n\nF: HF\n0.00004632\n0.00680610\n+0.35\n\n\n\n\n\n\n\nP: rel\n0.00012558\n0.01120617\n+0.35\n+0.89\n\n\n\n\n\n\nQ: clr\n0.00079013\n0.02810932\n+0.41\n+0.43\n+0.74\n\n\n\n\n\nlQ: clr\n0.00011717\n0.01082471\n+0.06\n+0.13\n+0.19\n+0.57\n\n\n\n\nlT: WD\n0.00104659\n0.03235111\n+0.26\n-0.06\n-0.04\n+0.37\n+0.50\n\n\nResidual\n\n0.08569796\n0.29274214\n\n\n\n\n\n\n\n\n\n\n\nissingular(m_cpx)\n\ntrue\n\n\n\nMixedModels.PCA(m_cpx)\n\n(item = \nPrincipal components based on correlation matrix\n (Intercept)   1.0     .      .      .      .\n P: rel        0.05   1.0     .      .      .\n Q: clr        0.36   0.38   1.0     .      .\n lQ: clr       0.37   0.03   0.03   1.0     .\n lT: WD       -0.11  -0.87  -0.01  -0.35   1.0\n\nNormalized cumulative variances:\n[0.4221, 0.686, 0.9051, 1.0, 1.0]\n\nComponent loadings\n                PC1    PC2    PC3    PC4    PC5\n (Intercept)  -0.3   -0.67   0.02  -0.67   0.07\n P: rel       -0.6    0.38   0.22  -0.05   0.67\n Q: clr       -0.31  -0.34   0.7    0.47  -0.28\n lQ: clr      -0.31  -0.41  -0.62   0.55   0.2\n lT: WD        0.6   -0.34   0.27   0.15   0.66, subj = \nPrincipal components based on correlation matrix\n (Intercept)   1.0     .      .      .      .     .\n F: HF         0.35   1.0     .      .      .     .\n P: rel        0.35   0.89   1.0     .      .     .\n Q: clr        0.41   0.43   0.74   1.0     .     .\n lQ: clr       0.06   0.13   0.19   0.57   1.0    .\n lT: WD        0.26  -0.06  -0.04   0.37   0.5   1.0\n\nNormalized cumulative variances:\n[0.4751, 0.733, 0.8783, 0.9457, 0.9999, 1.0]\n\nComponent loadings\n                PC1    PC2    PC3    PC4    PC5    PC6\n (Intercept)  -0.34   0.03   0.83   0.29  -0.31  -0.08\n F: HF        -0.45   0.42  -0.1   -0.51  -0.35   0.48\n P: rel       -0.52   0.35  -0.17  -0.06   0.25  -0.72\n Q: clr       -0.52  -0.15  -0.12   0.44   0.52   0.47\n lQ: clr      -0.31  -0.52  -0.43   0.22  -0.61  -0.16\n lT: WD       -0.21  -0.64   0.25  -0.64   0.27  -0.07)\n\n\nVariance-covariance matrix of random-effect structure suggests overparameterization for both subject-related and item-related components.\nWe don‚Äôt look at fixed effects before model selection.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#vcs-and-cps",
    "href": "mrk17.html#vcs-and-cps",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "4.2 VCs and CPs",
    "text": "4.2 VCs and CPs\nWe can also look separately at item- and subj-related VCs and CPs for subjects and items.\n\nfirst(m_cpx.Œª)\n\n5√ó5 LinearAlgebra.LowerTriangular{Float64, Matrix{Float64}}:\n  0.193296      ‚ãÖ             ‚ãÖ           ‚ãÖ          ‚ãÖ \n  0.0020979    0.0387261      ‚ãÖ           ‚ãÖ          ‚ãÖ \n  0.0156057    0.0156812     0.0371321    ‚ãÖ          ‚ãÖ \n  0.00756068   0.000110154  -0.0024608   0.0186678   ‚ãÖ \n -0.00485665  -0.0369802     0.01735    -0.0119339  0.0\n\n\nVP is zero for last diagonal entry; not supported by data.\n\nlast(m_cpx.Œª)\n\n6√ó6 LinearAlgebra.LowerTriangular{Float64, Matrix{Float64}}:\n 0.597809      ‚ãÖ           ‚ãÖ           ‚ãÖ           ‚ãÖ          ‚ãÖ \n 0.00815402   0.0217727    ‚ãÖ           ‚ãÖ           ‚ãÖ          ‚ãÖ \n 0.0134843    0.0312673   0.0174896    ‚ãÖ           ‚ãÖ          ‚ãÖ \n 0.039272     0.0296599   0.0714437   0.0411557    ‚ãÖ          ‚ãÖ \n 0.00204162   0.00439008  0.00579478  0.0342807   0.0116236   ‚ãÖ \n 0.0283568   -0.0180685   0.00121043  0.0795929  -0.0582267  0.0368125\n\n\nVP is zero for fourth diagonal entry; not supported by data.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#model-fit-1",
    "href": "mrk17.html#model-fit-1",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "5.1 Model fit",
    "text": "5.1 Model fit\nWe take out correlation parameters.\n\nm_zcp = let\n    form = @formula (1000/rt) ~ 1+F*P*Q*lQ*lT +\n                       zerocorr(1+F+P+Q+lQ+lT | subj) +\n                       zerocorr(1  +P+Q+lQ+lT | item);\n    fit(MixedModel, form, dat; contrasts, progress)\nend\n\nVarCorr(m_zcp)\nissingular(m_zcp)\nMixedModels.PCA(m_zcp)\n\n(item = \nPrincipal components based on correlation matrix\n (Intercept)  1.0  .    .    .    .\n P: rel       0.0  1.0  .    .    .\n Q: clr       0.0  0.0  1.0  .    .\n lQ: clr      0.0  0.0  0.0  1.0  .\n lT: WD       0.0  0.0  0.0  0.0  1.0\n\nNormalized cumulative variances:\n[0.2, 0.4, 0.6, 0.8, 1.0]\n\nComponent loadings\n               PC1   PC2   PC3   PC4   PC5\n (Intercept)  1.0   0.0   0.0   0.0   0.0\n P: rel       0.0   1.0   0.0   0.0   0.0\n Q: clr       0.0   0.0   1.0   0.0   0.0\n lQ: clr      0.0   0.0   0.0   1.0   0.0\n lT: WD       0.0   0.0   0.0   0.0   1.0, subj = \nPrincipal components based on correlation matrix\n (Intercept)  1.0  .    .    .    .    .\n F: HF        0.0  1.0  .    .    .    .\n P: rel       0.0  0.0  1.0  .    .    .\n Q: clr       0.0  0.0  0.0  1.0  .    .\n lQ: clr      0.0  0.0  0.0  0.0  1.0  .\n lT: WD       0.0  0.0  0.0  0.0  0.0  1.0\n\nNormalized cumulative variances:\n[0.1667, 0.3333, 0.5, 0.6667, 0.8333, 1.0]\n\nComponent loadings\n               PC1   PC2   PC3   PC4   PC5   PC6\n (Intercept)  1.0   0.0   0.0   0.0   0.0   0.0\n F: HF        0.0   1.0   0.0   0.0   0.0   0.0\n P: rel       0.0   0.0   1.0   0.0   0.0   0.0\n Q: clr       0.0   0.0   0.0   1.0   0.0   0.0\n lQ: clr      0.0   0.0   0.0   0.0   1.0   0.0\n lT: WD       0.0   0.0   0.0   0.0   0.0   1.0)\n\n\n\nMixedModels.likelihoodratiotest(m_zcp, m_cpx)\n\n\n\n\n\nmodel-dof\n-2 logLik\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + zerocorr(1 + F + P + Q + lQ + lT | subj) + zerocorr(1 + P + Q + lQ + lT | item)\n44\n-7188\n\n\n\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + (1 + F + P + Q + lQ + lT | subj) + (1 + P + Q + lQ + lT | item)\n69\n-7148\n41\n25\n0.0231\n\n\n\n\n\nLooks ok. It might be a good idea to prune the LMM by removing small VCs.\n\nm_zcp2 = let\n    form = @formula (1000/rt) ~ 1+F*P*Q*lQ*lT +\n                       zerocorr(1  +P+Q+lQ+lT | subj) +\n                       zerocorr(1  +P+Q   +lT | item);\n    fit(MixedModel, form, dat; contrasts, progress)\nend\nVarCorr(m_zcp2)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\nitem\n(Intercept)\n0.00320202\n0.05658637\n\n\n\n\n\n\n\nP: rel\n0.00007138\n0.00844840\n.\n\n\n\n\n\n\nQ: clr\n0.00014720\n0.01213251\n.\n.\n\n\n\n\n\nlT: WD\n0.00011708\n0.01082036\n.\n.\n.\n\n\n\nsubj\n(Intercept)\n0.03061084\n0.17495954\n\n\n\n\n\n\n\nP: rel\n0.00009921\n0.00996047\n.\n\n\n\n\n\n\nQ: clr\n0.00077305\n0.02780383\n.\n.\n\n\n\n\n\nlQ: clr\n0.00011890\n0.01090422\n.\n.\n.\n\n\n\n\nlT: WD\n0.00106148\n0.03258037\n.\n.\n.\n.\n\n\nResidual\n\n0.08591586\n0.29311407\n\n\n\n\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_zcp2, m_zcp, m_cpx)\n\n\n\n\n\nmodel-dof\n-2 logLik\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + zerocorr(1 + P + Q + lQ + lT | subj) + zerocorr(1 + P + Q + lT | item)\n42\n-7189\n\n\n\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + zerocorr(1 + F + P + Q + lQ + lT | subj) + zerocorr(1 + P + Q + lQ + lT | item)\n44\n-7188\n0\n2\n0.9634\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + (1 + F + P + Q + lQ + lT | subj) + (1 + P + Q + lQ + lT | item)\n69\n-7148\n41\n25\n0.0231\n\n\n\n\n\nWe can perhaps remove some more.\n\nm_zcp3 = let\n    form = @formula (1000/rt) ~ 1+F*P*Q*lQ*lT +\n                       zerocorr(1    +Q   +lT | subj) + (1 | item);\n    fit(MixedModel, form, dat; contrasts, progress)\nend\nVarCorr(m_zcp3)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\nitem\n(Intercept)\n0.0032049\n0.0566118\n\n\n\n\nsubj\n(Intercept)\n0.0306166\n0.1749760\n\n\n\n\n\nQ: clr\n0.0007629\n0.0276199\n.\n\n\n\n\nlT: WD\n0.0010645\n0.0326265\n.\n.\n\n\nResidual\n\n0.0864752\n0.2940667\n\n\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_zcp3, m_zcp2, m_zcp, m_cpx)\n\n\n\n\n\nmodel-dof\n-2 logLik\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + zerocorr(1 + Q + lT | subj) + (1 | item)\n37\n-7196\n\n\n\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + zerocorr(1 + P + Q + lQ + lT | subj) + zerocorr(1 + P + Q + lT | item)\n42\n-7189\n8\n5\n0.1781\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + zerocorr(1 + F + P + Q + lQ + lT | subj) + zerocorr(1 + P + Q + lQ + lT | item)\n44\n-7188\n0\n2\n0.9634\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + (1 + F + P + Q + lQ + lT | subj) + (1 + P + Q + lQ + lT | item)\n69\n-7148\n41\n25\n0.0231\n\n\n\n\n\nAnd another iteration.\n\nm_zcp4 = let\n    form = @formula (1000/rt) ~ 1+F*P*Q*lQ*lT +\n                       zerocorr(1         +lT | subj) + (1 | item);\n    fit(MixedModel, form, dat; contrasts, progress)\nend\nVarCorr(m_zcp4)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\nitem\n(Intercept)\n0.0032067\n0.0566278\n\n\n\nsubj\n(Intercept)\n0.0306610\n0.1751028\n\n\n\n\nlT: WD\n0.0010896\n0.0330098\n.\n\n\nResidual\n\n0.0872330\n0.2953523\n\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_zcp4, m_zcp3, m_zcp2, m_zcp, m_cpx)\n\n\n\n\n\nmodel-dof\n-2 logLik\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + zerocorr(1 + lT | subj) + (1 | item)\n36\n-7259\n\n\n\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + zerocorr(1 + Q + lT | subj) + (1 | item)\n37\n-7196\n63\n1\n&lt;1e-14\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + zerocorr(1 + P + Q + lQ + lT | subj) + zerocorr(1 + P + Q + lT | item)\n42\n-7189\n8\n5\n0.1781\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + zerocorr(1 + F + P + Q + lQ + lT | subj) + zerocorr(1 + P + Q + lQ + lT | item)\n44\n-7188\n0\n2\n0.9634\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + (1 + F + P + Q + lQ + lT | subj) + (1 + P + Q + lQ + lT | item)\n69\n-7148\n41\n25\n0.0231\n\n\n\n\n\nToo much removed. Stay with m_zcp3, but extend with CPs.\n\nm_prm = let\n    form = @formula (1000/rt) ~ 1+F*P*Q*lQ*lT +\n                               (1+    Q+lT | subj) + (1 | item);\n    fit(MixedModel, form, dat; contrasts, progress)\nend\nVarCorr(m_prm)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\nitem\n(Intercept)\n0.0032010\n0.0565772\n\n\n\n\nsubj\n(Intercept)\n0.0306214\n0.1749898\n\n\n\n\n\nQ: clr\n0.0007626\n0.0276156\n+0.42\n\n\n\n\nlT: WD\n0.0010621\n0.0325896\n+0.26\n+0.38\n\n\nResidual\n\n0.0864769\n0.2940695\n\n\n\n\n\n\n\n\n5.1.1 post-hoc LMM\n\nm_prm = let\n    form = @formula (1000/rt) ~ 1+F*P*Q*lQ*lT +\n                               (1+    Q+lT | subj) + (1 | item);\n    fit(MixedModel, form, dat; contrasts, progress)\nend\nVarCorr(m_prm)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\nitem\n(Intercept)\n0.0032010\n0.0565772\n\n\n\n\nsubj\n(Intercept)\n0.0306214\n0.1749898\n\n\n\n\n\nQ: clr\n0.0007626\n0.0276156\n+0.42\n\n\n\n\nlT: WD\n0.0010621\n0.0325896\n+0.26\n+0.38\n\n\nResidual\n\n0.0864769\n0.2940695",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#vcs-and-cps-1",
    "href": "mrk17.html#vcs-and-cps-1",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "5.2 VCs and CPs",
    "text": "5.2 VCs and CPs\n\nMixedModels.likelihoodratiotest(m_zcp3, m_prm, m_cpx)\n\n\n\n\n\nmodel-dof\n-2 logLik\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + zerocorr(1 + Q + lT | subj) + (1 | item)\n37\n-7196\n\n\n\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + (1 + Q + lT | subj) + (1 | item)\n40\n-7180\n16\n3\n0.0011\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + (1 + F + P + Q + lQ + lT | subj) + (1 + P + Q + lQ + lT | item)\n69\n-7148\n33\n29\n0.2904\n\n\n\n\n\nThe LRT favors the complex LMM, but not that œá¬≤ &lt; 2*(œá¬≤-dof); AIC and BIC suggest against selection.\n\ngof_summary = let\n  nms = [:m_zcp3, :m_prm, :m_cpx]\n  mods = eval.(nms)\n  lrt = MixedModels.likelihoodratiotest(m_zcp3, m_prm, m_cpx)\n  DataFrame(;\n    name = nms,\n    dof=dof.(mods),\n    deviance=round.(deviance.(mods), digits=0),\n    AIC=round.(aic.(mods),digits=0),\n    AICc=round.(aicc.(mods),digits=0),\n    BIC=round.(bic.(mods),digits=0),\n    œá¬≤=vcat(:., round.(Int, diff(collect(lrt.lrt.deviance)))),\n    œá¬≤_dof=vcat(:., diff(collect(lrt.lrt.dof))),\n    # StatsBase.PValue includes some pretty-printing methods\n    pvalue=vcat(:., StatsBase.PValue.(collect(lrt.lrt.pval[2:end])))\n  )\nend\n\n3√ó9 DataFrame\n\n\n\nRow\nname\ndof\ndeviance\nAIC\nAICc\nBIC\nœá¬≤\nœá¬≤_dof\npvalue\n\n\n\nSymbol\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nAny\nAny\nAny\n\n\n\n\n1\nm_zcp3\n37\n7196.0\n7270.0\n7270.0\n7555.0\n.\n.\n.\n\n\n2\nm_prm\n40\n7180.0\n7260.0\n7260.0\n7568.0\n-16\n3\n0.0011\n\n\n3\nm_cpx\n69\n7148.0\n7286.0\n7286.0\n7817.0\n-33\n29\n0.2904",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#crossed-fixed-effects",
    "href": "mrk17.html#crossed-fixed-effects",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "6.1 Crossed fixed effects",
    "text": "6.1 Crossed fixed effects\n\nm_mrk17_crossed =let\n   form = @formula (1000/rt) ~ 1 + F*P*Q*lQ*lT +\n        (1+Q | subj) + zerocorr(0+lT | subj) + zerocorr(1 + P | item) ;\n    fit(MixedModel, form, dat; contrasts, progress)\nend\n\nVarCorr(m_prm)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\nitem\n(Intercept)\n0.0032010\n0.0565772\n\n\n\n\nsubj\n(Intercept)\n0.0306214\n0.1749898\n\n\n\n\n\nQ: clr\n0.0007626\n0.0276156\n+0.42\n\n\n\n\nlT: WD\n0.0010621\n0.0325896\n+0.26\n+0.38\n\n\nResidual\n\n0.0864769\n0.2940695\n\n\n\n\n\n\n\n\nshow(m_mrk17_crossed)\n\nLinear mixed model fit by maximum likelihood\n :(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + (1 + Q | subj) + zerocorr(0 + lT | subj) + zerocorr(1 + P | item)\n   logLik   -2 logLik     AIC       AICc        BIC    \n -3593.4086  7186.8171  7264.8171  7265.0077  7565.3349\n\nVariance components:\n            Column    Variance   Std.Dev.    Corr.\nitem     (Intercept)  0.00320567 0.05661866\n         P: rel       0.00006693 0.00818081   .  \nsubj     (Intercept)  0.03061757 0.17497878\n         Q: clr       0.00076248 0.02761311 +0.42\n         lT: WD       0.00106208 0.03258956   .     .  \nResidual              0.08640801 0.29395239\n Number of obs: 16409; levels of grouping factors: 240, 73\n\n  Fixed-effects parameters:\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                                                   Coef.  Std. Error      z  Pr(&gt;|z|)\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n(Intercept)                                  1.63743      0.0209303   78.23    &lt;1e-99\nF: HF                                        0.019366     0.00431955   4.48    &lt;1e-05\nP: rel                                       0.0188081    0.00236113   7.97    &lt;1e-14\nQ: clr                                       0.042901     0.00396828  10.81    &lt;1e-26\nlQ: clr                                      0.00174653   0.00231531   0.75    0.4506\nlT: WD                                       0.00836023   0.0044618    1.87    0.0610\nF: HF & P: rel                              -0.00711249   0.00236171  -3.01    0.0026\nF: HF & Q: clr                              -0.00141259   0.00230067  -0.61    0.5392\nP: rel & Q: clr                              0.00134142   0.00230239   0.58    0.5601\nF: HF & lQ: clr                             -0.00105227   0.00232139  -0.45    0.6503\nP: rel & lQ: clr                            -0.00240742   0.00232056  -1.04    0.2995\nQ: clr & lQ: clr                             0.00759804   0.00231922   3.28    0.0011\nF: HF & lT: WD                               0.000536556  0.00231365   0.23    0.8166\nP: rel & lT: WD                              2.93226e-5   0.00231751   0.01    0.9899\nQ: clr & lT: WD                              0.00185532   0.00231773   0.80    0.4234\nlQ: clr & lT: WD                            -0.00524632   0.00231608  -2.27    0.0235\nF: HF & P: rel & Q: clr                     -0.000362238  0.00230272  -0.16    0.8750\nF: HF & P: rel & lQ: clr                    -0.00117754   0.00232025  -0.51    0.6118\nF: HF & Q: clr & lQ: clr                     0.00273699   0.00232283   1.18    0.2387\nP: rel & Q: clr & lQ: clr                   -0.00393229   0.0023222   -1.69    0.0904\nF: HF & P: rel & lT: WD                      0.0019864    0.00231879   0.86    0.3916\nF: HF & Q: clr & lT: WD                     -0.00112586   0.00231657  -0.49    0.6270\nP: rel & Q: clr & lT: WD                     0.000261826  0.00231924   0.11    0.9101\nF: HF & lQ: clr & lT: WD                     0.00150677   0.00232198   0.65    0.5164\nP: rel & lQ: clr & lT: WD                    8.67965e-5   0.00232177   0.04    0.9702\nQ: clr & lQ: clr & lT: WD                    0.00916751   0.00231827   3.95    &lt;1e-04\nF: HF & P: rel & Q: clr & lQ: clr           -0.00234443   0.00231978  -1.01    0.3122\nF: HF & P: rel & Q: clr & lT: WD            -0.00148721   0.00232057  -0.64    0.5216\nF: HF & P: rel & lQ: clr & lT: WD            0.00308764   0.00232145   1.33    0.1835\nF: HF & Q: clr & lQ: clr & lT: WD            0.00393598   0.00232128   1.70    0.0900\nP: rel & Q: clr & lQ: clr & lT: WD           0.00202623   0.00232125   0.87    0.3827\nF: HF & P: rel & Q: clr & lQ: clr & lT: WD   0.00144888   0.00231838   0.62    0.5320\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\nFinally, a look at the fixed effects. The four-factor interaction reported in Masson & Kliegl (2013) was not replicated.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#nested-fixed-effects",
    "href": "mrk17.html#nested-fixed-effects",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "6.2 Nested fixed effects",
    "text": "6.2 Nested fixed effects\n\nm_mrk17_nested =let\n   form = @formula (1000/rt) ~ 1 + Q/(F/P) +\n        (1+Q | subj) + zerocorr(0+lT | subj) + zerocorr(1 + P | item) ;\n    fit(MixedModel, form, dat; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n(Intercept)\n1.6374\n0.0209\n78.20\n&lt;1e-99\n0.0565\n0.1751\n\n\nQ: clr\n0.0428\n0.0040\n10.74\n&lt;1e-26\n\n0.0278\n\n\nQ: deg & F: HF\n0.0209\n0.0049\n4.26\n&lt;1e-04\n\n\n\n\nQ: clr & F: HF\n0.0179\n0.0049\n3.65\n0.0003\n\n\n\n\nQ: deg & F: LF & P: rel\n0.0244\n0.0047\n5.18\n&lt;1e-06\n\n\n\n\nQ: clr & F: LF & P: rel\n0.0278\n0.0047\n5.94\n&lt;1e-08\n\n\n\n\nQ: deg & F: HF & P: rel\n0.0110\n0.0047\n2.35\n0.0186\n\n\n\n\nQ: clr & F: HF & P: rel\n0.0126\n0.0046\n2.71\n0.0067\n\n\n\n\nP: rel\n\n\n\n\n0.0086\n\n\n\nlT: WD\n\n\n\n\n\n0.0337\n\n\nResidual\n0.2944",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#nesting-within-products-of-factors",
    "href": "mrk17.html#nesting-within-products-of-factors",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "7.1 Nesting within products of factors",
    "text": "7.1 Nesting within products of factors\nInclude parenthesis\n\nm_mrk17_nested =let\n   form = @formula (1000/rt) ~ 1 + Q/(F/P) +\n        (1+Q | subj) + zerocorr(0+lT | subj) + zerocorr(1 + P | item) ;\n    fit(MixedModel, form, dat; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n(Intercept)\n1.6374\n0.0209\n78.20\n&lt;1e-99\n0.0565\n0.1751\n\n\nQ: clr\n0.0428\n0.0040\n10.74\n&lt;1e-26\n\n0.0278\n\n\nQ: deg & F: HF\n0.0209\n0.0049\n4.26\n&lt;1e-04\n\n\n\n\nQ: clr & F: HF\n0.0179\n0.0049\n3.65\n0.0003\n\n\n\n\nQ: deg & F: LF & P: rel\n0.0244\n0.0047\n5.18\n&lt;1e-06\n\n\n\n\nQ: clr & F: LF & P: rel\n0.0278\n0.0047\n5.94\n&lt;1e-08\n\n\n\n\nQ: deg & F: HF & P: rel\n0.0110\n0.0047\n2.35\n0.0186\n\n\n\n\nQ: clr & F: HF & P: rel\n0.0126\n0.0046\n2.71\n0.0067\n\n\n\n\nP: rel\n\n\n\n\n0.0086\n\n\n\nlT: WD\n\n\n\n\n\n0.0337\n\n\nResidual\n0.2944",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#selection-in-fixed-effects",
    "href": "mrk17.html#selection-in-fixed-effects",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "7.2 Selection in fixed effects",
    "text": "7.2 Selection in fixed effects\n\nusing RegressionFormulae\n# m_prm_5 is equivalent to m_prm\nm_prm_5 = let\n    form = @formula (1000/rt) ~ 1+(F+P+Q+lQ+lT)^5 + (1+Q+lT | subj) + (1 | item);\n    fit(MixedModel, form, dat; contrasts, progress)\nend;\n\nm_prm_4 = let\n    form = @formula (1000/rt) ~ 1+(F+P+Q+lQ+lT)^4 + (1+Q+lT | subj) + (1 | item);\n    fit(MixedModel, form, dat; contrasts, progress)\nend;\n\nm_prm_3 = let\n    form = @formula (1000/rt) ~ 1+(F+P+Q+lQ+lT)^3 + (1+Q+lT | subj) + (1 | item);\n    fit(MixedModel, form, dat; contrasts, progress)\nend;\n\nm_prm_2 = let\n    form = @formula (1000/rt) ~ 1+(F+P+Q+lQ+lT)^2 + (1+Q+lT | subj) + (1 | item);\n    fit(MixedModel, form, dat; contrasts, progress)\nend;\n\nm_prm_1 = let\n    form = @formula (1000/rt) ~ 1+ F+P+Q+lQ+lT +   (1+Q+lT | subj) + (1 | item);\n    fit(MixedModel, form, dat; contrasts, progress)\nend;\n\n# Compare the fits\ngof_summary = let\n  nms = [:m_prm_1, :m_prm_2, :m_prm_3, :m_prm_4, :m_prm_5]\n  mods = eval.(nms)\n  lrt = MixedModels.likelihoodratiotest(m_prm_1, m_prm_2, m_prm_3, m_prm_4, m_prm_5)\n  DataFrame(;\n    name = nms,\n    dof=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    AICc=aicc.(mods),\n    BIC=bic.(mods),\n    œá¬≤=vcat(:., round.(Int, diff(collect(lrt.lrt.deviance)))),\n    œá¬≤_dof=vcat(:., diff(collect(lrt.lrt.dof))),\n    # StatsBase.PValue includes some pretty-printing methods\n    pvalue=vcat(:., StatsBase.PValue.(collect(lrt.lrt.pval[2:end])))\n  )\nend\n\n5√ó9 DataFrame\n\n\n\nRow\nname\ndof\ndeviance\nAIC\nAICc\nBIC\nœá¬≤\nœá¬≤_dof\npvalue\n\n\n\nSymbol\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nAny\nAny\nAny\n\n\n\n\n1\nm_prm_1\n14\n7237.39\n7265.39\n7265.41\n7373.27\n.\n.\n.\n\n\n2\nm_prm_2\n24\n7209.2\n7257.2\n7257.27\n7442.13\n-28\n10\n0.0017\n\n\n3\nm_prm_3\n34\n7187.57\n7255.57\n7255.71\n7517.56\n-22\n10\n0.0171\n\n\n4\nm_prm_4\n39\n7180.61\n7258.61\n7258.8\n7559.13\n-7\n5\n0.2237\n\n\n5\nm_prm_5\n40\n7180.21\n7260.21\n7260.41\n7568.44\n0\n1\n0.5278\n\n\n\n\n\n\nDepending on the level of precision of your hypothesis. You could stay with main effect (BIC), include 2-factor interactions (AIC; also called simple interactions) or include 3-factor interactions [œá¬≤ &lt; 2*(œá¬≤-dof); also called 2-way interactions].",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#posthoc-lmm",
    "href": "mrk17.html#posthoc-lmm",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "7.3 Posthoc LMM",
    "text": "7.3 Posthoc LMM\nWe are using only three factors for the illustruation.\n\nm_prm3 = let\n    form = @formula (1000/rt) ~ 1 + lT*lQ*Q +\n                               (1+    Q+lT | subj) + (1 | item);\n    fit(MixedModel, form, dat; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n(Intercept)\n1.6376\n0.0210\n78.10\n&lt;1e-99\n0.0596\n0.1750\n\n\nlT: WD\n0.0087\n0.0045\n1.93\n0.0530\n\n0.0328\n\n\nlQ: clr\n0.0016\n0.0023\n0.70\n0.4818\n\n\n\n\nQ: clr\n0.0428\n0.0040\n10.78\n&lt;1e-26\n\n0.0276\n\n\nlT: WD & lQ: clr\n-0.0056\n0.0023\n-2.39\n0.0167\n\n\n\n\nlT: WD & Q: clr\n0.0020\n0.0023\n0.87\n0.3862\n\n\n\n\nlQ: clr & Q: clr\n0.0076\n0.0023\n3.26\n0.0011\n\n\n\n\nlT: WD & lQ: clr & Q: clr\n0.0092\n0.0023\n3.94\n&lt;1e-04\n\n\n\n\nResidual\n0.2949\n\n\n\n\n\n\n\n\n\n\nThe lT & lQ & Q interactions is significant. Let‚Äôs follow it up with a post-hoc LMM, that is looking at the lQ & Q interaction in the two levels of whether the last word was a target or not.\n\nm_prm3_posthoc = let\n    form = @formula (1000/rt) ~ 1 + lT/(lQ*Q) +\n                               (1+    Q+lT | subj) + (1 | item);\n    fit(MixedModel, form, dat; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n(Intercept)\n1.6376\n0.0210\n78.10\n&lt;1e-99\n0.0596\n0.1750\n\n\nlT: WD\n0.0087\n0.0045\n1.93\n0.0530\n\n0.0328\n\n\nlT: NW & lQ: clr\n0.0072\n0.0033\n2.19\n0.0285\n\n\n\n\nlT: WD & lQ: clr\n-0.0039\n0.0033\n-1.19\n0.2326\n\n\n\n\nlT: NW & Q: clr\n0.0408\n0.0046\n8.87\n&lt;1e-18\n\n\n\n\nlT: WD & Q: clr\n0.0448\n0.0046\n9.73\n&lt;1e-21\n\n\n\n\nlT: NW & lQ: clr & Q: clr\n-0.0016\n0.0033\n-0.49\n0.6275\n\n\n\n\nlT: WD & lQ: clr & Q: clr\n0.0167\n0.0033\n5.08\n&lt;1e-06\n\n\n\n\nQ: clr\n\n\n\n\n\n0.0276\n\n\nResidual\n0.2949\n\n\n\n\n\n\n\n\n\n\nThe source of the interaction are trials where the last trial was a word target; there is no evidence for the interaction when the last trial was a nonword target.\nThe original and post-hoc LMM have the same goodness of fit.\n\n[objective(m_prm3), objective(m_prm3_posthoc)]\n\n2-element Vector{Float64}:\n 7291.145437442885\n 7291.14543744171",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#info",
    "href": "mrk17.html#info",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "7.4 Info",
    "text": "7.4 Info\n\nversioninfo()\n\nJulia Version 1.12.3\nCommit 966d0af0fdf (2025-12-15 11:20 UTC)\nBuild Info:\n  Official https://julialang.org release\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 16 √ó Intel(R) Xeon(R) E-2288G CPU @ 3.70GHz\n  WORD_SIZE: 64\n  LLVM: libLLVM-18.1.7 (ORCJIT, skylake)\n  GC: Built with stock GC\nThreads: 16 default, 1 interactive, 16 GC (on 16 virtual cores)\nEnvironment:\n  JULIA_PROJECT = @.\n  JULIA_LOAD_PATH = @:@stdlib\n\n\nThis page was rendered from git revision fb99b3c\n using Quarto 1.7.33.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "kwdyz11.html",
    "href": "kwdyz11.html",
    "title": "RePsychLing Kliegl et al.¬†(2011)",
    "section": "",
    "text": "We take the kwdyz11 dataset (Kliegl et al., 2011) from an experiment looking at three effects of visual cueing under four different cue-target relations (CTRs). Two horizontal rectangles are displayed above and below a central fixation point or they displayed in vertical orientation to the left and right of the fixation point. Subjects react to the onset of a small visual target occurring at one of the four ends of the two rectangles. The target is cued validly on 70% of trials by a brief flash of the corner of the rectangle at which it appears; it is cued invalidly at the three other locations 10% of the trials each.\nWe specify three contrasts for the four-level factor CTR that are derived from spatial, object-based, and attractor-like features of attention. They map onto sequential differences between appropriately ordered factor levels. At the level of fixed effects, there is the noteworthy result, that the attraction effect was estimated at 2 ms, that is clearly not significant. Nevertheless, there was a highly reliable variance component (VC) estimated for this effect. Moreover, the reliable individual differences in the attraction effect were negatively correlated with those in the spatial effect.\nUnfortunately, a few years after the publication, we determined that the reported LMM is actually singular and that the singularity is linked to a theoretically critical correlation parameter (CP) between the spatial effect and the attraction effect. Fortunately, there is also a larger dataset kkl15 from a replication and extension of this study (Kliegl et al., 2015), analyzed with kkl15.jl notebook. The critical CP (along with other fixed effects and CPs) was replicated in this study.\nA more comprehensive analysis was reported in the parsimonious mixed-model paper (Bates et al., 2015). Data and R scripts are also available in R-package RePsychLing. In this and the complementary kkl15.jl scripts, we provide some corresponding analyses with MixedModels.jl.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#residual-over-fitted-plot",
    "href": "kwdyz11.html#residual-over-fitted-plot",
    "title": "RePsychLing Kliegl et al.¬†(2011)",
    "section": "5.1 Residual-over-fitted plot",
    "text": "5.1 Residual-over-fitted plot\nThe slant in residuals show a lower and upper boundary of reaction times, that is we have have too few short and too few long residuals. Not ideal, but at least width of the residual band looks similar across the fitted values, that is there is no evidence for heteroskedasticity.\n\n\nCode\nset_aog_theme!()\ndraw(\n  data((; f=fitted(m1), r=residuals(m1))) *\n  mapping(:f =&gt; \"Fitted values\", :r =&gt; \"Residual from model m1\") *\n  visual(Scatter);\n)\n\n\n\n\n\n\n\nFigure¬†3: Residuals versus the fitted values for model m1 of the log response time.\n\n\n\n\nWith many observations the scatterplot is not that informative. Contour plots or heatmaps may be an alternative.\n\n\nCode\ndraw(\n  data((; f=fitted(m1), r=residuals(m1))) *\n  mapping(\n    :f =&gt; \"Fitted log response time\", :r =&gt; \"Residual from model m1\"\n  ) *\n  density();\n)\n\n\n\n\n\n\n\nFigure¬†4: Heatmap of residuals versus fitted values for model m1",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#q-q-plot",
    "href": "kwdyz11.html#q-q-plot",
    "title": "RePsychLing Kliegl et al.¬†(2011)",
    "section": "5.2 Q-Q plot",
    "text": "5.2 Q-Q plot\nThe plot of quantiles of model residuals over corresponding quantiles of the normal distribution should yield a straight line along the main diagonal.\n\nqqnorm(residuals(m1); qqline=:none)",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#observed-and-theoretical-normal-distribution",
    "href": "kwdyz11.html#observed-and-theoretical-normal-distribution",
    "title": "RePsychLing Kliegl et al.¬†(2011)",
    "section": "5.3 Observed and theoretical normal distribution",
    "text": "5.3 Observed and theoretical normal distribution\nThe violation of expectation is again due to the fact that the distribution of residuals is much narrower than expected from a normal distribution, as shown in Figure¬†5. Overall, it does not look too bad.\n\n\nCode\nlet\n  n = nrow(dat)\n  dat_rz = DataFrame(;\n    value=vcat(residuals(m1) ./ std(residuals(m1)), randn(n)),\n    curve=vcat(fill.(\"residual\", n), fill.(\"normal\", n)),\n  )\n  draw(\n    data(dat_rz) *\n    mapping(:value =&gt; \"Standardized residuals\"; color=:curve) *\n    density(; bandwidth=0.1);\n  )\nend\n\n\n\n\n\n\n\nFigure¬†5: Kernel density plot of the standardized residuals from model m1 compared to a Gaussian",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#overlay",
    "href": "kwdyz11.html#overlay",
    "title": "RePsychLing Kliegl et al.¬†(2011)",
    "section": "6.1 Overlay",
    "text": "6.1 Overlay\nThe first plot overlays shrinkage-corrected conditional modes of the random effects with within-subject-based and pooled GMs and experimental effects.\nTo be done",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#caterpillar-plot",
    "href": "kwdyz11.html#caterpillar-plot",
    "title": "RePsychLing Kliegl et al.¬†(2011)",
    "section": "6.2 Caterpillar plot",
    "text": "6.2 Caterpillar plot\nThe caterpillar plot, Figure¬†6, also reveals the high correlation between spatial sod and attraction dod effects.\n\n\nCode\ncaterpillar!(\n  Figure(; resolution=(800, 1000)), ranefinfo(m1, :Subj); orderby=2\n)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†6: Prediction intervals on the random effects for Subj in model m1",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#shrinkage-plot",
    "href": "kwdyz11.html#shrinkage-plot",
    "title": "RePsychLing Kliegl et al.¬†(2011)",
    "section": "6.3 Shrinkage plot",
    "text": "6.3 Shrinkage plot\nFigure¬†7 provides more evidence for a problem with the visualization of the spatial sod and attraction dod CP. The corresponding panel illustrates an implosion of conditional modes.\n\n\nCode\nshrinkageplot!(Figure(; resolution=(1000, 1000)), m1)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†7: Shrinkage plot of the conditional means of the random effects for model m1",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#generate-a-bootstrap-sample",
    "href": "kwdyz11.html#generate-a-bootstrap-sample",
    "title": "RePsychLing Kliegl et al.¬†(2011)",
    "section": "7.1 Generate a bootstrap sample",
    "text": "7.1 Generate a bootstrap sample\nWe generate 2500 samples for the 15 model parameters (4 fixed effect, 4 VCs, 6 CPs, and 1 residual).\n\n\nCode\nRandom.seed!(1234321)\nsamp = parametricbootstrap(2500, m1)\ntbl = samp.tbl\n\n\nTable with 26 columns and 2500 rows:\n      obj       Œ≤1       Œ≤2         Œ≤3         Œ≤4           œÉ         ‚ãØ\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n 1  ‚îÇ -12780.3  5.92882  0.0903291  0.0414832  -0.00664838  0.191959  ‚ãØ\n 2  ‚îÇ -12744.1  5.94081  0.0983995  0.0320538  -0.00301854  0.192152  ‚ãØ\n 3  ‚îÇ -13008.0  5.91439  0.0812454  0.0462893  -0.0131144   0.191086  ‚ãØ\n 4  ‚îÇ -12877.3  5.94958  0.0986854  0.0354056  -0.0164526   0.19162   ‚ãØ\n 5  ‚îÇ -12618.7  5.96242  0.0953758  0.0426122  -0.0024504   0.192605  ‚ãØ\n 6  ‚îÇ -12930.3  5.95782  0.0880451  0.0286956  -0.0113573   0.191552  ‚ãØ\n 7  ‚îÇ -12419.5  5.96508  0.0997066  0.0282138  -0.00930178  0.193267  ‚ãØ\n 8  ‚îÇ -12448.6  5.91907  0.0972766  0.0413664  -0.00679021  0.193108  ‚ãØ\n 9  ‚îÇ -12901.8  5.94039  0.0776488  0.0287678  -0.0197365   0.191522  ‚ãØ\n 10 ‚îÇ -12902.2  5.92652  0.0720784  0.0389607  -0.0076042   0.191632  ‚ãØ\n 11 ‚îÇ -12745.9  5.92423  0.0884694  0.0381242  -0.00675381  0.192067  ‚ãØ\n 12 ‚îÇ -13014.2  5.93067  0.0938317  0.0332705  0.00494861   0.191092  ‚ãØ\n 13 ‚îÇ -13249.4  5.88752  0.0882907  0.0390491  -0.00701633  0.190306  ‚ãØ\n 14 ‚îÇ -12511.7  5.91759  0.0708118  0.0457366  -0.0148541   0.192887  ‚ãØ\n 15 ‚îÇ -13041.7  5.9563   0.0953964  0.0357251  0.00111446   0.191246  ‚ãØ\n 16 ‚îÇ -12322.5  5.9297   0.0857692  0.0348357  -0.0161301   0.193445  ‚ãØ\n 17 ‚îÇ -13004.4  5.95912  0.0946321  0.0371024  -0.013721    0.191294  ‚ãØ\n ‚ãÆ  ‚îÇ    ‚ãÆ         ‚ãÆ         ‚ãÆ          ‚ãÆ           ‚ãÆ          ‚ãÆ      ‚ã±",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#shortest-coverage-interval",
    "href": "kwdyz11.html#shortest-coverage-interval",
    "title": "RePsychLing Kliegl et al.¬†(2011)",
    "section": "7.2 Shortest coverage interval",
    "text": "7.2 Shortest coverage interval\nThe upper limit of the interval for the critical CP CTR: sod, CTR: dod is hitting the upper wall of a perfect correlation. This is evidence of singularity. The other intervals do not exhibit such pathologies; they appear to be ok.\n\n\nCode\nconfint(samp)\n\n\nDictTable with 2 columns and 15 rows:\n par   lower       upper\n ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n Œ≤1  ‚îÇ 5.89946     5.97084\n Œ≤2  ‚îÇ 0.0708118   0.102437\n Œ≤3  ‚îÇ 0.0246617   0.0484438\n Œ≤4  ‚îÇ -0.0205562  0.00220715\n œÅ1  ‚îÇ 0.240039    0.684207\n œÅ2  ‚îÇ -0.755291   0.275447\n œÅ3  ‚îÇ -0.66473    0.456656\n œÅ4  ‚îÇ -0.196123   0.703365\n œÅ5  ‚îÇ 0.590554    0.999995\n œÅ6  ‚îÇ -0.892445   0.395364\n œÉ   ‚îÇ 0.190506    0.193579\n œÉ1  ‚îÇ 0.118876    0.169078\n œÉ2  ‚îÇ 0.0449739   0.0710317\n œÉ3  ‚îÇ 0.00970158  0.0400425\n œÉ4  ‚îÇ 0.014377    0.0383545",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#comparative-density-plots-of-bootstrapped-parameter-estimates",
    "href": "kwdyz11.html#comparative-density-plots-of-bootstrapped-parameter-estimates",
    "title": "RePsychLing Kliegl et al.¬†(2011)",
    "section": "7.3 Comparative density plots of bootstrapped parameter estimates",
    "text": "7.3 Comparative density plots of bootstrapped parameter estimates",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#residual",
    "href": "kwdyz11.html#residual",
    "title": "RePsychLing Kliegl et al.¬†(2011)",
    "section": "7.4 Residual",
    "text": "7.4 Residual\n\n\nCode\ndraw(\n  data(tbl) *\n  mapping(:œÉ =&gt; \"Residual standard deviation\") *\n  density();\n)\n\n\n\n\n\n\n\nFigure¬†8",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#fixed-effects-wo-gm",
    "href": "kwdyz11.html#fixed-effects-wo-gm",
    "title": "RePsychLing Kliegl et al.¬†(2011)",
    "section": "7.5 Fixed effects (w/o GM)",
    "text": "7.5 Fixed effects (w/o GM)\nThe shortest coverage interval for the GM ranges from 376 to 404 ms. To keep the plot range small we do not include its density here.\n\n\nCode\nlabels = [\n  \"CTR: sod\" =&gt; \"spatial effect\",\n  \"CTR: dos\" =&gt; \"object effect\",\n  \"CTR: dod\" =&gt; \"attraction effect\",\n  \"(Intercept)\" =&gt; \"grand mean\",\n]\ndraw(\n  data(tbl) *\n  mapping(\n    [:Œ≤2, :Œ≤3, :Œ≤4] .=&gt; \"Experimental effect size [ms]\";\n    color=dims(1) =&gt; renamer([\"spatial\", \"object\", \"attraction\"] .* \" effect\") =&gt;\n    \"Experimental effects\",\n  ) *\n  density();\n)\n\n\n\n\n\n\n\nFigure¬†9: Comparative density plots of the fixed-effects parameters for model m1\n\n\n\n\nThe densitiies correspond nicely with the shortest coverage intervals.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#variance-components-vcs",
    "href": "kwdyz11.html#variance-components-vcs",
    "title": "RePsychLing Kliegl et al.¬†(2011)",
    "section": "7.6 Variance components (VCs)",
    "text": "7.6 Variance components (VCs)\n\n\nCode\ndraw(\n  data(tbl) *\n  mapping(\n    [:œÉ1, :œÉ2, :œÉ3, :œÉ4] .=&gt; \"Standard deviations [ms]\";\n    color=dims(1) =&gt;\n    renamer(append!([\"Grand mean\"],[\"spatial\", \"object\", \"attraction\"] .* \" effect\")) =&gt;\n    \"Variance components\",\n  ) *\n  density();\n)\n\n\n\n\n\n\n\nFigure¬†10: Comparative density plots of the variance components for model m1\n\n\n\n\nThe VC are all very nicely defined.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#correlation-parameters-cps",
    "href": "kwdyz11.html#correlation-parameters-cps",
    "title": "RePsychLing Kliegl et al.¬†(2011)",
    "section": "7.7 Correlation parameters (CPs)",
    "text": "7.7 Correlation parameters (CPs)\n\n\nCode\nlet\n  labels = [\n    \"(Intercept), CTR: sod\" =&gt; \"GM, spatial\",\n    \"(Intercept), CTR: dos\" =&gt; \"GM, object\",\n    \"CTR: sod, CTR: dos\" =&gt; \"spatial, object\",\n    \"(Intercept), CTR: dod\" =&gt; \"GM, attraction\",\n    \"CTR: sod, CTR: dod\" =&gt; \"spatial, attraction\",\n    \"CTR: dos, CTR: dod\" =&gt; \"object, attraction\",\n  ]\n  draw(\n    data(tbl) *\n    mapping(\n      [:œÅ1, :œÅ2, :œÅ3, :œÅ4, :œÅ5, :œÅ6] .=&gt; \"Correlation\";\n      color=dims(1) =&gt; renamer(last.(labels)) =&gt; \"Correlation parameters\",\n    ) *\n    density();\n  )\nend\n\n\n\n\n\n\n\nFigure¬†11: Comparative density plots of the correlation parameters for model m1\n\n\n\n\nTwo of the CPs stand out positively. First, the correlation between GM and the spatial effect is well defined. Second, as discussed throughout this script, the CP between spatial and attraction effect is close to the 1.0 border and clearly not well defined. Therefore, this CP will be replicated with a larger sample in script kkl15.jl (Kliegl et al., 2015).",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kb07.html",
    "href": "kb07.html",
    "title": "Bootstrapping a fitted model",
    "section": "",
    "text": "Begin by loading the packages to be used.\n\n\nCode\nusing AlgebraOfGraphics\nusing CairoMakie\nusing DataFrames\nusing MixedModels\nusing Random\nusing SMLP2026: dataset\n\nconst progress=false\n\n\n\n1 Data set and model\nThe kb07 data (Kronm√ºller & Barr, 2007) are one of the datasets provided by the MixedModels package.\n\nkb07 = dataset(:kb07)\n\nArrow.Table with 1789 rows, 7 columns, and schema:\n :subj      String\n :item      String\n :spkr      String\n :prec      String\n :load      String\n :rt_trunc  Int16\n :rt_raw    Int16\n\n\nConvert the table to a DataFrame for summary.\n\ndescribe(DataFrame(kb07))\n\n7√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nDataType\n\n\n\n\n1\nsubj\n\nS030\n\nS103\n0\nString\n\n\n2\nitem\n\nI01\n\nI32\n0\nString\n\n\n3\nspkr\n\nnew\n\nold\n0\nString\n\n\n4\nprec\n\nbreak\n\nmaintain\n0\nString\n\n\n5\nload\n\nno\n\nyes\n0\nString\n\n\n6\nrt_trunc\n2182.2\n579\n1940.0\n5171\n0\nInt16\n\n\n7\nrt_raw\n2226.24\n579\n1940.0\n15923\n0\nInt16\n\n\n\n\n\n\nThe experimental factors; spkr, prec, and load, are two-level factors. The EffectsCoding contrast is used with these to create a \\(\\pm1\\) encoding.\n\ncontrasts = Dict{Symbol,Any}(nm =&gt; EffectsCoding() for nm in (:spkr, :prec, :load))\n\nThe display of an initial model fit\n\nkbm01 = let\n  form = @formula(\n    rt_trunc ~\n      1 +\n      spkr * prec * load +\n      (1 + spkr + prec + load | subj) +\n      (1 + spkr + prec + load | item)\n  )\n  fit(MixedModel, form, kb07; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\nœÉ_item\n\n\n(Intercept)\n2181.6728\n77.3025\n28.22\n&lt;1e-99\n301.7795\n362.1969\n\n\nspkr: old\n67.7486\n18.2889\n3.70\n0.0002\n42.9237\n40.6934\n\n\nprec: maintain\n-333.9211\n47.1525\n-7.08\n&lt;1e-11\n61.9966\n246.8936\n\n\nload: yes\n78.7703\n19.5367\n4.03\n&lt;1e-04\n65.1301\n42.3692\n\n\nspkr: old & prec: maintain\n-21.9656\n15.8062\n-1.39\n0.1646\n\n\n\n\nspkr: old & load: yes\n18.3842\n15.8062\n1.16\n0.2448\n\n\n\n\nprec: maintain & load: yes\n4.5338\n15.8062\n0.29\n0.7742\n\n\n\n\nspkr: old & prec: maintain & load: yes\n23.6074\n15.8062\n1.49\n0.1353\n\n\n\n\nResidual\n668.5033\n\n\n\n\n\n\n\n\n\n\ndoes not include the estimated correlations of the random effects.\nThe VarCorr extractor displays these.\n\nVarCorr(kbm01)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nsubj\n(Intercept)\n91070.8712\n301.7795\n\n\n\n\n\n\nspkr: old\n1842.4411\n42.9237\n+0.78\n\n\n\n\n\nprec: maintain\n3843.5766\n61.9966\n-0.59\n+0.03\n\n\n\n\nload: yes\n4241.9308\n65.1301\n+0.36\n+0.83\n+0.53\n\n\nitem\n(Intercept)\n131186.5676\n362.1969\n\n\n\n\n\n\nspkr: old\n1655.9495\n40.6934\n+0.44\n\n\n\n\n\nprec: maintain\n60956.4434\n246.8936\n-0.69\n+0.35\n\n\n\n\nload: yes\n1795.1488\n42.3692\n+0.32\n+0.16\n-0.14\n\n\nResidual\n\n446896.6953\n668.5033\n\n\n\n\n\n\n\n\nNone of the two-factor or three-factor interaction terms in the fixed-effects are significant. In the random-effects terms only the scalar random effects and the prec random effect for item appear to be warranted, leading to the reduced formula\n\nkbm02 = let\n  form = @formula(\n    rt_trunc ~\n      1 + spkr + prec + load + (1 | subj) + (1 + prec | item)\n  )\n  fit(MixedModel, form, kb07; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n(Intercept)\n2181.8526\n77.4680\n28.16\n&lt;1e-99\n364.7121\n298.0257\n\n\nspkr: old\n67.8790\n16.0785\n4.22\n&lt;1e-04\n\n\n\n\nprec: maintain\n-333.7906\n47.4476\n-7.03\n&lt;1e-11\n252.5236\n\n\n\nload: yes\n78.5904\n16.0785\n4.89\n&lt;1e-05\n\n\n\n\nResidual\n680.0318\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(kbm02)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\nitem\n(Intercept)\n133014.901\n364.712\n\n\n\n\nprec: maintain\n63768.182\n252.524\n-0.70\n\n\nsubj\n(Intercept)\n88819.305\n298.026\n\n\n\nResidual\n\n462443.261\n680.032\n\n\n\n\n\n\nThese two models are nested and can be compared with a likelihood-ratio test.\n\nMixedModels.likelihoodratiotest(kbm02, kbm01)\n\n\n\n\n\nmodel-dof\n-2 logLik\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\nrt_trunc ~ 1 + spkr + prec + load + (1 | subj) + (1 + prec | item)\n9\n-28664\n\n\n\n\n\nrt_trunc ~ 1 + spkr + prec + load + spkr & prec + spkr & load + prec & load + spkr & prec & load + (1 + spkr + prec + load | subj) + (1 + spkr + prec + load | item)\n29\n-28637\n27\n20\n0.1431\n\n\n\n\n\nThe p-value of approximately 17% leads us to prefer the simpler model, kbm02, to the more complex, kbm01.\n\n\n2 A bootstrap sample\nCreate a bootstrap sample of a few thousand parameter estimates from the reduced model. The pseudo-random number generator is initialized to a fixed value for reproducibility.\n\nRandom.seed!(1234321)\nkbm02samp = parametricbootstrap(2000, kbm02)\nkbm02tbl = kbm02samp.tbl\n\nTable with 14 columns and 2000 rows:\n      obj      Œ≤1       Œ≤2       Œ≤3        Œ≤4       œÉ        œÉ1       œÉ2       ‚ãØ\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n 1  ‚îÇ 28772.3  2163.91  60.5609  -312.083  53.8854  702.338  372.786  244.83   ‚ãØ\n 2  ‚îÇ 28715.1  2232.6   83.7997  -383.283  47.4501  693.167  391.838  214.006  ‚ãØ\n 3  ‚îÇ 28575.3  2194.67  65.0581  -348.838  78.2132  660.998  380.117  241.471  ‚ãØ\n 4  ‚îÇ 28728.5  2261.27  66.174   -351.407  89.6083  690.344  334.602  306.404  ‚ãØ\n 5  ‚îÇ 28631.4  2309.15  77.5687  -341.306  72.5969  676.497  323.765  269.472  ‚ãØ\n 6  ‚îÇ 28627.8  2091.9   54.8119  -295.505  74.487   667.146  426.799  278.075  ‚ãØ\n 7  ‚îÇ 28685.7  2291.62  73.7219  -396.565  84.4817  687.627  393.807  194.634  ‚ãØ\n 8  ‚îÇ 28654.4  2019.04  72.7075  -203.805  87.8736  679.825  304.365  208.357  ‚ãØ\n 9  ‚îÇ 28664.1  1999.72  60.8485  -308.63   74.0233  680.412  357.133  305.426  ‚ãØ\n 10 ‚îÇ 28618.3  2249.85  52.1815  -394.464  89.8091  670.656  385.055  226.33   ‚ãØ\n 11 ‚îÇ 28748.9  2166.95  77.9659  -326.597  81.0316  696.841  378.973  234.483  ‚ãØ\n 12 ‚îÇ 28731.4  2153.68  74.5144  -341.394  44.1759  701.902  323.252  177.604  ‚ãØ\n 13 ‚îÇ 28623.9  2155.23  93.193   -373.268  73.2941  671.256  335.986  217.315  ‚ãØ\n 14 ‚îÇ 28615.9  2344.1   62.8553  -314.684  88.7448  665.313  410.652  259.44   ‚ãØ\n 15 ‚îÇ 28636.2  2286.5   35.9889  -349.732  62.0524  670.473  361.643  304.034  ‚ãØ\n 16 ‚îÇ 28676.6  2289.43  76.9805  -298.764  87.139   686.466  316.202  232.855  ‚ãØ\n 17 ‚îÇ 28614.8  2044.53  53.2336  -304.859  112.995  675.065  354.35   220.039  ‚ãØ\n ‚ãÆ  ‚îÇ    ‚ãÆ        ‚ãÆ        ‚ãÆ        ‚ãÆ         ‚ãÆ        ‚ãÆ        ‚ãÆ        ‚ãÆ     ‚ã±\n\n\nOne of the uses of such a sample is to form ‚Äúconfidence intervals‚Äù on the parameters by obtaining the shortest interval that covers a given proportion (95%, by default) of the sample.\n\nconfint(kbm02samp)\n\nDictTable with 2 columns and 9 rows:\n par   lower      upper\n ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n Œ≤1  ‚îÇ 2022.91    2334.0\n Œ≤2  ‚îÇ 34.0592    96.72\n Œ≤3  ‚îÇ -430.239   -239.802\n Œ≤4  ‚îÇ 46.5349    109.526\n œÅ1  ‚îÇ -0.907255  -0.490079\n œÉ   ‚îÇ 657.341    702.655\n œÉ1  ‚îÇ 270.065    451.957\n œÉ2  ‚îÇ 181.74     325.13\n œÉ3  ‚îÇ 233.835    364.504\n\n\nA sample like this can be used for more than just creating an interval because it approximates the distribution of the estimator. For the fixed-effects parameters the estimators are close to being normally distributed, Figure¬†1.\n\n\nCode\ndraw(\n  data(kbm02samp.Œ≤) * mapping(:Œ≤; color=:coefname) * AlgebraOfGraphics.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†1: Comparative densities of the fixed-effects coefficients in kbm02samp\n\n\n\n\n\n\nCode\nlet pars = [\"œÉ1\", \"œÉ2\", \"œÉ3\"]\n  draw(\n    data(kbm02tbl) *\n    mapping(pars .=&gt; \"œÉ\"; color=dims(1) =&gt; renamer(pars)) *\n    AlgebraOfGraphics.density();\n    figure=(; resolution=(800, 450)),\n  )\nend\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†2: Density plot of bootstrap samples standard deviation of random effects\n\n\n\n\n\n\nCode\ndraw(\n  data(kbm02tbl) *\n  mapping(:œÅ1 =&gt; \"Correlation\") *\n  AlgebraOfGraphics.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†3: Density plot of correlation parameters in bootstrap sample from model kbm02\n\n\n\n\n\n\n3 References\n\n\nKronm√ºller, E., & Barr, D. J. (2007). Perspective-free pragmatics: Broken precedents and the recovery-from-preemption hypothesis. Journal of Memory and Language, 56(3), 436‚Äì455. https://doi.org/10.1016/j.jml.2006.05.002\n\n\nThis page was rendered from git revision fb99b3c\n using Quarto 1.7.33.\n\n\n\n\n Back to top",
    "crumbs": [
      "Bootstrap and profiling",
      "Bootstrapping a fitted model"
    ]
  },
  {
    "objectID": "glmm.html",
    "href": "glmm.html",
    "title": "Generalized linear mixed models",
    "section": "",
    "text": "Load the packages to be used\nCode\nusing AlgebraOfGraphics\nusing CairoMakie\nusing DataFrameMacros\nusing DataFrames\nusing MixedModels\nusing MixedModelsMakie\nusing SMLP2026: dataset\n\nconst progress=false",
    "crumbs": [
      "Generalized linear mixed models"
    ]
  },
  {
    "objectID": "glmm.html#matrix-notation-for-the-sleepstudy-model",
    "href": "glmm.html#matrix-notation-for-the-sleepstudy-model",
    "title": "Generalized linear mixed models",
    "section": "1 Matrix notation for the sleepstudy model",
    "text": "1 Matrix notation for the sleepstudy model\n\nsleepstudy = DataFrame(dataset(:sleepstudy))\n\n180√ó3 DataFrame155 rows omitted\n\n\n\nRow\nsubj\ndays\nreaction\n\n\n\nString\nInt8\nFloat64\n\n\n\n\n1\nS308\n0\n249.56\n\n\n2\nS308\n1\n258.705\n\n\n3\nS308\n2\n250.801\n\n\n4\nS308\n3\n321.44\n\n\n5\nS308\n4\n356.852\n\n\n6\nS308\n5\n414.69\n\n\n7\nS308\n6\n382.204\n\n\n8\nS308\n7\n290.149\n\n\n9\nS308\n8\n430.585\n\n\n10\nS308\n9\n466.353\n\n\n11\nS309\n0\n222.734\n\n\n12\nS309\n1\n205.266\n\n\n13\nS309\n2\n202.978\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n169\nS371\n8\n350.781\n\n\n170\nS371\n9\n369.469\n\n\n171\nS372\n0\n269.412\n\n\n172\nS372\n1\n273.474\n\n\n173\nS372\n2\n297.597\n\n\n174\nS372\n3\n310.632\n\n\n175\nS372\n4\n287.173\n\n\n176\nS372\n5\n329.608\n\n\n177\nS372\n6\n334.482\n\n\n178\nS372\n7\n343.22\n\n\n179\nS372\n8\n369.142\n\n\n180\nS372\n9\n364.124\n\n\n\n\n\n\n\ncontrasts = Dict(:subj =&gt; Grouping())\nm1 = let f = @formula reaction ~ 1 + days + (1 + days | subj)\n  fit(MixedModel, f, sleepstudy; contrasts, progress)\nend\nprintln(m1)\n\nLinear mixed model fit by maximum likelihood\n reaction ~ 1 + days + (1 + days | subj)\n   logLik   -2 logLik     AIC       AICc        BIC    \n  -875.9697  1751.9393  1763.9393  1764.4249  1783.0971\n\nVariance components:\n            Column    Variance Std.Dev.   Corr.\nsubj     (Intercept)  565.52071 23.78068\n         days          32.68242  5.71685 +0.08\nResidual              654.94015 25.59180\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                Coef.  Std. Error      z  Pr(&gt;|z|)\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n(Intercept)  251.405      6.6323   37.91    &lt;1e-99\ndays          10.4673     1.50224   6.97    &lt;1e-11\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\nThe response vector, y, has 180 elements. The fixed-effects coefficient vector, Œ≤, has 2 elements and the fixed-effects model matrix, X, is of size 180 √ó 2.\n\nm1.y\n\n180-element view(::Matrix{Float64}, :, 3) with eltype Float64:\n 249.56\n 258.7047\n 250.8006\n 321.4398\n 356.8519\n 414.6901\n 382.2038\n 290.1486\n 430.5853\n 466.3535\n   ‚ãÆ\n 273.474\n 297.5968\n 310.6316\n 287.1726\n 329.6076\n 334.4818\n 343.2199\n 369.1417\n 364.1236\n\n\n\nm1.Œ≤\n\n2-element Vector{Float64}:\n 251.40510484848386\n  10.467285959596222\n\n\n\nm1.X\n\n180√ó2 Matrix{Float64}:\n 1.0  0.0\n 1.0  1.0\n 1.0  2.0\n 1.0  3.0\n 1.0  4.0\n 1.0  5.0\n 1.0  6.0\n 1.0  7.0\n 1.0  8.0\n 1.0  9.0\n ‚ãÆ    \n 1.0  1.0\n 1.0  2.0\n 1.0  3.0\n 1.0  4.0\n 1.0  5.0\n 1.0  6.0\n 1.0  7.0\n 1.0  8.0\n 1.0  9.0\n\n\nThe second column of X is just the days vector and the first column is all 1‚Äôs.\nThere are 36 random effects, 2 for each of the 18 levels of subj. The ‚Äúestimates‚Äù (technically, the conditional means or conditional modes) are returned as a vector of matrices, one matrix for each grouping factor. In this case there is only one grouping factor for the random effects so there is one one matrix which contains 18 intercept random effects and 18 slope random effects.\n\nm1.b\n\n1-element Vector{Matrix{Float64}}:\n [2.8155742217547144 -40.04853513531905 ‚Ä¶ 0.7232972001480724 12.118974830479594; 9.075555065119188 -8.644061172833373 ‚Ä¶ -0.9710589056249114 1.3106856585650282]\n\n\n\nonly(m1.b)   # only one grouping factor\n\n2√ó18 Matrix{Float64}:\n 2.81557  -40.0485   -38.4332   22.8324   ‚Ä¶  -24.7105    0.723297  12.119\n 9.07556   -8.64406   -5.51337  -4.65878       4.65976  -0.971059   1.31069\n\n\nThere is a model matrix, Z, for the random effects. In general it has one chunk of columns for the first grouping factor, a chunk of columns for the second grouping factor, etc.\nIn this case there is only one grouping factor.\n\nInt.(first(m1.reterms))\n\n180√ó36 Matrix{Int64}:\n 1  0  0  0  0  0  0  0  0  0  0  0  0  ‚Ä¶  0  0  0  0  0  0  0  0  0  0  0  0\n 1  1  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  2  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  3  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  4  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  5  0  0  0  0  0  0  0  0  0  0  0  ‚Ä¶  0  0  0  0  0  0  0  0  0  0  0  0\n 1  6  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  7  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  8  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  9  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n ‚ãÆ              ‚ãÆ              ‚ãÆ        ‚ã±     ‚ãÆ              ‚ãÆ              ‚ãÆ\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  2\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  3\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  4\n 0  0  0  0  0  0  0  0  0  0  0  0  0  ‚Ä¶  0  0  0  0  0  0  0  0  0  0  1  5\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  6\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  7\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  8\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  9\n\n\nThe defining property of a linear model or linear mixed model is that the fitted values are linear combinations of the fixed-effects parameters and the random effects. We can write the fitted values as\n\nm1.X * m1.Œ≤ + only(m1.reterms) * vec(only(m1.b))\n\n180-element Vector{Float64}:\n 254.22067907023856\n 273.76352009495395\n 293.30636111966936\n 312.84920214438483\n 332.39204316910025\n 351.9348841938156\n 371.477725218531\n 391.02056624324644\n 410.5634072679618\n 430.10624829267726\n   ‚ãÆ\n 275.3020512971247\n 287.08002291528595\n 298.85799453344725\n 310.63596615160844\n 322.4139377697697\n 334.19190938793093\n 345.9698810060922\n 357.7478526242534\n 369.5258242424147\n\n\n\nfitted(m1)   # just to check that these are indeed the same as calculated above\n\n180-element Vector{Float64}:\n 254.22067907023856\n 273.763520094954\n 293.30636111966936\n 312.84920214438483\n 332.39204316910025\n 351.93488419381566\n 371.477725218531\n 391.02056624324644\n 410.56340726796185\n 430.10624829267726\n   ‚ãÆ\n 275.3020512971247\n 287.08002291528595\n 298.85799453344725\n 310.6359661516085\n 322.4139377697697\n 334.19190938793093\n 345.9698810060922\n 357.7478526242534\n 369.5258242424147\n\n\nIn symbols we would write the linear predictor expression as \\[\n\\boldsymbol{\\eta} = \\mathbf{X}\\boldsymbol{\\beta} +\\mathbf{Z b}\n\\] where \\(\\boldsymbol{\\eta}\\) has 180 elements, \\(\\boldsymbol{\\beta}\\) has 2 elements, \\(\\bf b\\) has 36 elements, \\(\\bf X\\) is of size 180 √ó 2 and \\(\\bf Z\\) is of size 180 √ó 36.\nFor a linear model or linear mixed model the linear predictor is the mean response, \\(\\boldsymbol\\mu\\). That is, we can write the probability model in terms of a 180-dimensional random variable, \\(\\mathcal Y\\), for the response and a 36-dimensional random variable, \\(\\mathcal B\\), for the random effects as \\[\n\\begin{aligned}\n(\\mathcal{Y} | \\mathcal{B}=\\bf{b}) &\\sim\\mathcal{N}(\\bf{ X\\boldsymbol\\beta + Z b},\\sigma^2\\bf{I})\\\\\\\\\n\\mathcal{B}&\\sim\\mathcal{N}(\\bf{0},\\boldsymbol{\\Sigma}_{\\boldsymbol\\theta}) .\n\\end{aligned}\n\\] where \\(\\boldsymbol{\\Sigma}_\\boldsymbol{\\theta}\\) is a 36 √ó 36 symmetric covariance matrix that has a special form - it consists of 18 diagonal blocks, each of size 2 √ó 2 and all the same.\nRecall that this symmetric matrix can be constructed from the parameters \\(\\boldsymbol\\theta\\), which generate the lower triangular matrix \\(\\boldsymbol\\lambda\\), and the estimate \\(\\widehat{\\sigma^2}\\).\n\nm1.Œ∏\n\n3-element Vector{Float64}:\n 0.9292304864356652\n 0.018164371001158423\n 0.22264644003494607\n\n\n\nŒª = only(m1.Œª)  # with multiple grouping factors there will be multiple Œª's\n\n2√ó2 LinearAlgebra.LowerTriangular{Float64, Matrix{Float64}}:\n 0.92923     ‚ãÖ \n 0.0181644  0.222646\n\n\n\nŒ£ = varest(m1) * (Œª * Œª')\n\n2√ó2 Matrix{Float64}:\n 565.521   11.0547\n  11.0547  32.6824\n\n\nCompare the diagonal elements to the Variance column of\n\nVarCorr(m1)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\nsubj\n(Intercept)\n565.52071\n23.78068\n\n\n\n\ndays\n32.68242\n5.71685\n+0.08\n\n\nResidual\n\n654.94015\n25.59180",
    "crumbs": [
      "Generalized linear mixed models"
    ]
  },
  {
    "objectID": "glmm.html#linear-predictors-in-lmms-and-glmms",
    "href": "glmm.html#linear-predictors-in-lmms-and-glmms",
    "title": "Generalized linear mixed models",
    "section": "2 Linear predictors in LMMs and GLMMs",
    "text": "2 Linear predictors in LMMs and GLMMs\nWriting the model for \\(\\mathcal Y\\) as \\[\n(\\mathcal{Y} | \\mathcal{B}=\\bf{b})\\sim\\mathcal{N}(\\bf{ X\\boldsymbol\\beta + Z b},\\sigma^2\\bf{I})\n\\] may seem like over-mathematization (or ‚Äúoverkill‚Äù, if you prefer) relative to expressions like \\[\ny_i = \\beta_1 x_{i,1} + \\beta_2 x_{i,2}+ b_1 z_{i,1} +\\dots+b_{36} z_{i,36}+\\epsilon_i\n\\] but this more abstract form is necessary for generalizations.\nThe way that I read the first form is\n\n\n\n\n\n\nThe conditional distribution of the response vector, \\(\\mathcal Y\\), given that the random effects vector, \\(\\mathcal B =\\bf b\\), is a multivariate normal (or Gaussian) distribution whose mean, \\(\\boldsymbol\\mu\\), is the linear predictor, \\(\\boldsymbol\\eta=\\bf{X\\boldsymbol\\beta+Zb}\\), and whose covariance matrix is \\(\\sigma^2\\bf I\\). That is, conditional on \\(\\bf b\\), the elements of \\(\\mathcal Y\\) are independent normal random variables with constant variance, \\(\\sigma^2\\), and means of the form \\(\\boldsymbol\\mu = \\boldsymbol\\eta = \\bf{X\\boldsymbol\\beta+Zb}\\).\n\n\n\nSo the only things that differ in the distributions of the \\(y_i\\)‚Äôs are the means and they are determined by this linear predictor, \\(\\boldsymbol\\eta = \\bf{X\\boldsymbol\\beta+Zb}\\).",
    "crumbs": [
      "Generalized linear mixed models"
    ]
  },
  {
    "objectID": "glmm.html#generalized-linear-mixed-models",
    "href": "glmm.html#generalized-linear-mixed-models",
    "title": "Generalized linear mixed models",
    "section": "3 Generalized Linear Mixed Models",
    "text": "3 Generalized Linear Mixed Models\nConsider first a GLMM for a vector, \\(\\bf y\\), of binary (i.e.¬†yes/no) responses. The probability model for the conditional distribution \\(\\mathcal Y|\\mathcal B=\\bf b\\) consists of independent Bernoulli distributions where the mean, \\(\\mu_i\\), for the i‚Äôth response is again determined by the i‚Äôth element of a linear predictor, \\(\\boldsymbol\\eta = \\mathbf{X}\\boldsymbol\\beta+\\mathbf{Z b}\\).\nHowever, in this case we will run into trouble if we try to make \\(\\boldsymbol\\mu=\\boldsymbol\\eta\\) because \\(\\mu_i\\) is the probability of ‚Äúsuccess‚Äù for the i‚Äôth response and must be between 0 and 1. We can‚Äôt guarantee that the i‚Äôth component of \\(\\boldsymbol\\eta\\) will be between 0 and 1. To get around this problem we apply a transformation to take \\(\\eta_i\\) to \\(\\mu_i\\). For historical reasons this transformation is called the inverse link, written \\(g^{-1}\\), and the opposite transformation - from the probability scale to an unbounded scale - is called the link, g.\nEach probability distribution in the exponential family (which is most of the important ones), has a canonical link which comes from the form of the distribution itself. The details aren‚Äôt as important as recognizing that the distribution itself determines a preferred link function.\nFor the Bernoulli distribution, the canonical link is the logit or log-odds function, \\[\n\\eta = g(\\mu) = \\log\\left(\\frac{\\mu}{1-\\mu}\\right),\n\\] (it‚Äôs called log-odds because it is the logarithm of the odds ratio, \\(p/(1-p)\\)) and the canonical inverse link is the logistic \\[\n\\mu=g^{-1}(\\eta)=\\frac{1}{1+\\exp(-\\eta)}.\n\\] This is why fitting a binary response is sometimes called logistic regression.\nFor later use we define a Julia logistic function. See this presentation for more information than you could possibly want to know on how Julia converts code like this to run on the processor.\n\nincrement(x) = x + one(x)\nlogistic(Œ∑) = inv(increment(exp(-Œ∑)))\n\nlogistic (generic function with 1 method)\n\n\nTo reiterate, the probability model for a Generalized Linear Mixed Model (GLMM) is \\[\n\\begin{aligned}\n(\\mathcal{Y} | \\mathcal{B}=\\bf{b}) &\\sim\\mathcal{D}(\\bf{g^{-1}(X\\boldsymbol\\beta + Z b)},\\phi)\\\\\\\\\n\\mathcal{B}&\\sim\\mathcal{N}(\\bf{0},\\Sigma_{\\boldsymbol\\theta}) .\n\\end{aligned}\n\\] where \\(\\mathcal{D}\\) is the distribution family (such as Bernoulli or Poisson), \\(g^{-1}\\) is the inverse link and \\(\\phi\\) is a scale parameter for \\(\\mathcal{D}\\) if it has one. The important cases of the Bernoulli and Poisson distributions don‚Äôt have a scale parameter - once you know the mean you know everything you need to know about the distribution. (For those following the presentation, this poem by John Keats is the one with the couplet ‚ÄúBeauty is truth, truth beauty - that is all ye know on earth and all ye need to know.‚Äù)\n\n3.1 An example of a Bernoulli GLMM\nThe contra dataset in the MixedModels package is from a survey on the use of artificial contraception by women in Bangladesh.\n\ncontra = DataFrame(dataset(:contra))\n\n1934√ó5 DataFrame1909 rows omitted\n\n\n\nRow\ndist\nurban\nlivch\nage\nuse\n\n\n\nString\nString\nString\nFloat64\nString\n\n\n\n\n1\nD01\nY\n3+\n18.44\nN\n\n\n2\nD01\nY\n0\n-5.56\nN\n\n\n3\nD01\nY\n2\n1.44\nN\n\n\n4\nD01\nY\n3+\n8.44\nN\n\n\n5\nD01\nY\n0\n-13.56\nN\n\n\n6\nD01\nY\n0\n-11.56\nN\n\n\n7\nD01\nY\n3+\n18.44\nN\n\n\n8\nD01\nY\n3+\n-3.56\nN\n\n\n9\nD01\nY\n1\n-5.56\nN\n\n\n10\nD01\nY\n3+\n1.44\nN\n\n\n11\nD01\nY\n0\n-11.56\nY\n\n\n12\nD01\nY\n0\n-2.56\nN\n\n\n13\nD01\nY\n1\n-4.56\nN\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n1923\nD61\nN\n0\n-11.56\nY\n\n\n1924\nD61\nN\n3+\n1.44\nN\n\n\n1925\nD61\nN\n1\n-5.56\nN\n\n\n1926\nD61\nN\n3+\n14.44\nN\n\n\n1927\nD61\nN\n3+\n19.44\nN\n\n\n1928\nD61\nN\n2\n-9.56\nY\n\n\n1929\nD61\nN\n2\n-2.56\nN\n\n\n1930\nD61\nN\n3+\n14.44\nN\n\n\n1931\nD61\nN\n2\n-4.56\nN\n\n\n1932\nD61\nN\n3+\n14.44\nN\n\n\n1933\nD61\nN\n0\n-13.56\nN\n\n\n1934\nD61\nN\n3+\n10.44\nN\n\n\n\n\n\n\n\ncombine(groupby(contra, :dist), nrow)\n\n60√ó2 DataFrame35 rows omitted\n\n\n\nRow\ndist\nnrow\n\n\n\nString\nInt64\n\n\n\n\n1\nD01\n117\n\n\n2\nD02\n20\n\n\n3\nD03\n2\n\n\n4\nD04\n30\n\n\n5\nD05\n39\n\n\n6\nD06\n65\n\n\n7\nD07\n18\n\n\n8\nD08\n37\n\n\n9\nD09\n23\n\n\n10\nD10\n13\n\n\n11\nD11\n21\n\n\n12\nD12\n29\n\n\n13\nD13\n24\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n49\nD49\n4\n\n\n50\nD50\n19\n\n\n51\nD51\n37\n\n\n52\nD52\n61\n\n\n53\nD53\n19\n\n\n54\nD55\n6\n\n\n55\nD56\n45\n\n\n56\nD57\n27\n\n\n57\nD58\n33\n\n\n58\nD59\n10\n\n\n59\nD60\n32\n\n\n60\nD61\n42\n\n\n\n\n\n\nThe information recorded included woman‚Äôs age, the number of live children she has, whether she lives in an urban or rural setting, and the political district in which she lives.\nThe age was centered. Unfortunately, the version of the data to which I had access did not record what the centering value was.\nA data plot, Figure¬†1, shows that the probability of contraception use is not linear in age - it is low for younger women, higher for women in the middle of the range (assumed to be women in late 20‚Äôs to early 30‚Äôs) and low again for older women (late 30‚Äôs to early 40‚Äôs in this survey).\nIf we fit a model with only the age term in the fixed effects, that term will not be significant. This doesn‚Äôt mean that there is no ‚Äúage effect‚Äù, it only means that there is no significant linear effect for age.\n\n\nCode\ndraw(\n  data(\n    @transform(\n      contra,\n      :numuse = Int(:use == \"Y\"),\n      :urb = ifelse(:urban == \"Y\", \"Urban\", \"Rural\")\n    )\n  ) *\n  mapping(\n    :age =&gt; \"Centered age (yr)\",\n    :numuse =&gt; \"Frequency of contraception use\";\n    col=:urb,\n    color=:livch,\n  ) *\n  smooth();\n  figure=(; size=(800, 450)),\n)\n\n\n\n\n\n\n\nFigure¬†1: Smoothed relative frequency of contraception use versus centered age for women in the 1989 Bangladesh Fertility Survey\n\n\n\n\n\ncontrasts = Dict(\n  :dist =&gt; Grouping(),\n  :urban =&gt; HelmertCoding(),\n  :livch =&gt; DummyCoding(), # default, but no harm in being explicit\n)\nnAGQ = 9\ndist = Bernoulli()\ngm1 = let\n  form = @formula(\n    use ~ 1 + age + abs2(age) + urban + livch + (1 | dist)\n  )\n  fit(MixedModel, form, contra, dist; nAGQ, contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_dist\n\n\n(Intercept)\n-0.6845\n0.1686\n-4.06\n&lt;1e-04\n0.4790\n\n\nage\n0.0036\n0.0092\n0.39\n0.6944\n\n\n\nabs2(age)\n-0.0046\n0.0007\n-6.30\n&lt;1e-09\n\n\n\nurban: Y\n0.3486\n0.0600\n5.81\n&lt;1e-08\n\n\n\nlivch: 1\n0.8134\n0.1622\n5.01\n&lt;1e-06\n\n\n\nlivch: 2\n0.9147\n0.1851\n4.94\n&lt;1e-06\n\n\n\nlivch: 3+\n0.9128\n0.1858\n4.91\n&lt;1e-06\n\n\n\n\n\n\nNotice that the linear term for age is not significant but the quadratic term for age is highly significant. We usually retain the lower order term, even if it is not significant, if the higher order term is significant.\nNotice also that the parameter estimates for the treatment contrasts for livch are similar. Thus the distinction of 1, 2, or 3+ children is not as important as the contrast between having any children and not having any. Those women who already have children are more likely to use artificial contraception.\nFurthermore, the women without children have a different probability vs age profile than the women with children. To allow for this we define a binary children factor and incorporate an age&children interaction.\n\nVarCorr(gm1)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\n\n\ndist\n(Intercept)\n0.22940\n0.47896\n\n\n\n\n\nNotice that there is no ‚Äúresidual‚Äù variance being estimated. This is because the Bernoulli distribution doesn‚Äôt have a scale parameter.\n\n\n3.2 Convert livch to a binary factor\n\n@transform!(contra, :children = ifelse(:livch ‚â† \"0\", \"Y\", \"N\"))\n# add the associated contrast specifier\ncontrasts[:children] = EffectsCoding()\n\nEffectsCoding(nothing, nothing)\n\n\n\ngm2 = let\n  form = @formula(\n    use ~\n      1 +\n      age * children +\n      abs2(age) +\n      children +\n      urban +\n      (1 | dist)\n  )\n  fit(MixedModel, form, contra, dist; nAGQ, contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_dist\n\n\n(Intercept)\n-0.3606\n0.1275\n-2.83\n0.0047\n0.4757\n\n\nage\n-0.0131\n0.0110\n-1.18\n0.2368\n\n\n\nchildren: Y\n0.6050\n0.1035\n5.85\n&lt;1e-08\n\n\n\nabs2(age)\n-0.0058\n0.0008\n-6.89\n&lt;1e-11\n\n\n\nurban: Y\n0.3568\n0.0602\n5.93\n&lt;1e-08\n\n\n\nage & children: Y\n0.0341\n0.0127\n2.68\n0.0073\n\n\n\n\n\n\n\n\nCode\nlet\n  mods = [gm2, gm1]\n  DataFrame(;\n    model=[:gm2, :gm1],\n    npar=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n2√ó6 DataFrame\n\n\n\nRow\nmodel\nnpar\ndeviance\nAIC\nBIC\nAICc\n\n\n\nSymbol\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\ngm2\n7\n2364.92\n2379.18\n2418.15\n2379.24\n\n\n2\ngm1\n8\n2372.46\n2388.73\n2433.27\n2388.81\n\n\n\n\n\n\nBecause these models are not nested, we cannot do a likelihood ratio test. Nevertheless we see that the deviance is much lower in the model with age & children even though the 3 levels of livch have been collapsed into a single level of children. There is a substantial decrease in the deviance even though there are fewer parameters in model gm2 than in gm1. This decrease is because the flexibility of the model - its ability to model the behavior of the response - is being put to better use in gm2 than in gm1.\nAt present the calculation of the geomdof as sum(influence(m)) is not correctly defined in our code for a GLMM so we need to do some more work before we can examine those values.\n\n\n3.3 Using urban&dist as a grouping factor\nIt turns out that there can be more difference between urban and rural settings within the same political district than there is between districts. To model this difference we build a model with urban&dist as a grouping factor.\n\ngm3 = let\n  form = @formula(\n    use ~\n      1 +\n      age * children +\n      abs2(age) +\n      children +\n      urban +\n      (1 | urban & dist)\n  )\n  fit(MixedModel, form, contra, dist; nAGQ, contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_urban & dist\n\n\n(Intercept)\n-0.3414\n0.1269\n-2.69\n0.0071\n0.5761\n\n\nage\n-0.0129\n0.0112\n-1.16\n0.2471\n\n\n\nchildren: Y\n0.6064\n0.1045\n5.80\n&lt;1e-08\n\n\n\nabs2(age)\n-0.0056\n0.0008\n-6.66\n&lt;1e-10\n\n\n\nurban: Y\n0.3935\n0.0859\n4.58\n&lt;1e-05\n\n\n\nage & children: Y\n0.0332\n0.0128\n2.59\n0.0096\n\n\n\n\n\n\n\n\nCode\nlet\n  mods = [gm3, gm2, gm1]\n  DataFrame(;\n    model=[:gm3, :gm2, :gm1],\n    npar=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n3√ó6 DataFrame\n\n\n\nRow\nmodel\nnpar\ndeviance\nAIC\nBIC\nAICc\n\n\n\nSymbol\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\ngm3\n7\n2353.82\n2368.48\n2407.45\n2368.54\n\n\n2\ngm2\n7\n2364.92\n2379.18\n2418.15\n2379.24\n\n\n3\ngm1\n8\n2372.46\n2388.73\n2433.27\n2388.81\n\n\n\n\n\n\nNotice that the parameter count in gm3 is the same as that of gm2 - the thing that has changed is the number of levels of the grouping factor- resulting in a much lower deviance for gm3. This reinforces the idea that a simple count of the number of parameters to be estimated does not always reflect the complexity of the model.\n\ngm2\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_dist\n\n\n(Intercept)\n-0.3606\n0.1275\n-2.83\n0.0047\n0.4757\n\n\nage\n-0.0131\n0.0110\n-1.18\n0.2368\n\n\n\nchildren: Y\n0.6050\n0.1035\n5.85\n&lt;1e-08\n\n\n\nabs2(age)\n-0.0058\n0.0008\n-6.89\n&lt;1e-11\n\n\n\nurban: Y\n0.3568\n0.0602\n5.93\n&lt;1e-08\n\n\n\nage & children: Y\n0.0341\n0.0127\n2.68\n0.0073\n\n\n\n\n\n\n\ngm3\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_urban & dist\n\n\n(Intercept)\n-0.3414\n0.1269\n-2.69\n0.0071\n0.5761\n\n\nage\n-0.0129\n0.0112\n-1.16\n0.2471\n\n\n\nchildren: Y\n0.6064\n0.1045\n5.80\n&lt;1e-08\n\n\n\nabs2(age)\n-0.0056\n0.0008\n-6.66\n&lt;1e-10\n\n\n\nurban: Y\n0.3935\n0.0859\n4.58\n&lt;1e-05\n\n\n\nage & children: Y\n0.0332\n0.0128\n2.59\n0.0096\n\n\n\n\n\n\nThe coefficient for age may be regarded as insignificant but we retain it for two reasons: we have a term of age¬≤ (written abs2(age)) in the model and we have a significant interaction age & children in the model.\n\n\n3.4 Predictions for some subgroups\nFor a ‚Äútypical‚Äù district (random effect near zero) the predictions on the linear predictor scale for a woman whose age is near the centering value (i.e.¬†centered age of zero) are:\n\nusing Effects\ndesign = Dict(\n  :children =&gt; [\"Y\", \"N\"], :urban =&gt; [\"Y\", \"N\"], :age =&gt; [0.0]\n)\npreds = effects(design, gm3; invlink=AutoInvLink())\n\n4√ó7 DataFrame\n\n\n\nRow\nchildren\nage\nurban\nuse: Y\nerr\nlower\nupper\n\n\n\nString\nFloat64\nString\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nY\n0.0\nY\n0.658935\n0.0338287\n0.625106\n0.692763\n\n\n2\nN\n0.0\nY\n0.364864\n0.0534088\n0.311455\n0.418272\n\n\n3\nY\n0.0\nN\n0.467918\n0.0281364\n0.439781\n0.496054\n\n\n4\nN\n0.0\nN\n0.207284\n0.0364077\n0.170877\n0.243692",
    "crumbs": [
      "Generalized linear mixed models"
    ]
  },
  {
    "objectID": "glmm.html#summarizing-the-results",
    "href": "glmm.html#summarizing-the-results",
    "title": "Generalized linear mixed models",
    "section": "4 Summarizing the results",
    "text": "4 Summarizing the results\n\nFrom the data plot we can see a quadratic trend in the probability by age.\nThe patterns for women with children are similar and we do not need to distinguish between 1, 2, and 3+ children.\nWe do distinguish between those women who do not have children and those with children. This shows up in a significant age & children interaction term.\n\nThis page was rendered from git revision fb99b3c\n using Quarto 1.7.33.",
    "crumbs": [
      "Generalized linear mixed models"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html",
    "href": "contrasts_kwdyz11.html",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "",
    "text": "Code\nusing AlgebraOfGraphics\nusing CairoMakie\nusing Chain\nusing DataFrames\nusing MixedModels\nusing SMLP2026: dataset\nusing StatsBase\nusing StatsModels\n\nCairoMakie.activate!(; type=\"png\")\n\nprogress = false\n\n\nfalse",
    "crumbs": [
      "Contrast coding and transformations",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html#seqdiffcoding",
    "href": "contrasts_kwdyz11.html#seqdiffcoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.1 SeqDiffCoding",
    "text": "4.1 SeqDiffCoding\nThe SeqDiffCoding contrast corresponds to MASS::contr.sdif() in R. The assignment of random factors such as Subj to Grouping() is necessary when the sample size is very large. We recommend to include it always, but in this tutorial we do so only in the first example.\n\nm1 = let levels = [\"val\", \"sod\", \"dos\", \"dod\"]\n  contrasts = Dict(\n    :CTR =&gt; SeqDiffCoding(; levels),\n  )\n  fit(MixedModel, form, dat1; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n(Intercept)\n389.7336\n7.0923\n54.95\n&lt;1e-99\n55.2077\n\n\nCTR: sod\n33.7817\n3.2877\n10.28\n&lt;1e-24\n23.2508\n\n\nCTR: dos\n13.9852\n2.3057\n6.07\n&lt;1e-08\n10.7542\n\n\nCTR: dod\n-2.7469\n2.2146\n-1.24\n0.2148\n9.5153\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nWhat does the intercept represent?\n\nmean(dat1.rt)\nmean(cellmeans.rt_mean)\n\n389.18622f0\n\n\nGrand Mean is mean of condition means.",
    "crumbs": [
      "Contrast coding and transformations",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html#hypothesiscoding",
    "href": "contrasts_kwdyz11.html#hypothesiscoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.2 HypothesisCoding",
    "text": "4.2 HypothesisCoding\nHypothesisCoding is the most general option available. We can implement all ‚Äúcanned‚Äù contrasts ourselves. The next example reproduces the test statistics from SeqDiffCoding - with a minor modification illustrating the flexibility of going beyond the default version.\n\nm1b = let levels = [\"val\", \"sod\", \"dos\", \"dod\"]\n  contrasts = Dict(\n    :CTR =&gt; HypothesisCoding(\n      [\n        -1  1 0  0\n         0 -1 1  0\n         0  0 1 -1\n      ];\n      levels,\n      labels=[\"spt\", \"obj\", \"grv\"],\n    ),\n  )\n  fit(MixedModel, form, dat1; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n(Intercept)\n389.7336\n7.0913\n54.96\n&lt;1e-99\n55.1995\n\n\nCTR: spt\n33.7817\n3.2873\n10.28\n&lt;1e-24\n23.2477\n\n\nCTR: obj\n13.9852\n2.3057\n6.07\n&lt;1e-08\n10.7538\n\n\nCTR: grv\n2.7470\n2.2142\n1.24\n0.2148\n9.5099\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nThe difference to the preprogrammed SeqDiffCoding is that for the third contrast we changed the direction of the contrast such that the sign of the effect is positive when the result is in agreement with theoretical expectation, that is we subtract the fourth level from the third, not the third level from the fourth.",
    "crumbs": [
      "Contrast coding and transformations",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html#dummycoding",
    "href": "contrasts_kwdyz11.html#dummycoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.3 DummyCoding",
    "text": "4.3 DummyCoding\nThis contrast corresponds to contr.treatment() in R\n\nm2 = let\n  contrasts = Dict(:CTR =&gt; DummyCoding(; base=\"val\"))\n  fit(MixedModel, form, dat1; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n(Intercept)\n358.0914\n6.1526\n58.20\n&lt;1e-99\n47.8994\n\n\nCTR: dod\n45.0200\n4.3627\n10.32\n&lt;1e-24\n32.2840\n\n\nCTR: dos\n47.7669\n3.5559\n13.43\n&lt;1e-40\n25.5305\n\n\nCTR: sod\n33.7817\n3.2866\n10.28\n&lt;1e-24\n23.2416\n\n\nResidual\n69.8349\n\n\n\n\n\n\n\n\n\nThe DummyCoding contrast has the disadvantage that the intercept returns the mean of the level specified as base, default is the first level, not the GM.",
    "crumbs": [
      "Contrast coding and transformations",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html#ychycaeitcoding",
    "href": "contrasts_kwdyz11.html#ychycaeitcoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.4 YchycaeitCoding",
    "text": "4.4 YchycaeitCoding\nThe contrasts returned by DummyCoding may be exactly what we want. Can‚Äôt we have them, but also have the intercept estimate the GM, rather than the mean of the base level? Yes, we can! We call this ‚ÄúYou can have your cake and it eat, too‚Äù-Coding (YchycaeitCoding). And we use HypothesisCoding to achieve this outcome.\n\nm2b = let levels = [\"val\", \"sod\", \"dos\", \"dod\"]\n  contrasts = Dict(\n    :CTR =&gt; HypothesisCoding(\n      [\n        -1 1 0 0\n        -1 0 1 0\n        -1 0 0 1\n      ];\n      levels,\n      labels=levels[2:end],\n    )\n  )\n  fit(MixedModel, form, dat1; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n(Intercept)\n389.7336\n7.0911\n54.96\n&lt;1e-99\n55.1977\n\n\nCTR: sod\n33.7817\n3.2874\n10.28\n&lt;1e-24\n23.2484\n\n\nCTR: dos\n47.7669\n3.5567\n13.43\n&lt;1e-40\n25.5373\n\n\nCTR: dod\n45.0200\n4.3636\n10.32\n&lt;1e-24\n32.2911\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nWe can simply move the column with -1s for a different base.\n\nm2c = let levels = [\"val\", \"sod\", \"dos\", \"dod\"]\n  contrasts = Dict(\n    :CTR =&gt; HypothesisCoding(\n      [\n       1 -1  0  0\n       0 -1  1  0\n       0 -1  0  1\n      ];\n      levels,\n      labels=[\"val\", \"dos\", \"dod\"],\n    )\n  )\n  fit(MixedModel, form, dat1; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n(Intercept)\n389.7336\n7.0915\n54.96\n&lt;1e-99\n55.2009\n\n\nCTR: val\n-33.7817\n3.2873\n-10.28\n&lt;1e-24\n23.2478\n\n\nCTR: dos\n13.9852\n2.3056\n6.07\n&lt;1e-08\n10.7524\n\n\nCTR: dod\n11.2383\n2.3584\n4.77\n&lt;1e-05\n11.4640\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nWe can simply relevel the factor with a different base.\n\nm2d = let levels = [\"sod\", \"val\", \"dos\", \"dod\"]\n  contrasts = Dict(\n    :CTR =&gt; HypothesisCoding(\n      [\n        -1 1 0 0\n        -1 0 1 0\n        -1 0 0 1\n      ];\n      levels,\n      labels=levels[2:end],\n    )\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n(Intercept)\n389.7336\n7.0912\n54.96\n&lt;1e-99\n55.1986\n\n\nCTR: val\n-33.7817\n3.2874\n-10.28\n&lt;1e-24\n23.2486\n\n\nCTR: dos\n13.9852\n2.3058\n6.07\n&lt;1e-08\n10.7550\n\n\nCTR: dod\n11.2383\n2.3585\n4.76\n&lt;1e-05\n11.4653\n\n\nResidual\n69.8348",
    "crumbs": [
      "Contrast coding and transformations",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html#effectscoding",
    "href": "contrasts_kwdyz11.html#effectscoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.5 EffectsCoding",
    "text": "4.5 EffectsCoding\nEffectsCoding estimates the difference between the Grand Mean and three of the four levels. The difference of the fourth levels can be computed from the Grand Mean and these three differences.\n\nm3 = let levels = [\"val\", \"sod\", \"dos\",  \"dod\"]\n  contrasts = Dict(:CTR =&gt; EffectsCoding(; levels, base=\"val\"))\n  fit(MixedModel, form, dat1; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n(Intercept)\n389.7336\n7.0911\n54.96\n&lt;1e-99\n55.1980\n\n\nCTR: sod\n2.1396\n1.3339\n1.60\n0.1087\n6.0082\n\n\nCTR: dos\n16.1248\n1.4404\n11.19\n&lt;1e-28\n7.3317\n\n\nCTR: dod\n13.3778\n1.9323\n6.92\n&lt;1e-11\n12.4630\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nThis contrast corresponds almost to contr.sum() in R. The ‚Äúalmost‚Äù qualification refers to the fact that EffectsCoding uses the first level as default base; contr.sum() uses the last factor level.\n\nm3b = let levels = [\"val\", \"sod\", \"dos\",  \"dod\"]\n  contrasts = Dict(:CTR =&gt; EffectsCoding(; levels, base = \"dod\"))\n  fit(MixedModel, form, dat1; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n(Intercept)\n389.7336\n7.0928\n54.95\n&lt;1e-99\n55.2118\n\n\nCTR: val\n-31.6422\n2.6427\n-11.97\n&lt;1e-32\n19.9535\n\n\nCTR: sod\n2.1396\n1.3340\n1.60\n0.1087\n6.0098\n\n\nCTR: dos\n16.1248\n1.4407\n11.19\n&lt;1e-28\n7.3350\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nHow could we achieve the default result with HypothesisCoding?\n\nm3c = let levels = [\"val\", \"sod\", \"dos\",  \"dod\"]\n  contrasts = Dict(\n    :CTR =&gt; HypothesisCoding(\n      [\n         -1/4   3/4 -1/4  -1/4   # b - GM = b - (a+b+c+d)/4 =&gt;  -1/4*a + 3/4*b - 1/4*c - 1/4*d\n         -1/4  -1/4  3/4  -1/4   # c - GM = c - (a+b+c+d)/4 =&gt;  -1/4*a - 1/4*b + 3/4*c - 1/4*d\n         -1/4  -1/4 -1/4   3/4   # d - GM = d - (a+b+c+d)/4 =&gt;  -1/4*a - 1/4*b - 1/4*c + 3/4*d\n      ];\n      levels,\n      labels=levels[2:end],\n    )\n  )\n  fit(MixedModel, form, dat1; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n(Intercept)\n389.7336\n7.0915\n54.96\n&lt;1e-99\n55.2014\n\n\nCTR: sod\n2.1396\n1.3338\n1.60\n0.1087\n6.0081\n\n\nCTR: dos\n16.1248\n1.4404\n11.19\n&lt;1e-28\n7.3321\n\n\nCTR: dod\n13.3778\n1.9325\n6.92\n&lt;1e-11\n12.4644\n\n\nResidual\n69.8348",
    "crumbs": [
      "Contrast coding and transformations",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html#helmertcoding",
    "href": "contrasts_kwdyz11.html#helmertcoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.6 HelmertCoding",
    "text": "4.6 HelmertCoding\nHelmertCoding codes each level as the difference from the average of the lower levels. With the default order of CTR levels we get the following test statistics. These contrasts are orthogonal.\n\nm4 = let levels = [\"val\", \"sod\", \"dos\",  \"dod\"]\n  contrasts = Dict(:CTR =&gt; HelmertCoding(; levels))\n  fit(MixedModel, form, dat1; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n(Intercept)\n389.7336\n7.0918\n54.96\n&lt;1e-99\n55.2033\n\n\nCTR: sod\n16.8909\n1.6438\n10.28\n&lt;1e-24\n11.6248\n\n\nCTR: dos\n10.2920\n0.8354\n12.32\n&lt;1e-34\n5.2573\n\n\nCTR: dod\n4.4593\n0.6441\n6.92\n&lt;1e-11\n4.1546\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\n+ HeC1: (b - a)/2           # (391 - 358)/2 = 16.5\n+ HeC2: (c - (b+a)/2)/3     # (405 - (391 + 358)/2)/3 = 10.17 \n+ HeC3: (d - (c+b+a)/3)/4   # (402 - (405 + 391 + 358)/3)/4 = 4.33\nWe can reconstruct the estimates, but they are scaled by the number of levels involved. With HypothesisCoding we can estimate the ‚Äúunscaled‚Äù differences. Also the labeling of the contrasts is not as informative as they could be.\n\nm4b = let levels = [\"val\", \"sod\", \"dos\",  \"dod\"]\n  contrasts = Dict(\n    :CTR =&gt; HypothesisCoding(\n      [\n          -1    1    0   0 \n         -1/2 -1/2   1   0\n         -1/3 -1/3 -1/3  1\n        \n      ];\n      levels,\n      labels= [\"2-1\", \"3-21\", \"4-321\"]\n    )\n  )\n  fit(MixedModel, form, dat1; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n(Intercept)\n389.7336\n7.0913\n54.96\n&lt;1e-99\n55.2000\n\n\nCTR: 2-1\n33.7817\n3.2874\n10.28\n&lt;1e-24\n23.2482\n\n\nCTR: 3-21\n30.8761\n2.5062\n12.32\n&lt;1e-34\n15.7718\n\n\nCTR: 4-321\n17.8371\n2.5765\n6.92\n&lt;1e-11\n16.6182\n\n\nResidual\n69.8348",
    "crumbs": [
      "Contrast coding and transformations",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html#reverse-helmertcoding",
    "href": "contrasts_kwdyz11.html#reverse-helmertcoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.7 Reverse HelmertCoding",
    "text": "4.7 Reverse HelmertCoding\nReverse HelmertCoding codes each level as the difference from the average of the higher levels. To estimate these effects we simply reverse the order of factor levels. Of course, the contrasts are also orthogonal.\n\nm4c = let levels = reverse(StatsModels.levels(dat1.CTR))\n  contrasts = Dict(:CTR =&gt; HelmertCoding(; levels))\n  fit(MixedModel, form, dat1; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n(Intercept)\n389.7336\n7.0916\n54.96\n&lt;1e-99\n55.2024\n\n\nCTR: dos\n1.3735\n1.1072\n1.24\n0.2148\n4.7564\n\n\nCTR: sod\n-4.2039\n0.6843\n-6.14\n&lt;1e-09\n3.3492\n\n\nCTR: val\n-10.5474\n0.8808\n-11.97\n&lt;1e-32\n6.6505\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\n+ HeC1:(c - d)/2            # (405 - 402)/2 = 1.5\n+ HeC2:(b - (c+d)/2)/3      # (391 - (405 + 402)/2)/3 = -4.17\n+ HeC3:(a - (b+c+d)/3/4     # (356  -(391 + 405 + 402)/3)/4 = -10.83\n‚Ä¶ and the unscaled-by-number-of-levels estimates.\n\nm4d = let levels = [\"val\", \"sod\", \"dos\",  \"dod\"]\n  contrasts = Dict(\n    :CTR =&gt; HypothesisCoding(\n      [\n        0    0     1   -1 \n        0    1   -1/2 -1/2\n        1  -1/3  -1/3 -1/3\n      ];\n      levels,\n      labels= [\"3-4\", \"2-34\", \"1-234\"]\n    )\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n(Intercept)\n389.7336\n7.0911\n54.96\n&lt;1e-99\n55.1980\n\n\nCTR: 3-4\n2.7470\n2.2141\n1.24\n0.2147\n9.5087\n\n\nCTR: 2-34\n-12.6117\n2.0532\n-6.14\n&lt;1e-09\n10.0524\n\n\nCTR: 1-234\n-42.1895\n3.5237\n-11.97\n&lt;1e-32\n26.6057\n\n\nResidual\n69.8348",
    "crumbs": [
      "Contrast coding and transformations",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html#anova-coding",
    "href": "contrasts_kwdyz11.html#anova-coding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "5.1 Anova Coding",
    "text": "5.1 Anova Coding\nFactorial designs (i.e., lab experiments) are traditionally analyzed with analysis of variance. The test statistics of main effects and interactions are based on an orthogonal set of contrasts. We specify them with HypothesisCoding.\n\n5.1.1 A(2) x B(2)\nAn A(2) x B(2) design can be recast as an F(4) design with the levels (A1-B1, A1-B2, A2-B1, A2-B2). The following contrast specification returns estimates for the main effect of A, the main effect of B, and the interaction of A and B. In a figure With A on the x-axis and the levels of B shown as two lines, the interaction tests the null hypothesis that the two lines are parallel. A positive coefficient implies overadditivity (diverging lines toward the right) and a negative coefficient underadditivity (converging lines).\n\nm5 = let levels = [\"val\", \"sod\", \"dos\", \"dod\"]\n  contrasts = Dict(\n    :CTR =&gt; HypothesisCoding(\n      [\n        -1 -1 +1 +1          # A\n        -1 +1 -1 +1          # B\n        +1 -1 -1 +1          # A x B\n      ];\n      levels,\n      labels=[\"A\", \"B\", \"AxB\"],\n    ),\n  )\n  fit(MixedModel, form, dat1; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n(Intercept)\n389.7336\n7.0912\n54.96\n&lt;1e-99\n55.1989\n\n\nCTR: A\n59.0052\n5.1826\n11.39\n&lt;1e-29\n36.2079\n\n\nCTR: B\n31.0348\n4.6750\n6.64\n&lt;1e-10\n31.7137\n\n\nCTR: AxB\n-36.5287\n3.0927\n-11.81\n&lt;1e-31\n16.0052\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nIt is also helpful to see the corresponding layout of the four means for the interaction of A and B (i.e., the third contrast)\n        B1     B2\n   A1   +1     -1\n   A2   -1     +1\nThus, interaction tests whether the difference between main diagonal and minor diagonal is different from zero.\n\n\n5.1.2 A(2) x B(2) x C(2)\nGoing beyond the four level factor; it is also helpful to see the corresponding layout of the eight means for the interaction of A and B and C.\n          C1              C2\n      B1     B2        B1     B2\n A1   +1     -1   A1   -1     +1\n A2   -1     +1   A2   +1     -1",
    "crumbs": [
      "Contrast coding and transformations",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html#nested-coding",
    "href": "contrasts_kwdyz11.html#nested-coding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "5.2 Nested coding",
    "text": "5.2 Nested coding\nNested contrasts are often specified as follow up as post-hoc tests for ANOVA interactions. They are orthogonal. We specify them with HypothesisCoding.\nAn A(2) x B(2) design can be recast as an F(4) design with the levels (A1-B1, A1-B2, A2-B1, A2-B2). The following contrast specification returns an estimate for the main effect of A and the effects of B nested in the two levels of A. In a figure With A on the x-axis and the levels of B shown as two lines, the second contrast tests whether A1-B1 is different from A1-B2 and the third contrast tests whether A2-B1 is different from A2-B2.\n\n\nm6 = let levels = [\"val\", \"sod\", \"dos\", \"dod\"]\n  contrasts = Dict(\n    :CTR =&gt; HypothesisCoding(\n      [\n        -1 -1 +1 +1\n        -1 +1  0  0\n         0  0 +1 -1\n      ];\n      levels,\n      labels=[\"do_so\", \"spt\", \"grv\"],\n    ),\n  )\n  fit(MixedModel, form, dat1; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n(Intercept)\n389.7336\n7.0907\n54.96\n&lt;1e-99\n55.1946\n\n\nCTR: do_so\n59.0052\n5.1827\n11.38\n&lt;1e-29\n36.2089\n\n\nCTR: spt\n33.7817\n3.2872\n10.28\n&lt;1e-24\n23.2466\n\n\nCTR: grv\n2.7470\n2.2143\n1.24\n0.2148\n9.5107\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nThe three contrasts for one main effect and two nested contrasts are orthogonal. There is no test of the interaction (parallelism).",
    "crumbs": [
      "Contrast coding and transformations",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html#standard-contrasts",
    "href": "contrasts_kwdyz11.html#standard-contrasts",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "7.1 Standard contrasts",
    "text": "7.1 Standard contrasts\nThe most commonly used contrasts are DummyCoding and EffectsCoding (which are similar to contr.treatment() and contr.sum() in R, respectively).",
    "crumbs": [
      "Contrast coding and transformations",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html#exotic-contrasts-rk-well",
    "href": "contrasts_kwdyz11.html#exotic-contrasts-rk-well",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "7.2 ‚ÄúExotic‚Äù contrasts (rk: well ‚Ä¶)",
    "text": "7.2 ‚ÄúExotic‚Äù contrasts (rk: well ‚Ä¶)\nWe also provide HelmertCoding and SeqDiffCoding (corresponding to base R‚Äôs contr.helmert() and MASS::contr.sdif()).",
    "crumbs": [
      "Contrast coding and transformations",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html#manual-contrasts",
    "href": "contrasts_kwdyz11.html#manual-contrasts",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "7.3 Manual contrasts",
    "text": "7.3 Manual contrasts\nContrastsCoding()\nThere are two ways to manually specify contrasts. First, you can specify them directly via ContrastsCoding. If you do, it‚Äôs good practice to specify the levels corresponding to the rows of the matrix, although they can be omitted in which case they‚Äôll be inferred from the data.\nHypothesisCoding()\nA better way to specify manual contrasts is via HypothesisCoding, where each row of the matrix corresponds to the weights given to the cell means of the levels corresponding to each column (see Schad et al. (2020) for more information).\nThis page was rendered from git revision fb99b3c\n using Quarto 1.7.33.",
    "crumbs": [
      "Contrast coding and transformations",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "check_emotikon_transform.html",
    "href": "check_emotikon_transform.html",
    "title": "Transformed and original metrics in Emotikon",
    "section": "",
    "text": "In F√ºhner et al. (2021) the original metric of two tasks (Star, S20) is time, but they were transformed to speed scores in the publication prior to computing z-scores. The critical result is the absence of evidence for the age x Sex x Test interaction. Is this interaction significant if we analyse all tasks in their original metric?\nFitting the LMM of the publication takes time, roughly 1 hour. However, if you save the model parameters (and other relevant information), you can restore the fitted model object very quickly. The notebook also illustrates this procedure."
  },
  {
    "objectID": "check_emotikon_transform.html#getting-the-packages-and-data",
    "href": "check_emotikon_transform.html#getting-the-packages-and-data",
    "title": "Transformed and original metrics in Emotikon",
    "section": "1 Getting the packages and data",
    "text": "1 Getting the packages and data\n\n\nCode\nusing AlgebraOfGraphics\nusing Arrow\nusing CairoMakie\nusing DataFrames\nusing DataFrameMacros\nusing MixedModels\nusing MixedModelsMakie\nusing RCall\nusing SMLP2026: dataset\nusing Serialization\nusing StatsBase\n\n\n\n1.1 Data and figure in publication\n\ndat = DataFrame(dataset(:fggk21))\n\n525126√ó7 DataFrame525101 rows omitted\n\n\n\nRow\nCohort\nSchool\nChild\nSex\nage\nTest\nscore\n\n\n\nString\nString\nString\nString\nFloat64\nString\nFloat64\n\n\n\n\n1\n2013\nS100067\nC002352\nmale\n7.99452\nS20_r\n5.26316\n\n\n2\n2013\nS100067\nC002352\nmale\n7.99452\nBPT\n3.7\n\n\n3\n2013\nS100067\nC002352\nmale\n7.99452\nSLJ\n125.0\n\n\n4\n2013\nS100067\nC002352\nmale\n7.99452\nStar_r\n2.47146\n\n\n5\n2013\nS100067\nC002352\nmale\n7.99452\nRun\n1053.0\n\n\n6\n2013\nS100067\nC002353\nmale\n7.99452\nS20_r\n5.0\n\n\n7\n2013\nS100067\nC002353\nmale\n7.99452\nBPT\n4.1\n\n\n8\n2013\nS100067\nC002353\nmale\n7.99452\nSLJ\n116.0\n\n\n9\n2013\nS100067\nC002353\nmale\n7.99452\nStar_r\n1.76778\n\n\n10\n2013\nS100067\nC002353\nmale\n7.99452\nRun\n1089.0\n\n\n11\n2013\nS100067\nC002354\nmale\n7.99452\nS20_r\n4.54545\n\n\n12\n2013\nS100067\nC002354\nmale\n7.99452\nBPT\n3.9\n\n\n13\n2013\nS100067\nC002354\nmale\n7.99452\nSLJ\n111.0\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n525115\n2018\nS401470\nC117964\nmale\n9.10609\nStar_r\n1.63704\n\n\n525116\n2018\nS401470\nC117964\nmale\n9.10609\nRun\n864.0\n\n\n525117\n2018\nS401470\nC117965\nfemale\n9.10609\nS20_r\n4.65116\n\n\n525118\n2018\nS401470\nC117965\nfemale\n9.10609\nBPT\n3.8\n\n\n525119\n2018\nS401470\nC117965\nfemale\n9.10609\nSLJ\n123.0\n\n\n525120\n2018\nS401470\nC117965\nfemale\n9.10609\nStar_r\n1.52889\n\n\n525121\n2018\nS401470\nC117965\nfemale\n9.10609\nRun\n1080.0\n\n\n525122\n2018\nS800200\nC117966\nmale\n9.10609\nS20_r\n4.54545\n\n\n525123\n2018\nS800200\nC117966\nmale\n9.10609\nBPT\n3.8\n\n\n525124\n2018\nS800200\nC117966\nmale\n9.10609\nSLJ\n100.0\n\n\n525125\n2018\nS800200\nC117966\nmale\n9.10609\nStar_r\n2.18506\n\n\n525126\n2018\nS800200\nC117966\nmale\n9.10609\nRun\n990.0\n\n\n\n\n\n\n\n@transform!(dat, :a1 = :age - 8.5);\nselect!(groupby(dat, :Test), :, :score =&gt; zscore =&gt; :zScore);\ndescribe(dat)\n\n9√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nDataType\n\n\n\n\n1\nTest\n\nBPT\n\nStar_r\n0\nString\n\n\n2\nCohort\n\n2011\n\n2019\n0\nString\n\n\n3\nSchool\n\nS100043\n\nS800200\n0\nString\n\n\n4\nChild\n\nC002352\n\nC117966\n0\nString\n\n\n5\nSex\n\nfemale\n\nmale\n0\nString\n\n\n6\nage\n8.56073\n7.99452\n8.55852\n9.10609\n0\nFloat64\n\n\n7\nscore\n226.141\n1.14152\n4.65116\n1530.0\n0\nFloat64\n\n\n8\na1\n0.0607297\n-0.505476\n0.0585216\n0.606092\n0\nFloat64\n\n\n9\nzScore\n-3.91914e-13\n-3.1542\n0.00031088\n3.55078\n0\nFloat64\n\n\n\n\n\n\n\n\n1.2 Data and figure with z-scores based on original metric\n\n# dat_om = rcopy(R\"readRDS('./data/fggk21_om.rds')\");  #Don't know what the _om is\n# @transform!(dat_om, :a1 = :age - 8.5);\n# select!(groupby(dat_om, :Test), :, :score =&gt; zscore =&gt; :zScore);\n# describe(dat_om)"
  },
  {
    "objectID": "check_emotikon_transform.html#lmms",
    "href": "check_emotikon_transform.html#lmms",
    "title": "Transformed and original metrics in Emotikon",
    "section": "2 LMMs",
    "text": "2 LMMs\n\n2.1 Contrasts\n\ncontrasts = Dict(\n  :Test =&gt; SeqDiffCoding(),\n  :Sex =&gt; HelmertCoding(),\n);\n\n\n\n2.2 Formula\n\nf1 = @formula zScore ~\n  1 +\n  Test * a1 * Sex +\n  (1 + Test + a1 + Sex | School) +\n  (1 + Test | Child) +\n  zerocorr(1 + Test | Cohort);\n\n\n\n2.3 Restore LMM m1 from publication\n\nCommand for fitting LMM m1 = fit(MixedModel, f1, dat, contrasts=contr)\nFit statistics for LMM m1: Minimizing 5179 Time: 0 Time: 1:00:38 ( 0.70 s/it)\n\n\nm1x = LinearMixedModel(f1, dat; contrasts)\nrestoreoptsum!(m1x, \"./fits/fggk21_m1_optsum.json\")\n\n\n‚îå Warning: optsum was saved with an older version of MixedModels.jl: consider resaving.\n‚îî @ MixedModels ~/.julia/packages/MixedModels/L0NHA/src/serialization.jl:91\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\nœÉ_School\nœÉ_Cohort\n\n\n(Intercept)\n-0.0383\n0.0108\n-3.56\n0.0004\n0.5939\n0.2024\n0.0157\n\n\nTest: Run\n-0.0228\n0.0274\n-0.83\n0.4052\n0.8384\n0.3588\n0.0651\n\n\nTest: S20_r\n-0.0147\n0.0405\n-0.36\n0.7171\n0.5825\n0.3596\n0.1107\n\n\nTest: SLJ\n0.0328\n0.0330\n0.99\n0.3198\n0.4127\n0.3027\n0.0896\n\n\nTest: Star_r\n0.0006\n0.0197\n0.03\n0.9763\n0.5574\n0.3620\n0.0313\n\n\na1\n0.2713\n0.0086\n31.63\n&lt;1e-99\n\n0.0966\n\n\n\nSex: male\n0.2064\n0.0024\n86.55\n&lt;1e-99\n\n0.0245\n\n\n\nTest: Run & a1\n-0.4464\n0.0131\n-34.05\n&lt;1e-99\n\n\n\n\n\nTest: S20_r & a1\n0.1473\n0.0114\n12.97\n&lt;1e-37\n\n\n\n\n\nTest: SLJ & a1\n-0.0068\n0.0103\n-0.66\n0.5116\n\n\n\n\n\nTest: Star_r & a1\n0.0761\n0.0111\n6.84\n&lt;1e-11\n\n\n\n\n\nTest: Run & Sex: male\n-0.0900\n0.0037\n-24.10\n&lt;1e-99\n\n\n\n\n\nTest: S20_r & Sex: male\n-0.0912\n0.0032\n-28.23\n&lt;1e-99\n\n\n\n\n\nTest: SLJ & Sex: male\n0.0330\n0.0029\n11.24\n&lt;1e-28\n\n\n\n\n\nTest: Star_r & Sex: male\n-0.0720\n0.0032\n-22.65\n&lt;1e-99\n\n\n\n\n\na1 & Sex: male\n0.0010\n0.0069\n0.14\n0.8876\n\n\n\n\n\nTest: Run & a1 & Sex: male\n-0.0154\n0.0126\n-1.22\n0.2233\n\n\n\n\n\nTest: S20_r & a1 & Sex: male\n0.0129\n0.0109\n1.18\n0.2380\n\n\n\n\n\nTest: SLJ & a1 & Sex: male\n-0.0098\n0.0100\n-0.98\n0.3256\n\n\n\n\n\nTest: Star_r & a1 & Sex: male\n0.0166\n0.0108\n1.54\n0.1241\n\n\n\n\n\nResidual\n0.5880\n\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m1x)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.3527294\n0.5939103\n\n\n\n\n\n\n\n\n\nTest: Run\n0.7029003\n0.8383915\n+0.11\n\n\n\n\n\n\n\n\nTest: S20_r\n0.3393356\n0.5825252\n+0.19\n-0.53\n\n\n\n\n\n\n\nTest: SLJ\n0.1702900\n0.4126621\n+0.05\n-0.14\n-0.29\n\n\n\n\n\n\nTest: Star_r\n0.3107227\n0.5574251\n-0.10\n+0.01\n-0.13\n-0.42\n\n\n\n\nSchool\n(Intercept)\n0.0409640\n0.2023957\n\n\n\n\n\n\n\n\n\nTest: Run\n0.1287690\n0.3588440\n+0.26\n\n\n\n\n\n\n\n\nTest: S20_r\n0.1293351\n0.3596319\n+0.01\n-0.57\n\n\n\n\n\n\n\nTest: SLJ\n0.0916522\n0.3027411\n-0.13\n+0.01\n-0.53\n\n\n\n\n\n\nTest: Star_r\n0.1310575\n0.3620187\n+0.26\n+0.09\n-0.06\n-0.28\n\n\n\n\n\na1\n0.0093412\n0.0966499\n+0.48\n+0.25\n-0.15\n-0.01\n+0.12\n\n\n\n\nSex: male\n0.0005999\n0.0244934\n+0.09\n+0.13\n-0.01\n+0.05\n-0.19\n+0.25\n\n\nCohort\n(Intercept)\n0.0002452\n0.0156587\n\n\n\n\n\n\n\n\n\nTest: Run\n0.0042389\n0.0651068\n.\n\n\n\n\n\n\n\n\nTest: S20_r\n0.0122535\n0.1106954\n.\n.\n\n\n\n\n\n\n\nTest: SLJ\n0.0080210\n0.0895599\n.\n.\n.\n\n\n\n\n\n\nTest: Star_r\n0.0009828\n0.0313498\n.\n.\n.\n.\n\n\n\n\nResidual\n\n0.3456872\n0.5879517\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4 Restore new LMM m1_om Star and S20 in original metric\n\nCommand for fitting LMM m1_om = fit(MixedModel, f1, dat_om, contrasts=contr)\nMinimizing 10502 Time: 0 Time: 2:09:40 ( 0.74 s/it)\nStore with: julia&gt; saveoptsum(‚Äú./fits/fggk21_m1_om_optsum.json‚Äù, m1_om)\nOnly for short-term and when desperate: julia&gt; serialize(‚Äú./fits/m1_om.jls‚Äù, m1_om);\n\n\n2.4.1 ‚Ä¶ restoreoptsum!()\n\nm1_om = LinearMixedModel(f1, dat; contrasts);\nrestoreoptsum!(m1_om, \"./fits/fggk21_m1_om_optsum.json\");\n\n\n\n2.4.2 ‚Ä¶ deserialize()\n\nm1x_om = deserialize(\"./fits/m1_om.jls\")\n\n\nVarCorr(m1x_om)\n\n\n\n\n2.5 Residual diagnostics for LMM m1\nResidual plots for published LMM\n\n#scatter(fitted(m1x), residuals(m1x)\n\n\n#qqnorm(m1x)\n\n\n\n2.6 Residual diagnostics for LMM m1_om\nResidual plots for LMM with Star and Speed in original metric.\n\n#scatter(fitted(m1_om_v2), residuals(m1_om_v2)\n\n\n#qqnorm(m1_om_v2)\n\n\n\nF√ºhner, T., Granacher, U., Golle, K., & Kliegl, R. (2021). Age and sex effects in physical fitness components of 108,295 third graders including 515 primary schools and 9 cohorts. Scientific Reports, 11(1). https://doi.org/10.1038/s41598-021-97000-4\n\n\nThis page was rendered from git revision fb99b3c\n using Quarto 1.7.33."
  },
  {
    "objectID": "arrow.html",
    "href": "arrow.html",
    "title": "Saving data with Arrow",
    "section": "",
    "text": "1 The Arrow storage format\nThe Arrow storage format provides a language-agnostic storage and memory specification for columnar data tables, which just means ‚Äúsomething that looks like a data frame in R‚Äù. That is, an arrow table is an ordered, named collection of columns, all of the same length.\nThe columns can be of different types including numeric values, character strings, and factor-like representations - called DictEncoded.\nAn Arrow file can be read or written from R, Python, Julia and many other languages. Somewhat confusingly in R and Python the name feather, which refers to an earlier version of the storage format, is used in some function names like read_feather.\nInternally, the SMLP2026 package uses Arrow to store all of its datasets.\n\n\n2 The Emotikon data\nThe SMLP2021 repository contains a version of the data from F√ºhner et al. (2021) in notebooks/data/fggk21.arrow. After that file was created there were changes in the master RDS file on the osf.io site for the project. We will recreate the Arrow file here then split it into two separate tables, one with a row for each child in the study and one with a row for each test result.\nThe Arrow package for Julia does not export any function names, which means that the function to read an Arrow file must be called as Arrow.Table. It returns a column table, as described in the Tables package. This is like a read-only data frame, which can be easily converted to a full-fledged DataFrame if desired.\nThis arrangement allows for the Arrow package not to depend on the DataFrames package, which is a heavy-weight dependency, but still easily produce a DataFrame if warranted.\nLoad the packages to be used.\n\n\nCode\nusing AlgebraOfGraphics\nusing Arrow\nusing CairoMakie\nusing Chain\nusing DataFrameMacros\nusing DataFrames\nusing Downloads\nusing KernelDensity\nusing RCall   # access R from within Julia\nusing StatsBase\n\nCairoMakie.activate!(; type=\"svg\")\nusing AlgebraOfGraphics: density\n\n\n\n\n3 Downloading and importing the RDS file\nThis is similar to some of the code shown by Julius Krumbiegel on Monday. In the data directory of the emotikon project on osf.io under Data, the url for the rds data file is found to be [https://osf.io/xawdb/]. Note that we want version 2 of this file.\n\nfn = Downloads.download(\"https://osf.io/xawdb/download?version=2\");\n\n\ndfrm = rcopy(reval(\"\"\"readRDS(\"$(fn)\")\"\"\"))\n\n525126√ó7 DataFrame525101 rows omitted\n\n\n\nRow\nCohort\nSchool\nChild\nSex\nage\nTest\nscore\n\n\n\nCat‚Ä¶\nCat‚Ä¶\nCat‚Ä¶\nCat‚Ä¶\nFloat64\nCat‚Ä¶\nFloat64\n\n\n\n\n1\n2013\nS100067\nC002352\nmale\n7.99452\nS20_r\n5.26316\n\n\n2\n2013\nS100067\nC002352\nmale\n7.99452\nBPT\n3.7\n\n\n3\n2013\nS100067\nC002352\nmale\n7.99452\nSLJ\n125.0\n\n\n4\n2013\nS100067\nC002352\nmale\n7.99452\nStar_r\n2.47146\n\n\n5\n2013\nS100067\nC002352\nmale\n7.99452\nRun\n1053.0\n\n\n6\n2013\nS100067\nC002353\nmale\n7.99452\nS20_r\n5.0\n\n\n7\n2013\nS100067\nC002353\nmale\n7.99452\nBPT\n4.1\n\n\n8\n2013\nS100067\nC002353\nmale\n7.99452\nSLJ\n116.0\n\n\n9\n2013\nS100067\nC002353\nmale\n7.99452\nStar_r\n1.76778\n\n\n10\n2013\nS100067\nC002353\nmale\n7.99452\nRun\n1089.0\n\n\n11\n2013\nS100067\nC002354\nmale\n7.99452\nS20_r\n4.54545\n\n\n12\n2013\nS100067\nC002354\nmale\n7.99452\nBPT\n3.9\n\n\n13\n2013\nS100067\nC002354\nmale\n7.99452\nSLJ\n111.0\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n525115\n2018\nS401470\nC117964\nmale\n9.10609\nStar_r\n1.63704\n\n\n525116\n2018\nS401470\nC117964\nmale\n9.10609\nRun\n864.0\n\n\n525117\n2018\nS401470\nC117965\nfemale\n9.10609\nS20_r\n4.65116\n\n\n525118\n2018\nS401470\nC117965\nfemale\n9.10609\nBPT\n3.8\n\n\n525119\n2018\nS401470\nC117965\nfemale\n9.10609\nSLJ\n123.0\n\n\n525120\n2018\nS401470\nC117965\nfemale\n9.10609\nStar_r\n1.52889\n\n\n525121\n2018\nS401470\nC117965\nfemale\n9.10609\nRun\n1080.0\n\n\n525122\n2018\nS800200\nC117966\nmale\n9.10609\nS20_r\n4.54545\n\n\n525123\n2018\nS800200\nC117966\nmale\n9.10609\nBPT\n3.8\n\n\n525124\n2018\nS800200\nC117966\nmale\n9.10609\nSLJ\n100.0\n\n\n525125\n2018\nS800200\nC117966\nmale\n9.10609\nStar_r\n2.18506\n\n\n525126\n2018\nS800200\nC117966\nmale\n9.10609\nRun\n990.0\n\n\n\n\n\n\nNow write this file as a Arrow file and read it back in.\n\narrowfn = joinpath(\"data\", \"fggk21.arrow\")\nArrow.write(arrowfn, dfrm; compress=:lz4)\ntbl = Arrow.Table(arrowfn)\n\nArrow.Table with 525126 rows, 7 columns, and schema:\n :Cohort  CategoricalArrays.CategoricalValue{String, UInt32}\n :School  CategoricalArrays.CategoricalValue{String, UInt32}\n :Child   CategoricalArrays.CategoricalValue{String, UInt32}\n :Sex     CategoricalArrays.CategoricalValue{String, UInt32}\n :age     Float64\n :Test    CategoricalArrays.CategoricalValue{String, UInt32}\n :score   Float64\n\n\n\nfilesize(arrowfn)\n\n3079506\n\n\n\ndf = DataFrame(tbl)\n\n525126√ó7 DataFrame525101 rows omitted\n\n\n\nRow\nCohort\nSchool\nChild\nSex\nage\nTest\nscore\n\n\n\nCat‚Ä¶\nCat‚Ä¶\nCat‚Ä¶\nCat‚Ä¶\nFloat64\nCat‚Ä¶\nFloat64\n\n\n\n\n1\n2013\nS100067\nC002352\nmale\n7.99452\nS20_r\n5.26316\n\n\n2\n2013\nS100067\nC002352\nmale\n7.99452\nBPT\n3.7\n\n\n3\n2013\nS100067\nC002352\nmale\n7.99452\nSLJ\n125.0\n\n\n4\n2013\nS100067\nC002352\nmale\n7.99452\nStar_r\n2.47146\n\n\n5\n2013\nS100067\nC002352\nmale\n7.99452\nRun\n1053.0\n\n\n6\n2013\nS100067\nC002353\nmale\n7.99452\nS20_r\n5.0\n\n\n7\n2013\nS100067\nC002353\nmale\n7.99452\nBPT\n4.1\n\n\n8\n2013\nS100067\nC002353\nmale\n7.99452\nSLJ\n116.0\n\n\n9\n2013\nS100067\nC002353\nmale\n7.99452\nStar_r\n1.76778\n\n\n10\n2013\nS100067\nC002353\nmale\n7.99452\nRun\n1089.0\n\n\n11\n2013\nS100067\nC002354\nmale\n7.99452\nS20_r\n4.54545\n\n\n12\n2013\nS100067\nC002354\nmale\n7.99452\nBPT\n3.9\n\n\n13\n2013\nS100067\nC002354\nmale\n7.99452\nSLJ\n111.0\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n525115\n2018\nS401470\nC117964\nmale\n9.10609\nStar_r\n1.63704\n\n\n525116\n2018\nS401470\nC117964\nmale\n9.10609\nRun\n864.0\n\n\n525117\n2018\nS401470\nC117965\nfemale\n9.10609\nS20_r\n4.65116\n\n\n525118\n2018\nS401470\nC117965\nfemale\n9.10609\nBPT\n3.8\n\n\n525119\n2018\nS401470\nC117965\nfemale\n9.10609\nSLJ\n123.0\n\n\n525120\n2018\nS401470\nC117965\nfemale\n9.10609\nStar_r\n1.52889\n\n\n525121\n2018\nS401470\nC117965\nfemale\n9.10609\nRun\n1080.0\n\n\n525122\n2018\nS800200\nC117966\nmale\n9.10609\nS20_r\n4.54545\n\n\n525123\n2018\nS800200\nC117966\nmale\n9.10609\nBPT\n3.8\n\n\n525124\n2018\nS800200\nC117966\nmale\n9.10609\nSLJ\n100.0\n\n\n525125\n2018\nS800200\nC117966\nmale\n9.10609\nStar_r\n2.18506\n\n\n525126\n2018\nS800200\nC117966\nmale\n9.10609\nRun\n990.0\n\n\n\n\n\n\n\n\n4 Avoiding needless repetition\nOne of the principles of relational database design is that information should not be repeated needlessly. Each row of df is determined by a combination of Child and Test, together producing a score, which can be converted to a zScore.\nThe other columns in the table, Cohort, School, age, and Sex, are properties of the Child.\nStoring these values redundantly in the full table takes up space but, more importantly, allows for inconsistency. As it stands, a given Child could be recorded as being in one Cohort for the Run test and in another Cohort for the S20_r test and nothing about the table would detect this as being an error.\nThe approach used in relational databases is to store the information for score in one table that contains only Child, Test and score, store the information for the Child in another table including Cohort, School, age and Sex. These tables can then be combined to create the table to be used for analysis by joining the different tables together.\nThe maintainers of the DataFrames package have put in a lot of work over the past few years to make joins quite efficient in Julia. Thus the processing penalty of reassembling the big table from three smaller tables is minimal.\nIt is important to note that the main advantage of using smaller tables that are joined together to produce the analysis table is the fact that the information in the analysis table is consistent by design.\n\n\n5 Creating the smaller table\n\nChild = unique(select(df, :Child, :School, :Cohort, :Sex, :age))\n\n108295√ó5 DataFrame108270 rows omitted\n\n\n\nRow\nChild\nSchool\nCohort\nSex\nage\n\n\n\nCat‚Ä¶\nCat‚Ä¶\nCat‚Ä¶\nCat‚Ä¶\nFloat64\n\n\n\n\n1\nC002352\nS100067\n2013\nmale\n7.99452\n\n\n2\nC002353\nS100067\n2013\nmale\n7.99452\n\n\n3\nC002354\nS100067\n2013\nmale\n7.99452\n\n\n4\nC002355\nS100122\n2013\nfemale\n7.99452\n\n\n5\nC002356\nS100146\n2013\nmale\n7.99452\n\n\n6\nC002357\nS100146\n2013\nmale\n7.99452\n\n\n7\nC002358\nS100146\n2013\nmale\n7.99452\n\n\n8\nC002359\nS100183\n2013\nfemale\n7.99452\n\n\n9\nC002360\nS100195\n2013\nfemale\n7.99452\n\n\n10\nC002361\nS100213\n2013\nmale\n7.99452\n\n\n11\nC002362\nS100237\n2013\nfemale\n7.99452\n\n\n12\nC002363\nS100237\n2013\nfemale\n7.99452\n\n\n13\nC002364\nS100250\n2013\nfemale\n7.99452\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n108284\nC117943\nS130539\n2018\nfemale\n9.10609\n\n\n108285\nC117944\nS130539\n2018\nmale\n9.10609\n\n\n108286\nC117945\nS130539\n2018\nmale\n9.10609\n\n\n108287\nC117946\nS130539\n2018\nmale\n9.10609\n\n\n108288\nC117956\nS400580\n2018\nmale\n9.10609\n\n\n108289\nC117957\nS400919\n2018\nmale\n9.10609\n\n\n108290\nC117958\nS400919\n2018\nmale\n9.10609\n\n\n108291\nC117959\nS400919\n2018\nmale\n9.10609\n\n\n108292\nC117962\nS401250\n2018\nfemale\n9.10609\n\n\n108293\nC117964\nS401470\n2018\nmale\n9.10609\n\n\n108294\nC117965\nS401470\n2018\nfemale\n9.10609\n\n\n108295\nC117966\nS800200\n2018\nmale\n9.10609\n\n\n\n\n\n\n\nlength(unique(Child.Child))  # should be 108295\n\n108295\n\n\n\nfilesize(\n  Arrow.write(\"./data/fggk21_Child.arrow\", Child; compress=:lz4)\n)\n\n1776266\n\n\n\nfilesize(\n  Arrow.write(\n    \"./data/fggk21_Score.arrow\",\n    select(df, :Child, :Test, :score);\n    compress=:lz4,\n  ),\n)\n\n2794706\n\n\n\n\n\n\n\n\nNote\n\n\n\nA careful examination of the file sizes versus that of ./data/fggk21.arrow will show that the separate tables combined take up more space than the original because of the compression. Compression algorithms are often more successful when applied to larger files.\n\n\nNow read the Arrow tables in and reassemble the original table.\n\nScore = DataFrame(Arrow.Table(\"./data/fggk21_Score.arrow\"))\n\n525126√ó3 DataFrame525101 rows omitted\n\n\n\nRow\nChild\nTest\nscore\n\n\n\nCat‚Ä¶\nCat‚Ä¶\nFloat64\n\n\n\n\n1\nC002352\nS20_r\n5.26316\n\n\n2\nC002352\nBPT\n3.7\n\n\n3\nC002352\nSLJ\n125.0\n\n\n4\nC002352\nStar_r\n2.47146\n\n\n5\nC002352\nRun\n1053.0\n\n\n6\nC002353\nS20_r\n5.0\n\n\n7\nC002353\nBPT\n4.1\n\n\n8\nC002353\nSLJ\n116.0\n\n\n9\nC002353\nStar_r\n1.76778\n\n\n10\nC002353\nRun\n1089.0\n\n\n11\nC002354\nS20_r\n4.54545\n\n\n12\nC002354\nBPT\n3.9\n\n\n13\nC002354\nSLJ\n111.0\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n525115\nC117964\nStar_r\n1.63704\n\n\n525116\nC117964\nRun\n864.0\n\n\n525117\nC117965\nS20_r\n4.65116\n\n\n525118\nC117965\nBPT\n3.8\n\n\n525119\nC117965\nSLJ\n123.0\n\n\n525120\nC117965\nStar_r\n1.52889\n\n\n525121\nC117965\nRun\n1080.0\n\n\n525122\nC117966\nS20_r\n4.54545\n\n\n525123\nC117966\nBPT\n3.8\n\n\n525124\nC117966\nSLJ\n100.0\n\n\n525125\nC117966\nStar_r\n2.18506\n\n\n525126\nC117966\nRun\n990.0\n\n\n\n\n\n\nAt this point we can create the z-score column by standardizing the scores for each Test. The code to do this follows Julius‚Äôs presentation on Monday.\n\n@transform!(groupby(Score, :Test), :zScore = @bycol zscore(:score))\n\n525126√ó4 DataFrame525101 rows omitted\n\n\n\nRow\nChild\nTest\nscore\nzScore\n\n\n\nCat‚Ä¶\nCat‚Ä¶\nFloat64\nFloat64\n\n\n\n\n1\nC002352\nS20_r\n5.26316\n1.7913\n\n\n2\nC002352\nBPT\n3.7\n-0.0622317\n\n\n3\nC002352\nSLJ\n125.0\n-0.0336567\n\n\n4\nC002352\nStar_r\n2.47146\n1.46874\n\n\n5\nC002352\nRun\n1053.0\n0.331058\n\n\n6\nC002353\nS20_r\n5.0\n1.15471\n\n\n7\nC002353\nBPT\n4.1\n0.498354\n\n\n8\nC002353\nSLJ\n116.0\n-0.498822\n\n\n9\nC002353\nStar_r\n1.76778\n-0.9773\n\n\n10\nC002353\nRun\n1089.0\n0.574056\n\n\n11\nC002354\nS20_r\n4.54545\n0.0551481\n\n\n12\nC002354\nBPT\n3.9\n0.218061\n\n\n13\nC002354\nSLJ\n111.0\n-0.757248\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n525115\nC117964\nStar_r\n1.63704\n-1.43175\n\n\n525116\nC117964\nRun\n864.0\n-0.944681\n\n\n525117\nC117965\nS20_r\n4.65116\n0.31086\n\n\n525118\nC117965\nBPT\n3.8\n0.0779146\n\n\n525119\nC117965\nSLJ\n123.0\n-0.137027\n\n\n525120\nC117965\nStar_r\n1.52889\n-1.8077\n\n\n525121\nC117965\nRun\n1080.0\n0.513306\n\n\n525122\nC117966\nS20_r\n4.54545\n0.0551481\n\n\n525123\nC117966\nBPT\n3.8\n0.0779146\n\n\n525124\nC117966\nSLJ\n100.0\n-1.32578\n\n\n525125\nC117966\nStar_r\n2.18506\n0.473217\n\n\n525126\nC117966\nRun\n990.0\n-0.0941883\n\n\n\n\n\n\n\nChild = DataFrame(Arrow.Table(\"./data/fggk21_Child.arrow\"))\n\n108295√ó5 DataFrame108270 rows omitted\n\n\n\nRow\nChild\nSchool\nCohort\nSex\nage\n\n\n\nCat‚Ä¶\nCat‚Ä¶\nCat‚Ä¶\nCat‚Ä¶\nFloat64\n\n\n\n\n1\nC002352\nS100067\n2013\nmale\n7.99452\n\n\n2\nC002353\nS100067\n2013\nmale\n7.99452\n\n\n3\nC002354\nS100067\n2013\nmale\n7.99452\n\n\n4\nC002355\nS100122\n2013\nfemale\n7.99452\n\n\n5\nC002356\nS100146\n2013\nmale\n7.99452\n\n\n6\nC002357\nS100146\n2013\nmale\n7.99452\n\n\n7\nC002358\nS100146\n2013\nmale\n7.99452\n\n\n8\nC002359\nS100183\n2013\nfemale\n7.99452\n\n\n9\nC002360\nS100195\n2013\nfemale\n7.99452\n\n\n10\nC002361\nS100213\n2013\nmale\n7.99452\n\n\n11\nC002362\nS100237\n2013\nfemale\n7.99452\n\n\n12\nC002363\nS100237\n2013\nfemale\n7.99452\n\n\n13\nC002364\nS100250\n2013\nfemale\n7.99452\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n108284\nC117943\nS130539\n2018\nfemale\n9.10609\n\n\n108285\nC117944\nS130539\n2018\nmale\n9.10609\n\n\n108286\nC117945\nS130539\n2018\nmale\n9.10609\n\n\n108287\nC117946\nS130539\n2018\nmale\n9.10609\n\n\n108288\nC117956\nS400580\n2018\nmale\n9.10609\n\n\n108289\nC117957\nS400919\n2018\nmale\n9.10609\n\n\n108290\nC117958\nS400919\n2018\nmale\n9.10609\n\n\n108291\nC117959\nS400919\n2018\nmale\n9.10609\n\n\n108292\nC117962\nS401250\n2018\nfemale\n9.10609\n\n\n108293\nC117964\nS401470\n2018\nmale\n9.10609\n\n\n108294\nC117965\nS401470\n2018\nfemale\n9.10609\n\n\n108295\nC117966\nS800200\n2018\nmale\n9.10609\n\n\n\n\n\n\n\ndf1 = disallowmissing!(leftjoin(Score, Child; on=:Child))\n\n525126√ó8 DataFrame525101 rows omitted\n\n\n\nRow\nChild\nTest\nscore\nzScore\nSchool\nCohort\nSex\nage\n\n\n\nCat‚Ä¶\nCat‚Ä¶\nFloat64\nFloat64\nCat‚Ä¶\nCat‚Ä¶\nCat‚Ä¶\nFloat64\n\n\n\n\n1\nC002352\nS20_r\n5.26316\n1.7913\nS100067\n2013\nmale\n7.99452\n\n\n2\nC002352\nBPT\n3.7\n-0.0622317\nS100067\n2013\nmale\n7.99452\n\n\n3\nC002352\nSLJ\n125.0\n-0.0336567\nS100067\n2013\nmale\n7.99452\n\n\n4\nC002352\nStar_r\n2.47146\n1.46874\nS100067\n2013\nmale\n7.99452\n\n\n5\nC002352\nRun\n1053.0\n0.331058\nS100067\n2013\nmale\n7.99452\n\n\n6\nC002353\nS20_r\n5.0\n1.15471\nS100067\n2013\nmale\n7.99452\n\n\n7\nC002353\nBPT\n4.1\n0.498354\nS100067\n2013\nmale\n7.99452\n\n\n8\nC002353\nSLJ\n116.0\n-0.498822\nS100067\n2013\nmale\n7.99452\n\n\n9\nC002353\nStar_r\n1.76778\n-0.9773\nS100067\n2013\nmale\n7.99452\n\n\n10\nC002353\nRun\n1089.0\n0.574056\nS100067\n2013\nmale\n7.99452\n\n\n11\nC002354\nS20_r\n4.54545\n0.0551481\nS100067\n2013\nmale\n7.99452\n\n\n12\nC002354\nBPT\n3.9\n0.218061\nS100067\n2013\nmale\n7.99452\n\n\n13\nC002354\nSLJ\n111.0\n-0.757248\nS100067\n2013\nmale\n7.99452\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n525115\nC117964\nStar_r\n1.63704\n-1.43175\nS401470\n2018\nmale\n9.10609\n\n\n525116\nC117964\nRun\n864.0\n-0.944681\nS401470\n2018\nmale\n9.10609\n\n\n525117\nC117965\nS20_r\n4.65116\n0.31086\nS401470\n2018\nfemale\n9.10609\n\n\n525118\nC117965\nBPT\n3.8\n0.0779146\nS401470\n2018\nfemale\n9.10609\n\n\n525119\nC117965\nSLJ\n123.0\n-0.137027\nS401470\n2018\nfemale\n9.10609\n\n\n525120\nC117965\nStar_r\n1.52889\n-1.8077\nS401470\n2018\nfemale\n9.10609\n\n\n525121\nC117965\nRun\n1080.0\n0.513306\nS401470\n2018\nfemale\n9.10609\n\n\n525122\nC117966\nS20_r\n4.54545\n0.0551481\nS800200\n2018\nmale\n9.10609\n\n\n525123\nC117966\nBPT\n3.8\n0.0779146\nS800200\n2018\nmale\n9.10609\n\n\n525124\nC117966\nSLJ\n100.0\n-1.32578\nS800200\n2018\nmale\n9.10609\n\n\n525125\nC117966\nStar_r\n2.18506\n0.473217\nS800200\n2018\nmale\n9.10609\n\n\n525126\nC117966\nRun\n990.0\n-0.0941883\nS800200\n2018\nmale\n9.10609\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe call to disallowmissing! is because the join will create columns that allow for missing values but we know that we should not get missing values in the result. This call will fail if, for some reason, missing values were created.\n\n\n\n\n6 Discovering patterns in the data\nOne of the motivations for creating the Child table was to be able to bin the ages according to the age of each child, not the age of each Child-Test combination. Not all children have all 5 test results. We can check the number of results by grouping on :Child and evaluate the number of rows in each group.\n\nnobsChild = combine(groupby(Score, :Child), nrow =&gt; :ntest)\n\n108295√ó2 DataFrame108270 rows omitted\n\n\n\nRow\nChild\nntest\n\n\n\nCat‚Ä¶\nInt64\n\n\n\n\n1\nC002352\n5\n\n\n2\nC002353\n5\n\n\n3\nC002354\n5\n\n\n4\nC002355\n5\n\n\n5\nC002356\n5\n\n\n6\nC002357\n5\n\n\n7\nC002358\n5\n\n\n8\nC002359\n4\n\n\n9\nC002360\n5\n\n\n10\nC002361\n4\n\n\n11\nC002362\n5\n\n\n12\nC002363\n5\n\n\n13\nC002364\n5\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n108284\nC117943\n5\n\n\n108285\nC117944\n5\n\n\n108286\nC117945\n5\n\n\n108287\nC117946\n5\n\n\n108288\nC117956\n5\n\n\n108289\nC117957\n4\n\n\n108290\nC117958\n5\n\n\n108291\nC117959\n5\n\n\n108292\nC117962\n5\n\n\n108293\nC117964\n5\n\n\n108294\nC117965\n5\n\n\n108295\nC117966\n5\n\n\n\n\n\n\nNow create a table of the number of children with 1, 2, ‚Ä¶, 5 test scores.\n\ncombine(groupby(nobsChild, :ntest), nrow)\n\n5√ó2 DataFrame\n\n\n\nRow\nntest\nnrow\n\n\n\nInt64\nInt64\n\n\n\n\n1\n1\n462\n\n\n2\n2\n729\n\n\n3\n3\n1739\n\n\n4\n4\n8836\n\n\n5\n5\n96529\n\n\n\n\n\n\nA natural question at this point is whether there is something about those students who have few observations. For example, are they from only a few schools?\nOne approach to examining properties like is to add the number of observations for each child to the :Child table. Later we can group the table according to this :ntest to look at properties of :Child by :ntest.\n\ngdf = groupby(\n  disallowmissing!(leftjoin(Child, nobsChild; on=:Child)), :ntest\n)\n\nGroupedDataFrame with 5 groups based on key: ntestFirst Group (462 rows): ntest = 1437 rows omitted\n\n\n\nRow\nChild\nSchool\nCohort\nSex\nage\nntest\n\n\n\nCat‚Ä¶\nCat‚Ä¶\nCat‚Ä¶\nCat‚Ä¶\nFloat64\nInt64\n\n\n\n\n1\nC002452\nS101175\n2013\nmale\n7.99452\n1\n\n\n2\nC002625\nS103329\n2013\nmale\n7.99452\n1\n\n\n3\nC002754\nS104814\n2013\nfemale\n7.99452\n1\n\n\n4\nC003269\nS102258\n2012\nfemale\n7.99726\n1\n\n\n5\nC003599\nS105843\n2012\nfemale\n7.99726\n1\n\n\n6\nC003807\nS100754\n2011\nmale\n8.0\n1\n\n\n7\nC003985\nS102945\n2011\nmale\n8.0\n1\n\n\n8\nC004086\nS104255\n2011\nmale\n8.0\n1\n\n\n9\nC004657\nS101400\n2014\nmale\n8.03833\n1\n\n\n10\nC005036\nS105909\n2014\nmale\n8.03833\n1\n\n\n11\nC005440\nS101023\n2019\nmale\n8.05202\n1\n\n\n12\nC005523\nS101825\n2019\nfemale\n8.05202\n1\n\n\n13\nC005697\nS103615\n2019\nmale\n8.05202\n1\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n451\nC112638\nS103718\n2015\nfemale\n9.0486\n1\n\n\n452\nC114749\nS112938\n2017\nmale\n9.06502\n1\n\n\n453\nC115460\nS101953\n2015\nmale\n9.08145\n1\n\n\n454\nC115569\nS100572\n2017\nmale\n9.08419\n1\n\n\n455\nC115587\nS100754\n2017\nfemale\n9.08419\n1\n\n\n456\nC117108\nS103196\n2018\nfemale\n9.10609\n1\n\n\n457\nC117229\nS103615\n2018\nmale\n9.10609\n1\n\n\n458\nC117230\nS103615\n2018\nmale\n9.10609\n1\n\n\n459\nC117419\nS104954\n2018\nfemale\n9.10609\n1\n\n\n460\nC117437\nS105004\n2018\nmale\n9.10609\n1\n\n\n461\nC117539\nS105636\n2018\nmale\n9.10609\n1\n\n\n462\nC117659\nS106483\n2018\nfemale\n9.10609\n1\n\n\n\n‚ãÆLast Group (96529 rows): ntest = 596504 rows omitted\n\n\n\nRow\nChild\nSchool\nCohort\nSex\nage\nntest\n\n\n\nCat‚Ä¶\nCat‚Ä¶\nCat‚Ä¶\nCat‚Ä¶\nFloat64\nInt64\n\n\n\n\n1\nC002352\nS100067\n2013\nmale\n7.99452\n5\n\n\n2\nC002353\nS100067\n2013\nmale\n7.99452\n5\n\n\n3\nC002354\nS100067\n2013\nmale\n7.99452\n5\n\n\n4\nC002355\nS100122\n2013\nfemale\n7.99452\n5\n\n\n5\nC002356\nS100146\n2013\nmale\n7.99452\n5\n\n\n6\nC002357\nS100146\n2013\nmale\n7.99452\n5\n\n\n7\nC002358\nS100146\n2013\nmale\n7.99452\n5\n\n\n8\nC002360\nS100195\n2013\nfemale\n7.99452\n5\n\n\n9\nC002362\nS100237\n2013\nfemale\n7.99452\n5\n\n\n10\nC002363\nS100237\n2013\nfemale\n7.99452\n5\n\n\n11\nC002364\nS100250\n2013\nfemale\n7.99452\n5\n\n\n12\nC002365\nS100304\n2013\nmale\n7.99452\n5\n\n\n13\nC002366\nS100304\n2013\nmale\n7.99452\n5\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n96518\nC117942\nS130539\n2018\nmale\n9.10609\n5\n\n\n96519\nC117943\nS130539\n2018\nfemale\n9.10609\n5\n\n\n96520\nC117944\nS130539\n2018\nmale\n9.10609\n5\n\n\n96521\nC117945\nS130539\n2018\nmale\n9.10609\n5\n\n\n96522\nC117946\nS130539\n2018\nmale\n9.10609\n5\n\n\n96523\nC117956\nS400580\n2018\nmale\n9.10609\n5\n\n\n96524\nC117958\nS400919\n2018\nmale\n9.10609\n5\n\n\n96525\nC117959\nS400919\n2018\nmale\n9.10609\n5\n\n\n96526\nC117962\nS401250\n2018\nfemale\n9.10609\n5\n\n\n96527\nC117964\nS401470\n2018\nmale\n9.10609\n5\n\n\n96528\nC117965\nS401470\n2018\nfemale\n9.10609\n5\n\n\n96529\nC117966\nS800200\n2018\nmale\n9.10609\n5\n\n\n\n\n\n\nAre the sexes represented more-or-less equally?\n\ncombine(groupby(first(gdf), :Sex), nrow =&gt; :nchild)\n\n2√ó2 DataFrame\n\n\n\nRow\nSex\nnchild\n\n\n\nCat‚Ä¶\nInt64\n\n\n\n\n1\nmale\n230\n\n\n2\nfemale\n232\n\n\n\n\n\n\n\ncombine(groupby(last(gdf), :Sex), nrow =&gt; :nchild)\n\n2√ó2 DataFrame\n\n\n\nRow\nSex\nnchild\n\n\n\nCat‚Ä¶\nInt64\n\n\n\n\n1\nmale\n47552\n\n\n2\nfemale\n48977\n\n\n\n\n\n\n\n\n7 Reading Arrow files in other languages\nThere are Arrow implementations for R (the arrow package) and for Python (pyarrow).\n\nimport pyarrow.feather: read_table\nread_table(\"./data/fggk21.arrow\")\n\n\nlibrary(arrow)\nread_ipc_file(\"./data/fggk21.arrow\")\n\n\n\n8 References\n\n\nF√ºhner, T., Granacher, U., Golle, K., & Kliegl, R. (2021). Age and sex effects in physical fitness components of 108,295 third graders including 515 primary schools and 9 cohorts. Scientific Reports, 11(1). https://doi.org/10.1038/s41598-021-97000-4\n\n\nThis page was rendered from git revision fb99b3c\n using Quarto 1.7.33.\n\n\n\n\n Back to top",
    "crumbs": [
      "Getting started with Julia",
      "Saving data with Arrow"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\n Back to top"
  },
  {
    "objectID": "AoGPlots.html",
    "href": "AoGPlots.html",
    "title": "Creating multi-panel plots",
    "section": "",
    "text": "This notebook shows creating a multi-panel plot similar to Figure 2 of F√ºhner et al. (2021).\nThe data are available from the SMLP2026 example datasets.\n\n\nCode\nusing Arrow\nusing AlgebraOfGraphics\nusing CairoMakie   # for displaying static plots\nusing DataFrames\nusing Statistics\nusing StatsBase\nusing SMLP2026: dataset\n\n\n\ntbl = dataset(\"fggk21\")\n\nArrow.Table with 525126 rows, 7 columns, and schema:\n :Cohort  String\n :School  String\n :Child   String\n :Sex     String\n :age     Float64\n :Test    String\n :score   Float64\n\n\n\ntypeof(tbl)\n\nArrow.Table\n\n\n\ndf = DataFrame(tbl)\ntypeof(df)\n\nDataFrame\n\n\n\n1 Creating a summary data frame\nThe response to be plotted is the mean score by Test and Sex and age, rounded to the nearest 0.1 years.\nThe first task is to round the age to 1 digit after the decimal place, which can be done with select applied to a DataFrame. In some ways this is the most complicated expression in creating the plot so we will break it down. select is applied to DataFrame(dat), which is the conversion of the Arrow.Table, dat, to a DataFrame. This is necessary because an Arrow.Table is immutable but a DataFrame can be modified.\nThe arguments after the DataFrame describe how to modify the contents. The first : indicates that all the existing columns should be included. The other expression can be pairs (created with the =&gt; operator) of the form :col =&gt; function or of the form :col =&gt; function =&gt; :newname. (See the documentation of the DataFrames package for details.)\nIn this case the function is an anonymous function of the form round.(x, digits=1) where ‚Äúdot-broadcasting‚Äù is used to apply to the entire column (see this documentation for details).\n\ntransform!(df, :age, :age =&gt; (x -&gt; x .- 8.5) =&gt; :a1) # centered age (linear)\nselect!(groupby(df, :Test), :, :score =&gt; zscore =&gt; :zScore) # z-score\ntlabels = [     # establish order and labels of tbl.Test\n  \"Run\" =&gt; \"Endurance\",\n  \"Star_r\" =&gt; \"Coordination\",\n  \"S20_r\" =&gt; \"Speed\",\n  \"SLJ\" =&gt; \"PowerLOW\",\n  \"BPT\" =&gt; \"PowerUP\",\n];\n\nThe next stage is a group-apply-combine operation to group the rows by Sex, Test and rnd_age then apply mean to the zScore and also apply length to zScore to record the number in each group.\n\ndf2 = combine(\n  groupby(\n    select(df, :, :age =&gt; ByRow(x -&gt; round(x; digits=1)) =&gt; :age),\n    [:Sex, :Test, :age],\n  ),\n  :zScore =&gt; mean =&gt; :zScore,\n  :zScore =&gt; length =&gt; :n,\n)\n\n120√ó5 DataFrame95 rows omitted\n\n\n\nRow\nSex\nTest\nage\nzScore\nn\n\n\n\nString\nString\nFloat64\nFloat64\nInt64\n\n\n\n\n1\nmale\nS20_r\n8.0\n-0.0265138\n1223\n\n\n2\nmale\nBPT\n8.0\n0.026973\n1227\n\n\n3\nmale\nSLJ\n8.0\n0.121609\n1227\n\n\n4\nmale\nStar_r\n8.0\n-0.0571726\n1186\n\n\n5\nmale\nRun\n8.0\n0.292695\n1210\n\n\n6\nfemale\nS20_r\n8.0\n-0.35164\n1411\n\n\n7\nfemale\nBPT\n8.0\n-0.610355\n1417\n\n\n8\nfemale\nSLJ\n8.0\n-0.279872\n1418\n\n\n9\nfemale\nStar_r\n8.0\n-0.268221\n1381\n\n\n10\nfemale\nRun\n8.0\n-0.245573\n1387\n\n\n11\nmale\nS20_r\n8.1\n0.0608397\n3042\n\n\n12\nmale\nBPT\n8.1\n0.0955413\n3069\n\n\n13\nmale\nSLJ\n8.1\n0.123099\n3069\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n109\nmale\nStar_r\n9.0\n0.254973\n4049\n\n\n110\nmale\nRun\n9.0\n0.258082\n4034\n\n\n111\nfemale\nS20_r\n9.1\n-0.0286172\n1154\n\n\n112\nfemale\nBPT\n9.1\n-0.0752301\n1186\n\n\n113\nfemale\nSLJ\n9.1\n-0.094587\n1174\n\n\n114\nfemale\nStar_r\n9.1\n0.00276252\n1162\n\n\n115\nfemale\nRun\n9.1\n-0.235591\n1150\n\n\n116\nmale\nS20_r\n9.1\n0.325745\n1303\n\n\n117\nmale\nBPT\n9.1\n0.616416\n1320\n\n\n118\nmale\nSLJ\n9.1\n0.267577\n1310\n\n\n119\nmale\nStar_r\n9.1\n0.254342\n1297\n\n\n120\nmale\nRun\n9.1\n0.251045\n1294\n\n\n\n\n\n\n\n\n2 Creating the plot\nThe AlgebraOfGraphics package applies operators to the results of functions such as data (specify the data table to be used), mapping (designate the roles of columns), and visual (type of visual presentation).\n\nlet\n  design = mapping(:age, :zScore; color=:Sex, col=:Test)\n  lines = design * linear()\n  means = design * visual(Scatter; markersize=5)\n  draw(data(df2) * means + data(df) * lines)\nend\n\n\n\n\n\nTBD: Relabel factor levels (Boys, Girls; fitness components for Test)\nTBD: Relevel factors; why not levels from Tables?\nTBD: Set range (7.8 to 9.2 and tick marks (8, 8.5, 9) of axes.\nTBD: Move legend in plot?\n\n\n\nF√ºhner, T., Granacher, U., Golle, K., & Kliegl, R. (2021). Age and sex effects in physical fitness components of 108,295 third graders including 515 primary schools and 9 cohorts. Scientific Reports, 11(1). https://doi.org/10.1038/s41598-021-97000-4\n\n\nThis page was rendered from git revision fb99b3c\n using Quarto 1.7.33.\n\n\n\n\n Back to top",
    "crumbs": [
      "Visualizations and diagnostics",
      "Creating multi-panel plots"
    ]
  },
  {
    "objectID": "bootstrap.html",
    "href": "bootstrap.html",
    "title": "Parametric bootstrap for mixed-effects models",
    "section": "",
    "text": "The speed of MixedModels.jl relative to its predecessors makes the parametric bootstrap much more computationally tractable. This is valuable because the parametric bootstrap can be used to produce more accurate confidence intervals than methods based on standard errors or profiling of the likelihood surface.\nThis page is adapted from the MixedModels.jl docs\n\n1 The parametric bootstrap\nBootstrapping is a family of procedures for generating sample values of a statistic, allowing for visualization of the distribution of the statistic or for inference from this sample of values. Bootstrapping also belongs to a larger family of procedures called resampling, which are based on creating new samples of data from an existing one, then computing statistics on the new samples, in order to examine the distribution of the relevant statistics.\nA parametric bootstrap is used with a parametric model, m, that has been fit to data. The procedure is to simulate n response vectors from m using the estimated parameter values and refit m to these responses in turn, accumulating the statistics of interest at each iteration.\nThe parameters of a LinearMixedModel object are the fixed-effects parameters (Œ≤), the standard deviation (œÉ), of the per-observation noise, and the covariance parameter (Œ∏), that defines the variance-covariance matrices of the random effects. A technical description of the covariance parameter can be found in the MixedModels.jl docs. Lisa Schwetlick and Daniel Backhaus have provided a more beginner-friendly description of the covariance parameter in the documentation for MixedModelsSim.jl. For today‚Äôs purposes ‚Äì looking at the uncertainty in the estimates from a fitted model ‚Äì we can simply use values from the fitted model, but we will revisit the parametric bootstrap as a convenient way to simulate new data, potentially with different parameter values, for power analysis.\nAttach the packages to be used\n\n\nCode\nusing AlgebraOfGraphics\nusing CairoMakie\nusing DataFrames\nusing MixedModels\nusing MixedModelsMakie\nusing Random\nusing SMLP2026: dataset\n\nusing AlgebraOfGraphics: AlgebraOfGraphics as AoG\nconst progress=false\n\n\nNote that the precise stream of random numbers generated for a given seed can change between Julia versions. For exact reproducibility, you either need to have the exact same Julia version or use the StableRNGs package.\n\n\n2 A model of moderate complexity\nThe kb07 data (Kronm√ºller & Barr, 2007) are one of the datasets provided by the MixedModels package.\n\nkb07 = dataset(:kb07)\n\nArrow.Table with 1789 rows, 7 columns, and schema:\n :subj      String\n :item      String\n :spkr      String\n :prec      String\n :load      String\n :rt_trunc  Int16\n :rt_raw    Int16\n\n\nConvert the table to a DataFrame for summary.\n\nkb07 = DataFrame(kb07)\ndescribe(kb07)\n\n7√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nDataType\n\n\n\n\n1\nsubj\n\nS030\n\nS103\n0\nString\n\n\n2\nitem\n\nI01\n\nI32\n0\nString\n\n\n3\nspkr\n\nnew\n\nold\n0\nString\n\n\n4\nprec\n\nbreak\n\nmaintain\n0\nString\n\n\n5\nload\n\nno\n\nyes\n0\nString\n\n\n6\nrt_trunc\n2182.2\n579\n1940.0\n5171\n0\nInt16\n\n\n7\nrt_raw\n2226.24\n579\n1940.0\n15923\n0\nInt16\n\n\n\n\n\n\nThe experimental factors; spkr, prec, and load, are two-level factors.\n\ncontrasts = Dict(:spkr =&gt; EffectsCoding(),\n                 :prec =&gt; EffectsCoding(),\n                 :load =&gt; EffectsCoding(),\n)\n\nThe EffectsCoding contrast is used with these to create a ¬±1 encoding.\nWe can look at an initial fit of moderate complexity:\n\nform = @formula(rt_trunc ~ 1 + spkr * prec * load +\n                          (1 + spkr + prec + load | subj) +\n                          (1 + spkr + prec + load | item))\nm0 = fit(MixedModel, form, kb07; contrasts, progress)\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\nœÉ_item\n\n\n(Intercept)\n2181.6728\n77.3025\n28.22\n&lt;1e-99\n301.7795\n362.1969\n\n\nspkr: old\n67.7486\n18.2889\n3.70\n0.0002\n42.9237\n40.6934\n\n\nprec: maintain\n-333.9211\n47.1525\n-7.08\n&lt;1e-11\n61.9966\n246.8936\n\n\nload: yes\n78.7703\n19.5367\n4.03\n&lt;1e-04\n65.1301\n42.3692\n\n\nspkr: old & prec: maintain\n-21.9656\n15.8062\n-1.39\n0.1646\n\n\n\n\nspkr: old & load: yes\n18.3842\n15.8062\n1.16\n0.2448\n\n\n\n\nprec: maintain & load: yes\n4.5338\n15.8062\n0.29\n0.7742\n\n\n\n\nspkr: old & prec: maintain & load: yes\n23.6074\n15.8062\n1.49\n0.1353\n\n\n\n\nResidual\n668.5033\n\n\n\n\n\n\n\n\n\n\nThe default display in Quarto uses the pretty MIME show method for the model and omits the estimated correlations of the random effects.\nThe VarCorr extractor displays these.\n\nVarCorr(m0)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nsubj\n(Intercept)\n91070.8712\n301.7795\n\n\n\n\n\n\nspkr: old\n1842.4411\n42.9237\n+0.78\n\n\n\n\n\nprec: maintain\n3843.5766\n61.9966\n-0.59\n+0.03\n\n\n\n\nload: yes\n4241.9308\n65.1301\n+0.36\n+0.83\n+0.53\n\n\nitem\n(Intercept)\n131186.5676\n362.1969\n\n\n\n\n\n\nspkr: old\n1655.9495\n40.6934\n+0.44\n\n\n\n\n\nprec: maintain\n60956.4434\n246.8936\n-0.69\n+0.35\n\n\n\n\nload: yes\n1795.1488\n42.3692\n+0.32\n+0.16\n-0.14\n\n\nResidual\n\n446896.6953\n668.5033\n\n\n\n\n\n\n\n\nNone of the two-factor or three-factor interaction terms in the fixed-effects are significant. In the random-effects terms only the scalar random effects and the prec random effect for item appear to be warranted, leading to the reduced formula\n\n# formula f4 from https://doi.org/10.33016/nextjournal.100002\nform = @formula(rt_trunc ~ 1 + spkr * prec * load + (1 | subj) + (1 + prec | item))\n\nm1 = fit(MixedModel, form, kb07; contrasts, progress)\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n(Intercept)\n2181.7582\n77.4710\n28.16\n&lt;1e-99\n364.7293\n298.1107\n\n\nspkr: old\n67.8114\n16.0526\n4.22\n&lt;1e-04\n\n\n\n\nprec: maintain\n-333.8582\n47.4631\n-7.03\n&lt;1e-11\n252.6694\n\n\n\nload: yes\n78.6849\n16.0525\n4.90\n&lt;1e-06\n\n\n\n\nspkr: old & prec: maintain\n-21.8802\n16.0525\n-1.36\n0.1729\n\n\n\n\nspkr: old & load: yes\n18.3214\n16.0526\n1.14\n0.2537\n\n\n\n\nprec: maintain & load: yes\n4.4710\n16.0526\n0.28\n0.7806\n\n\n\n\nspkr: old & prec: maintain & load: yes\n23.5219\n16.0525\n1.47\n0.1428\n\n\n\n\nResidual\n678.9319\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m1)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\nitem\n(Intercept)\n133027.439\n364.729\n\n\n\n\nprec: maintain\n63841.834\n252.669\n-0.70\n\n\nsubj\n(Intercept)\n88870.013\n298.111\n\n\n\nResidual\n\n460948.573\n678.932\n\n\n\n\n\n\nThese two models are nested and can be compared with a likelihood-ratio test.\n\nMixedModels.likelihoodratiotest(m0, m1)\n\n\n\n\n\nmodel-dof\n-2 logLik\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\nrt_trunc ~ 1 + spkr + prec + load + spkr & prec + spkr & load + prec & load + spkr & prec & load + (1 + spkr + prec + load | subj) + (1 + spkr + prec + load | item)\n29\n-28637\n\n\n\n\n\nrt_trunc ~ 1 + spkr + prec + load + spkr & prec + spkr & load + prec & load + spkr & prec & load + (1 | subj) + (1 + prec | item)\n13\n-28658\n21\n-16\n0.1649\n\n\n\n\n\nThe p-value of approximately 20% leads us to prefer the simpler model, m1, to the more complex, m0.\n\n\n3 Bootstrap basics\nTo bootstrap the model parameters, first initialize a random number generator then create a bootstrap sample and extract the table of parameter estimates from it.\n\nconst RNG = MersenneTwister(42)\nsamp = parametricbootstrap(RNG, 5_000, m1)\ntbl = samp.tbl\n\nTable with 18 columns and 5000 rows:\n      obj      Œ≤1       Œ≤2       Œ≤3        Œ≤4       Œ≤5        Œ≤6        ‚ãØ\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n 1  ‚îÇ 28666.6  2173.04  55.6592  -359.246  87.1657  -35.2652  32.9777   ‚ãØ\n 2  ‚îÇ 28709.1  1973.88  88.8108  -313.754  83.8083  -23.2173  -15.9147  ‚ãØ\n 3  ‚îÇ 28575.8  2167.28  48.2509  -297.174  79.0748  -22.07    41.8946   ‚ãØ\n 4  ‚îÇ 28721.3  2259.36  65.9821  -386.726  82.7247  -35.3908  14.3295   ‚ãØ\n 5  ‚îÇ 28590.9  2182.95  45.0315  -410.477  109.293  -19.132   42.9369   ‚ãØ\n 6  ‚îÇ 28518.9  2167.98  61.0523  -360.559  75.7268  15.1612   61.9125   ‚ãØ\n 7  ‚îÇ 28702.9  2210.25  70.8323  -349.703  99.4985  -32.9348  16.797    ‚ãØ\n 8  ‚îÇ 28726.1  2235.04  59.248   -353.738  58.0711  -35.2804  10.0765   ‚ãØ\n 9  ‚îÇ 28734.7  2042.97  72.1467  -300.828  71.2183  -14.2944  10.7301   ‚ãØ\n 10 ‚îÇ 28609.2  2011.1   76.404   -254.604  63.9238  -37.8758  3.95621   ‚ãØ\n 11 ‚îÇ 28636.7  2166.35  71.1184  -324.401  58.5484  -21.7266  -31.4973  ‚ãØ\n 12 ‚îÇ 28659.2  2158.36  64.3944  -336.451  80.3281  -39.3608  1.96649   ‚ãØ\n 13 ‚îÇ 28637.3  2147.09  70.6536  -399.734  60.0463  -35.6682  28.4599   ‚ãØ\n 14 ‚îÇ 28672.7  2138.23  75.4572  -262.36   96.9987  -32.0501  26.9257   ‚ãØ\n 15 ‚îÇ 28662.5  2239.34  65.1886  -373.371  83.4639  -29.7632  5.43283   ‚ãØ\n 16 ‚îÇ 28604.9  2088.54  62.0142  -268.106  99.684   -27.3686  49.5337   ‚ãØ\n 17 ‚îÇ 28539.6  2313.47  61.1166  -337.952  74.7689  -14.5982  48.7173   ‚ãØ\n ‚ãÆ  ‚îÇ    ‚ãÆ        ‚ãÆ        ‚ãÆ        ‚ãÆ         ‚ãÆ        ‚ãÆ         ‚ãÆ      ‚ã±\n\n\nAn empirical density plot of the estimates of the residual standard deviation is obtained as\n\nplt = data(tbl) * mapping(:œÉ) * AoG.density()\ndraw(plt; axis=(;title=\"Parametric bootstrap estimates of œÉ\"))\n\n\n\n\nA density plot of the estimates of the standard deviation of the random effects is obtained as\n\nplt = data(tbl) * mapping(\n  [:œÉ1, :œÉ2, :œÉ3] .=&gt; \"Bootstrap replicates of standard deviations\";\n  color=dims(1) =&gt; renamer([\"Item intercept\", \"Item speaker\", \"Subj\"])\n) * AoG.density()\ndraw(plt; figure=(;supertitle=\"Parametric bootstrap estimates of variance components\"))\n\n\n\n\nThe bootstrap sample can be used to generate intervals that cover a certain percentage of the bootstrapped values. We refer to these as ‚Äúcoverage intervals‚Äù, similar to a confidence interval. The shortest such intervals, obtained with the confint extractor, correspond to a highest posterior density interval in Bayesian inference.\nWe generate these for all random and fixed effects:\n\nconfint(samp; method=:shortest)\n\nDictTable with 2 columns and 13 rows:\n par   lower      upper\n ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n Œ≤1  ‚îÇ 2033.36    2332.44\n Œ≤2  ‚îÇ 34.9886    98.421\n Œ≤3  ‚îÇ -430.862   -246.17\n Œ≤4  ‚îÇ 48.9002    112.424\n Œ≤5  ‚îÇ -51.4927   12.1684\n Œ≤6  ‚îÇ -13.6705   49.681\n Œ≤7  ‚îÇ -27.2138   34.6244\n Œ≤8  ‚îÇ -6.36897   55.7056\n œÅ1  ‚îÇ -0.899991  -0.460395\n œÉ   ‚îÇ 654.51     700.692\n œÉ1  ‚îÇ 264.116    448.735\n œÉ2  ‚îÇ 174.043    316.132\n œÉ3  ‚îÇ 228.265    359.429\n\n\n\ndraw(\n  data(samp.Œ≤) * mapping(:Œ≤; color=:coefname) * AoG.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\nFor the fixed effects, MixedModelsMakie provides a convenience interface to plot the combined coverage intervals and density plots\n\nridgeplot(samp)\n\n\n\n\nOften the intercept will be on a different scale and potentially less interesting, so we can stop it from being included in the plot:\n\nridgeplot(samp; show_intercept=false)\n\n\n\n\n\n\n4 Singularity\nLet‚Äôs consider the classic dysetuff dataset:\n\ndyestuff = dataset(:dyestuff)\nmdye = fit(MixedModel, @formula(yield ~ 1 + (1 | batch)), dyestuff)\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_batch\n\n\n(Intercept)\n1527.5000\n17.6946\n86.33\n&lt;1e-99\n37.2603\n\n\nResidual\n49.5101\n\n\n\n\n\n\n\n\n\n\nsampdye = parametricbootstrap(MersenneTwister(1234321), 10_000, mdye)\ntbldye = sampdye.tbl\n\nTable with 5 columns and 10000 rows:\n      obj      Œ≤1       œÉ        œÉ1       Œ∏1\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n 1  ‚îÇ 316.738  1528.14  43.5735  22.6691  0.520249\n 2  ‚îÇ 336.101  1552.01  58.1535  39.5166  0.679521\n 3  ‚îÇ 322.046  1501.55  51.857   0.0      0.0\n 4  ‚îÇ 326.893  1525.24  43.6512  66.3744  1.52057\n 5  ‚îÇ 331.544  1522.05  50.7415  51.0074  1.00524\n 6  ‚îÇ 326.892  1550.23  53.5174  19.0928  0.356759\n 7  ‚îÇ 313.786  1504.16  45.1878  0.0      0.0\n 8  ‚îÇ 322.636  1494.05  49.8556  17.7733  0.356494\n 9  ‚îÇ 323.582  1529.74  46.4678  35.2094  0.757716\n 10 ‚îÇ 321.639  1532.6   45.3573  32.4904  0.71632\n 11 ‚îÇ 331.565  1548.25  55.8562  28.7512  0.514736\n 12 ‚îÇ 310.076  1481.8   34.9418  38.4381  1.10006\n 13 ‚îÇ 314.492  1541.95  40.3751  28.3694  0.702646\n 14 ‚îÇ 340.876  1530.48  63.006   42.6409  0.676776\n 15 ‚îÇ 309.778  1496.17  41.5411  8.08345  0.194589\n 16 ‚îÇ 335.375  1530.32  57.4864  38.9066  0.676797\n 17 ‚îÇ 318.347  1511.28  41.0926  39.1128  0.95182\n ‚ãÆ  ‚îÇ    ‚ãÆ        ‚ãÆ        ‚ãÆ        ‚ãÆ        ‚ãÆ\n\n\n\nplt = data(tbldye) * mapping(:œÉ1) * AoG.density()\ndraw(plt; axis=(;title=\"Parametric bootstrap estimates of œÉ_batch\"))\n\n\n\n\nNotice that this density plot has a spike, or mode, at zero. Although this mode appears to be diffuse, this is an artifact of the way that density plots are created. In fact, it is a pulse, as can be seen from a histogram.\n\nplt = data(tbldye) * mapping(:œÉ1) * AoG.histogram(;bins=100)\ndraw(plt; axis=(;title=\"Parametric bootstrap estimates of œÉ_batch\"))\n\n\n\n\nA value of zero for the standard deviation of the random effects is an example of a singular covariance. It is easy to detect the singularity in the case of a scalar random-effects term. However, it is not as straightforward to detect singularity in vector-valued random-effects terms.\nFor example, if we bootstrap a model fit to the sleepstudy data\n\nsleepstudy = dataset(:sleepstudy)\nmsleep = fit(MixedModel, @formula(reaction ~ 1 + days + (1 + days | subj)),\n             sleepstudy)\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\n\n\n(Intercept)\n251.4051\n6.6323\n37.91\n&lt;1e-99\n23.7807\n\n\ndays\n10.4673\n1.5022\n6.97\n&lt;1e-11\n5.7169\n\n\nResidual\n25.5918\n\n\n\n\n\n\n\n\n\n\nsampsleep = parametricbootstrap(MersenneTwister(666), 10_000, msleep)\ntblsleep = sampsleep.tbl\n\nTable with 10 columns and 10000 rows:\n      obj      Œ≤1       Œ≤2       œÉ        œÉ1       œÉ2       œÅ1           ‚ãØ\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n 1  ‚îÇ 1694.07  258.109  10.7928  22.1713  13.6388  5.42876  0.430479     ‚ãØ\n 2  ‚îÇ 1731.59  241.267  9.77438  23.5943  23.8822  7.07983  -0.277566    ‚ãØ\n 3  ‚îÇ 1706.51  254.403  10.6731  23.0742  21.4347  3.78781  0.49333      ‚ãØ\n 4  ‚îÇ 1737.33  257.833  7.61741  24.5027  27.2652  4.8567   -0.0191252   ‚ãØ\n 5  ‚îÇ 1764.5   244.154  13.0045  25.8322  30.9874  7.78249  -0.532281    ‚ãØ\n 6  ‚îÇ 1730.85  247.672  13.2977  24.3583  18.6134  5.8777   -0.054227    ‚ãØ\n 7  ‚îÇ 1709.66  256.847  8.60195  22.4272  26.887   6.62176  -0.638       ‚ãØ\n 8  ‚îÇ 1776.49  252.028  9.56227  27.3708  27.7     6.53336  -0.353793    ‚ãØ\n 9  ‚îÇ 1732.12  248.687  11.5173  23.7153  22.2868  6.69779  0.304228     ‚ãØ\n 10 ‚îÇ 1748.89  249.387  10.2581  25.3961  24.7506  5.4466   0.00469564   ‚ãØ\n 11 ‚îÇ 1735.34  247.16   8.57304  25.5931  19.2144  3.99176  1.0          ‚ãØ\n 12 ‚îÇ 1757.34  248.074  9.7696   25.9832  29.2862  4.74156  0.216257     ‚ãØ\n 13 ‚îÇ 1727.32  248.007  12.8942  23.9315  32.1767  3.50421  0.152108     ‚ãØ\n 14 ‚îÇ 1753.38  252.684  9.88844  25.2124  21.3447  7.64868  -0.00705099  ‚ãØ\n 15 ‚îÇ 1731.05  262.218  7.70813  25.0737  18.1157  4.89355  -0.280423    ‚ãØ\n 16 ‚îÇ 1742.68  252.273  12.7376  24.449   20.6836  7.47368  0.0242447    ‚ãØ\n 17 ‚îÇ 1727.37  243.956  12.107   24.6336  26.5018  3.89659  -0.411449    ‚ãØ\n ‚ãÆ  ‚îÇ    ‚ãÆ        ‚ãÆ        ‚ãÆ        ‚ãÆ        ‚ãÆ        ‚ãÆ          ‚ãÆ       ‚ã±\n\n\nthe singularity can be exhibited as a standard deviation of zero or as a correlation of ¬±1.\n\nconfint(sampsleep)\n\nDictTable with 2 columns and 6 rows:\n par   lower      upper\n ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n Œ≤1  ‚îÇ 238.265    264.28\n Œ≤2  ‚îÇ 7.56523    13.4072\n œÅ1  ‚îÇ -0.409988  1.0\n œÉ   ‚îÇ 22.6674    28.5759\n œÉ1  ‚îÇ 10.5612    33.2579\n œÉ2  ‚îÇ 3.08373    7.73195\n\n\nA histogram of the estimated correlations from the bootstrap sample has a spike at +1.\n\nplt = data(tblsleep) * mapping(:œÅ1) * AoG.histogram(;bins=100)\ndraw(plt; axis=(;title=\"Parametric bootstrap samples of correlation of random effects\"))\n\n\n\n\nor, as a count,\n\ncount(tblsleep.œÅ1  .‚âà 1)\n\n313\n\n\nClose examination of the histogram shows a few values of -1.\n\ncount(tblsleep.œÅ1 .‚âà -1)\n\n0\n\n\nFurthermore there are even a few cases where the estimate of the standard deviation of the random effect for the intercept is zero.\n\ncount(tblsleep.œÉ1 .‚âà 0)\n\n1\n\n\nThere is a general condition to check for singularity of an estimated covariance matrix or matrices in a bootstrap sample. The parameter optimized in the estimation is Œ∏, the relative covariance parameter. Some of the elements of this parameter vector must be non-negative and, when one of these components is approximately zero, one of the covariance matrices will be singular.\nThe issingular method for a MixedModel object that tests if a parameter vector Œ∏ corresponds to a boundary or singular fit.\nThis operation is encapsulated in a method for the issingular function that works on MixedModelBootstrap objects.\n\ncount(issingular(sampsleep))\n\n314\n\n\n\n\n5 References\n\n\nKronm√ºller, E., & Barr, D. J. (2007). Perspective-free pragmatics: Broken precedents and the recovery-from-preemption hypothesis. Journal of Memory and Language, 56(3), 436‚Äì455. https://doi.org/10.1016/j.jml.2006.05.002\n\n\nThis page was rendered from git revision fb99b3c\n using Quarto 1.7.33.\n\n\n\n\n Back to top",
    "crumbs": [
      "Bootstrap and profiling",
      "Parametric bootstrap for mixed-effects models"
    ]
  },
  {
    "objectID": "contrasts_fggk21.html",
    "href": "contrasts_fggk21.html",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "",
    "text": "This script uses a subset of data reported in F√ºhner et al. (2021).\nTo circumvent delays associated with model fitting we work with models that are less complex than those in the reference publication. All the data to reproduce the models in the publication are used here, too; the script requires only a few changes to specify the more complex models in the paper.\nAll children were between 6.0 and 6.99 years at legal keydate (30 September) of school enrollment, that is in their ninth year of life in the third grade. To avoid delays associated with model fitting we work with a reduced data set and less complex models than those in the reference publication. The script requires only a few changes to specify the more complex models in the paper.\nThe script is structured in three main sections:",
    "crumbs": [
      "Contrast coding and transformations",
      "Mixed Models Tutorial: Contrast Coding"
    ]
  },
  {
    "objectID": "contrasts_fggk21.html#packages-and-functions",
    "href": "contrasts_fggk21.html#packages-and-functions",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "1.1 Packages and functions",
    "text": "1.1 Packages and functions\n\n\nCode\nusing AlgebraOfGraphics\nusing CairoMakie\nusing Chain\nusing CategoricalArrays\nusing DataFrames\nusing DataFrameMacros\nusing MixedModels\nusing ProgressMeter\nusing Statistics\nusing StatsBase\n\nusing MixedModels: likelihoodratiotest\nusing SMLP2026: dataset\n\nprogress = isinteractive()",
    "crumbs": [
      "Contrast coding and transformations",
      "Mixed Models Tutorial: Contrast Coding"
    ]
  },
  {
    "objectID": "contrasts_fggk21.html#readme-for-datasetfggk21",
    "href": "contrasts_fggk21.html#readme-for-datasetfggk21",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "1.2 Readme for dataset(\"fggk21\")",
    "text": "1.2 Readme for dataset(\"fggk21\")\nNumber of scores: 525126\n\nCohort: 9 levels; 2011-2019\nSchool: 515 levels\nChild: 108295 levels; all children are between 8.0 and 8.99 years old\nSex: ‚ÄúGirls‚Äù (n=55,086), ‚ÄúBoys‚Äù (n= 53,209)\nage: testdate - middle of month of birthdate\nTest: 5 levels\n\nEndurance (Run): 6 minute endurance run [m]; to nearest 9m in 9x18m field\nCoordination (Star_r): star coordination run [m/s]; 9x9m field, 4 x diagonal = 50.912 m\nSpeed(S20_r): 20-meters sprint [m/s]\nMuscle power low (SLJ): standing long jump [cm]\nMuscle power up (BPT): 1-kg medicine ball push test [m]\n\nscore - see units",
    "crumbs": [
      "Contrast coding and transformations",
      "Mixed Models Tutorial: Contrast Coding"
    ]
  },
  {
    "objectID": "contrasts_fggk21.html#preprocessing",
    "href": "contrasts_fggk21.html#preprocessing",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "1.3 Preprocessing",
    "text": "1.3 Preprocessing\n\n1.3.1 Read data\n\ntbl = dataset(:fggk21)\n\nArrow.Table with 525126 rows, 7 columns, and schema:\n :Cohort  String\n :School  String\n :Child   String\n :Sex     String\n :age     Float64\n :Test    String\n :score   Float64\n\n\n\ndf = DataFrame(tbl)\ndescribe(df)\n\n7√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nDataType\n\n\n\n\n1\nCohort\n\n2011\n\n2019\n0\nString\n\n\n2\nSchool\n\nS100043\n\nS800200\n0\nString\n\n\n3\nChild\n\nC002352\n\nC117966\n0\nString\n\n\n4\nSex\n\nfemale\n\nmale\n0\nString\n\n\n5\nage\n8.56073\n7.99452\n8.55852\n9.10609\n0\nFloat64\n\n\n6\nTest\n\nBPT\n\nStar_r\n0\nString\n\n\n7\nscore\n226.141\n1.14152\n4.65116\n1530.0\n0\nFloat64\n\n\n\n\n\n\n\n\n1.3.2 Extract a stratified subsample\nWe extract a random sample of 500 children from the Sex (2) x Test (5) cells of the design. Cohort and School are random.\n\ndat = @chain df begin\n  @transform(:Sex = :Sex == \"female\" ? \"Girls\" : \"Boys\")\n  @groupby(:Test, :Sex)\n  combine(x -&gt; x[sample(1:nrow(x), 500), :])\nend\n\n5000√ó7 DataFrame4975 rows omitted\n\n\n\nRow\nTest\nSex\nCohort\nSchool\nChild\nage\nscore\n\n\n\nString\nString\nString\nString\nString\nFloat64\nFloat64\n\n\n\n\n1\nS20_r\nBoys\n2017\nS106082\nC058365\n8.56126\n4.65116\n\n\n2\nS20_r\nBoys\n2011\nS101485\nC041707\n8.41889\n4.65116\n\n\n3\nS20_r\nBoys\n2019\nS103305\nC057023\n8.55578\n4.65116\n\n\n4\nS20_r\nBoys\n2015\nS101758\nC083951\n8.79671\n3.92157\n\n\n5\nS20_r\nBoys\n2018\nS106781\nC091332\n8.85421\n4.87805\n\n\n6\nS20_r\nBoys\n2019\nS102350\nC028317\n8.3039\n4.65116\n\n\n7\nS20_r\nBoys\n2015\nS103639\nC018651\n8.21629\n4.44444\n\n\n8\nS20_r\nBoys\n2018\nS110346\nC101972\n8.93908\n4.44444\n\n\n9\nS20_r\nBoys\n2018\nS101710\nC043098\n8.44079\n4.25532\n\n\n10\nS20_r\nBoys\n2012\nS110346\nC022817\n8.24914\n4.44444\n\n\n11\nS20_r\nBoys\n2018\nS105170\nC043726\n8.44079\n4.54545\n\n\n12\nS20_r\nBoys\n2018\nS101047\nC100959\n8.93908\n5.12821\n\n\n13\nS20_r\nBoys\n2013\nS104206\nC013046\n8.16427\n4.7619\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n4989\nRun\nGirls\n2018\nS104784\nC101583\n8.93908\n918.0\n\n\n4990\nRun\nGirls\n2011\nS111430\nC089421\n8.83231\n1134.0\n\n\n4991\nRun\nGirls\n2016\nS104954\nC044379\n8.44627\n1023.0\n\n\n4992\nRun\nGirls\n2018\nS100341\nC033677\n8.35866\n990.0\n\n\n4993\nRun\nGirls\n2019\nS104280\nC085142\n8.80219\n864.0\n\n\n4994\nRun\nGirls\n2011\nS104711\nC008355\n8.08487\n1130.0\n\n\n4995\nRun\nGirls\n2019\nS110346\nC038261\n8.38877\n855.0\n\n\n4996\nRun\nGirls\n2012\nS106677\nC032124\n8.33402\n1140.0\n\n\n4997\nRun\nGirls\n2019\nS103330\nC005643\n8.05202\n810.0\n\n\n4998\nRun\nGirls\n2016\nS111168\nC094413\n8.87885\n1200.0\n\n\n4999\nRun\nGirls\n2016\nS100572\nC083185\n8.79398\n918.0\n\n\n5000\nRun\nGirls\n2016\nS105764\nC063541\n8.61602\n900.0\n\n\n\n\n\n\n\n\n1.3.3 Transformations\n\ntransform!(dat, :age, :age =&gt; (x -&gt; x .- 8.5) =&gt; :a1) # centered age (linear)\nselect!(groupby(dat, :Test), :, :score =&gt; zscore =&gt; :zScore) # z-score\n\n5000√ó9 DataFrame4975 rows omitted\n\n\n\nRow\nTest\nSex\nCohort\nSchool\nChild\nage\nscore\na1\nzScore\n\n\n\nString\nString\nString\nString\nString\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nS20_r\nBoys\n2017\nS106082\nC058365\n8.56126\n4.65116\n0.0612594\n0.330263\n\n\n2\nS20_r\nBoys\n2011\nS101485\nC041707\n8.41889\n4.65116\n-0.0811088\n0.330263\n\n\n3\nS20_r\nBoys\n2019\nS103305\nC057023\n8.55578\n4.65116\n0.0557837\n0.330263\n\n\n4\nS20_r\nBoys\n2015\nS101758\nC083951\n8.79671\n3.92157\n0.296715\n-1.44817\n\n\n5\nS20_r\nBoys\n2018\nS106781\nC091332\n8.85421\n4.87805\n0.354209\n0.883313\n\n\n6\nS20_r\nBoys\n2019\nS102350\nC028317\n8.3039\n4.65116\n-0.196099\n0.330263\n\n\n7\nS20_r\nBoys\n2015\nS103639\nC018651\n8.21629\n4.44444\n-0.28371\n-0.173627\n\n\n8\nS20_r\nBoys\n2018\nS110346\nC101972\n8.93908\n4.44444\n0.439083\n-0.173627\n\n\n9\nS20_r\nBoys\n2018\nS101710\nC043098\n8.44079\n4.25532\n-0.059206\n-0.634632\n\n\n10\nS20_r\nBoys\n2012\nS110346\nC022817\n8.24914\n4.44444\n-0.250856\n-0.173627\n\n\n11\nS20_r\nBoys\n2018\nS105170\nC043726\n8.44079\n4.54545\n-0.059206\n0.0725921\n\n\n12\nS20_r\nBoys\n2018\nS101047\nC100959\n8.93908\n5.12821\n0.439083\n1.49309\n\n\n13\nS20_r\nBoys\n2013\nS104206\nC013046\n8.16427\n4.7619\n-0.335729\n0.600204\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n4989\nRun\nGirls\n2018\nS104784\nC101583\n8.93908\n918.0\n0.439083\n-0.556439\n\n\n4990\nRun\nGirls\n2011\nS111430\nC089421\n8.83231\n1134.0\n0.332307\n0.843864\n\n\n4991\nRun\nGirls\n2016\nS104954\nC044379\n8.44627\n1023.0\n-0.0537303\n0.124264\n\n\n4992\nRun\nGirls\n2018\nS100341\nC033677\n8.35866\n990.0\n-0.141342\n-0.0896712\n\n\n4993\nRun\nGirls\n2019\nS104280\nC085142\n8.80219\n864.0\n0.30219\n-0.906514\n\n\n4994\nRun\nGirls\n2011\nS104711\nC008355\n8.08487\n1130.0\n-0.415127\n0.817932\n\n\n4995\nRun\nGirls\n2019\nS110346\nC038261\n8.38877\n855.0\n-0.111225\n-0.96486\n\n\n4996\nRun\nGirls\n2012\nS106677\nC032124\n8.33402\n1140.0\n-0.165982\n0.882761\n\n\n4997\nRun\nGirls\n2019\nS103330\nC005643\n8.05202\n810.0\n-0.447981\n-1.25659\n\n\n4998\nRun\nGirls\n2016\nS111168\nC094413\n8.87885\n1200.0\n0.37885\n1.27173\n\n\n4999\nRun\nGirls\n2016\nS100572\nC083185\n8.79398\n918.0\n0.293977\n-0.556439\n\n\n5000\nRun\nGirls\n2016\nS105764\nC063541\n8.61602\n900.0\n0.116016\n-0.67313\n\n\n\n\n\n\n\ndat2 = combine(\n  groupby(dat, [:Test, :Sex]),\n  :score =&gt; mean,\n  :score =&gt; std,\n  :zScore =&gt; mean,\n  :zScore =&gt; std,\n)\n\n10√ó6 DataFrame\n\n\n\nRow\nTest\nSex\nscore_mean\nscore_std\nzScore_mean\nzScore_std\n\n\n\nString\nString\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nS20_r\nBoys\n4.59711\n0.417368\n0.198513\n1.01736\n\n\n2\nBPT\nBoys\n3.9978\n0.703395\n0.319386\n0.988361\n\n\n3\nSLJ\nBoys\n129.812\n19.4448\n0.224684\n0.9993\n\n\n4\nStar_r\nBoys\n2.05916\n0.298452\n0.131318\n1.05729\n\n\n5\nRun\nBoys\n1044.56\n161.515\n0.264048\n1.04708\n\n\n6\nS20_r\nGirls\n4.43423\n0.386591\n-0.198513\n0.94234\n\n\n7\nBPT\nGirls\n3.5432\n0.644736\n-0.319386\n0.905938\n\n\n8\nSLJ\nGirls\n121.068\n18.4828\n-0.224684\n0.949861\n\n\n9\nStar_r\nGirls\n1.98503\n0.260185\n-0.131318\n0.921729\n\n\n10\nRun\nGirls\n963.102\n134.996\n-0.264048\n0.87516\n\n\n\n\n\n\n\n\n1.3.4 Figure of age x Sex x Test interactions\nThe main results of relevance here are shown in Figure 2 of Scientific Reports 11:17566.",
    "crumbs": [
      "Contrast coding and transformations",
      "Mixed Models Tutorial: Contrast Coding"
    ]
  },
  {
    "objectID": "contrasts_fggk21.html#seqdiffcoding-contr1",
    "href": "contrasts_fggk21.html#seqdiffcoding-contr1",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "2.1 SeqDiffCoding: contr1",
    "text": "2.1 SeqDiffCoding: contr1\nSeqDiffCoding was used in the publication. This specification tests pairwise differences between the five neighboring levels of Test, that is:\n\nSDC1: 2-1\nSDC2: 3-2\nSDC3: 4-3\nSDC4: 5-4\n\nThe levels were sorted such that these contrasts map onto four a priori hypotheses; in other words, they are theoretically motivated pairwise comparisons. The motivation also encompasses theoretically motivated interactions with Sex. The order of levels can also be explicitly specified during contrast construction. This is very useful if levels are in a different order in the dataframe. We recommend the explicit specification to increase transparency of the code.\nThe statistical disadvantage of SeqDiffCoding is that the contrasts are not orthogonal, that is the contrasts are correlated. This is obvious from the fact that levels 2, 3, and 4 are all used in two contrasts. One consequence of this is that correlation parameters estimated between neighboring contrasts (e.g., 2-1 and 3-2) are difficult to interpret. Usually, they will be negative because assuming some practical limitation on the overall range (e.g., between levels 1 and 3), a small ‚Äú2-1‚Äù effect ‚Äúcorrelates‚Äù negatively with a larger ‚Äú3-2‚Äù effect for mathematical reasons.\nObviously, the tradeoff between theoretical motivation and statistical purity is something that must be considered carefully when planning the analysis.\n\ncontr1 = Dict(\n    :Sex =&gt; EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n    :Test =&gt; SeqDiffCoding(;\n      levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"]\n    ),\n  )\n\nDict{Symbol, StatsModels.AbstractContrasts} with 2 entries:\n  :Test =&gt; SeqDiffCoding([\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"])\n  :Sex  =&gt; EffectsCoding(nothing, [\"Girls\", \"Boys\"])\n\n\n\nm_ovi_SeqDiff_1 = lmm(@formula(zScore ~ 1 + Test + (1 | Child)),\n                      dat; contrasts=contr1, progress)\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n(Intercept)\n-0.0006\n0.0143\n-0.04\n0.9641\n0.7660\n\n\nTest: Star_r\n0.0046\n0.0443\n0.10\n0.9173\n\n\n\nTest: S20_r\n0.0108\n0.0443\n0.24\n0.8076\n\n\n\nTest: SLJ\n-0.0127\n0.0441\n-0.29\n0.7733\n\n\n\nTest: BPT\n-0.0028\n0.0442\n-0.06\n0.9492\n\n\n\nResidual\n0.6438\n\n\n\n\n\n\n\n\n\nIn this case, any differences between tests identified by the contrasts would be spurious because each test was standardized (i.e., M=0, \\(SD\\)=1). The differences could also be due to an imbalance in the number of boys and girls or in the number of missing observations for each test.\nThe primary interest in this study related to interactions of the test contrasts with and age and Sex. We start with age (linear) and its interaction with the four test contrasts.\n\nm_ovi_SeqDiff_2 = lmm(@formula(zScore ~ 1 + Test * a1 + (1 | Child)),\n                      dat; contrasts=contr1, progress)\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n(Intercept)\n-0.0204\n0.0146\n-1.40\n0.1621\n0.7581\n\n\nTest: Star_r\n-0.0017\n0.0452\n-0.04\n0.9695\n\n\n\nTest: S20_r\n0.0118\n0.0451\n0.26\n0.7941\n\n\n\nTest: SLJ\n-0.0075\n0.0450\n-0.17\n0.8678\n\n\n\nTest: BPT\n-0.0154\n0.0452\n-0.34\n0.7327\n\n\n\na1\n0.3024\n0.0487\n6.21\n&lt;1e-09\n\n\n\nTest: Star_r & a1\n0.1028\n0.1487\n0.69\n0.4892\n\n\n\nTest: S20_r & a1\n0.0273\n0.1525\n0.18\n0.8581\n\n\n\nTest: SLJ & a1\n-0.1173\n0.1520\n-0.77\n0.4402\n\n\n\nTest: BPT & a1\n0.1914\n0.1512\n1.27\n0.2056\n\n\n\nResidual\n0.6466\n\n\n\n\n\n\n\n\n\nThe difference between older and younger children is larger for Star_r than for Run (0.2473). S20_r did not differ significantly from Star_r (-0.0377) and SLJ (-0.0113) The largest difference in developmental gain was between BPT and SLJ (0.3355).\nPlease note that standard errors of this LMM are anti-conservative because the LMM is missing a lot of information in the RES (e..g., contrast-related VCs snd CPs for Child, School, and Cohort.\nNext we add the main effect of Sex and its interaction with the four test contrasts.\n\nm_ovi_SeqDiff_3 = lmm(@formula(zScore ~ 1 + Test * (a1 + Sex) + (1 | Child)),\n                      dat; contrasts=contr1, progress)\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n(Intercept)\n-0.0179\n0.0142\n-1.26\n0.2071\n0.7147\n\n\nTest: Star_r\n-0.0028\n0.0441\n-0.06\n0.9500\n\n\n\nTest: S20_r\n0.0103\n0.0440\n0.23\n0.8143\n\n\n\nTest: SLJ\n-0.0063\n0.0438\n-0.14\n0.8865\n\n\n\nTest: BPT\n-0.0150\n0.0440\n-0.34\n0.7328\n\n\n\na1\n0.2658\n0.0474\n5.61\n&lt;1e-07\n\n\n\nSex: Boys\n0.2231\n0.0139\n16.10\n&lt;1e-57\n\n\n\nTest: Star_r & a1\n0.1233\n0.1451\n0.85\n0.3953\n\n\n\nTest: S20_r & a1\n0.0110\n0.1488\n0.07\n0.9409\n\n\n\nTest: SLJ & a1\n-0.1118\n0.1483\n-0.75\n0.4506\n\n\n\nTest: BPT & a1\n0.1864\n0.1475\n1.26\n0.2063\n\n\n\nTest: Star_r & Sex: Boys\n-0.1332\n0.0431\n-3.09\n0.0020\n\n\n\nTest: S20_r & Sex: Boys\n0.0691\n0.0431\n1.61\n0.1084\n\n\n\nTest: SLJ & Sex: Boys\n0.0321\n0.0428\n0.75\n0.4542\n\n\n\nTest: BPT & Sex: Boys\n0.0809\n0.0429\n1.88\n0.0594\n\n\n\nResidual\n0.6542\n\n\n\n\n\n\n\n\n\nThe significant interactions with Sex reflect mostly differences related to muscle power, where the physiological constitution gives boys an advantage. The sex difference is smaller when coordination and cognition play a role ‚Äì as in the Star_r test. (Caveat: SEs are estimated with an underspecified RES.)\nThe final step in this first series is to add the interactions between the three covariates. A significant interaction between any of the four Test contrasts and age (linear) x Sex was hypothesized to reflect a prepubertal signal (i.e., hormones start to rise in girls‚Äô ninth year of life). However, this hypothesis is linked to a specific shape of the interaction: Girls would need to gain more than boys in tests of muscular power.\n\n# we'll be re-using this formula with other contrast specifications,\n# so we assign it to its own variable\nf_ovi = @formula(zScore ~ 1 + Test * a1 * Sex + (1 | Child))\nm_ovi_SeqDiff = lmm(f_ovi, dat; contrasts=contr1, progress)\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n(Intercept)\n-0.0189\n0.0142\n-1.33\n0.1830\n0.7098\n\n\nTest: Star_r\n-0.0003\n0.0441\n-0.01\n0.9938\n\n\n\nTest: S20_r\n0.0082\n0.0440\n0.19\n0.8516\n\n\n\nTest: SLJ\n-0.0052\n0.0438\n-0.12\n0.9058\n\n\n\nTest: BPT\n-0.0154\n0.0441\n-0.35\n0.7266\n\n\n\na1\n0.2652\n0.0474\n5.60\n&lt;1e-07\n\n\n\nSex: Boys\n0.2185\n0.0142\n15.39\n&lt;1e-52\n\n\n\nTest: Star_r & a1\n0.1224\n0.1451\n0.84\n0.3990\n\n\n\nTest: S20_r & a1\n0.0049\n0.1489\n0.03\n0.9740\n\n\n\nTest: SLJ & a1\n-0.1032\n0.1484\n-0.70\n0.4870\n\n\n\nTest: BPT & a1\n0.1882\n0.1476\n1.28\n0.2023\n\n\n\nTest: Star_r & Sex: Boys\n-0.1227\n0.0441\n-2.78\n0.0054\n\n\n\nTest: S20_r & Sex: Boys\n0.0598\n0.0440\n1.36\n0.1745\n\n\n\nTest: SLJ & Sex: Boys\n0.0380\n0.0438\n0.87\n0.3864\n\n\n\nTest: BPT & Sex: Boys\n0.0792\n0.0441\n1.80\n0.0723\n\n\n\na1 & Sex: Boys\n0.0714\n0.0474\n1.51\n0.1317\n\n\n\nTest: Star_r & a1 & Sex: Boys\n-0.1528\n0.1451\n-1.05\n0.2924\n\n\n\nTest: S20_r & a1 & Sex: Boys\n0.1616\n0.1489\n1.08\n0.2779\n\n\n\nTest: SLJ & a1 & Sex: Boys\n-0.1063\n0.1484\n-0.72\n0.4737\n\n\n\nTest: BPT & a1 & Sex: Boys\n0.0267\n0.1476\n0.18\n0.8564\n\n\n\nResidual\n0.6587\n\n\n\n\n\n\n\n\n\nThe results are very clear: Despite an abundance of statistical power there is no evidence for the differences between boys and girls in how much they gain in the ninth year of life in these five tests. The authors argue that, in this case, absence of evidence looks very much like evidence of absence of a hypothesized interaction.\nIn the next two sections we use different contrasts. Does this have a bearing on this result? We still ignore for now that we are looking at anti-conservative test statistics.",
    "crumbs": [
      "Contrast coding and transformations",
      "Mixed Models Tutorial: Contrast Coding"
    ]
  },
  {
    "objectID": "contrasts_fggk21.html#helmertcoding-contr2",
    "href": "contrasts_fggk21.html#helmertcoding-contr2",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "2.2 HelmertCoding: contr2",
    "text": "2.2 HelmertCoding: contr2\nThe second set of contrasts uses HelmertCoding. Helmert coding codes each level as the difference from the average of the lower levels. With the default order of Test levels we get the following test statistics which we describe in reverse order of appearance in model output\n\nHeC4: 5 - mean(1,2,3,4)\nHeC3: 4 - mean(1,2,3)\nHeC2: 3 - mean(1,2)\nHeC1: 2 - 1\n\nIn the model output, HeC1 will be reported first and HeC4 last.\nThere is some justification for the HeC4 specification in a post-hoc manner because the fifth test (BPT) turned out to be different from the other four tests in that high performance is most likely not only related to physical fitness, but also to overweight/obesity, that is for a subset of children high scores on this test might be indicative of physical unfitness. A priori the SDC4 contrast 5-4 between BPT (5) and SLJ (4) was motivated because conceptually both are tests of the physical fitness component Muscular Power, BPT for upper limbs and SLJ for lower limbs, respectively.\nOne could argue that there is justification for HeC3 because Run (1), Star_r (2), and S20 (3) involve running but SLJ (4) does not. Sports scientists, however, recoil. For them it does not make much sense to average the different running tests, because they draw on completely different physiological resources; it is a variant of the old apples-and-oranges problem.\nThe justification for HeC3 is thatRun (1) and Star_r (2) draw more strongly on cardiorespiratory Endurance than S20 (3) due to the longer duration of the runs compared to sprinting for 20 m which is a pure measure of the physical-fitness component Speed. Again, sports scientists are not very happy with this proposal.\nFinally, HeC1 contrasts the fitness components Endurance, indicated best by Run (1), and Coordination, indicated by Star_r (2). Endurance (i.e., running for 6 minutes) is considered to be the best indicator of health-related status among the five tests because it is a rather pure measure of cardiorespiratory fitness. The Star_r test requires execution of a pre-instructed sequence of forward, sideways, and backward runs. This coordination of body movements implies a demand on working memory (i.e., remembering the order of these subruns) and executive control processes, but performance also depends on endurance. HeC1 yields a measure of Coordination ‚Äúcorrected‚Äù for the contribution of Endurance.\nThe statistical advantage of HelmertCoding is that the resulting contrasts are orthogonal (uncorrelated). This allows for optimal partitioning of variance and statistical power. It is also more efficient to estimate ‚Äúorthogonal‚Äù than ‚Äúnon-orthogonal‚Äù random-effect structures.\n\ncontr2 = Dict(\n  :Sex =&gt; EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n  :Test =&gt; HelmertCoding(;\n    levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n  ),\n);\n\n\nm_ovi_Helmert = lmm(f_ovi, dat; contrasts=contr2, progress)\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n(Intercept)\n-0.0189\n0.0142\n-1.33\n0.1830\n0.7098\n\n\nTest: Star_r\n-0.0002\n0.0221\n-0.01\n0.9938\n\n\n\nTest: S20_r\n0.0027\n0.0127\n0.21\n0.8323\n\n\n\nTest: SLJ\n0.0000\n0.0090\n0.01\n0.9957\n\n\n\nTest: BPT\n-0.0031\n0.0070\n-0.44\n0.6613\n\n\n\na1\n0.2652\n0.0474\n5.60\n&lt;1e-07\n\n\n\nSex: Boys\n0.2185\n0.0142\n15.39\n&lt;1e-52\n\n\n\nTest: Star_r & a1\n0.0612\n0.0726\n0.84\n0.3990\n\n\n\nTest: S20_r & a1\n0.0220\n0.0430\n0.51\n0.6084\n\n\n\nTest: SLJ & a1\n-0.0148\n0.0300\n-0.49\n0.6217\n\n\n\nTest: BPT & a1\n0.0288\n0.0233\n1.23\n0.2177\n\n\n\nTest: Star_r & Sex: Boys\n-0.0614\n0.0221\n-2.78\n0.0054\n\n\n\nTest: S20_r & Sex: Boys\n-0.0005\n0.0127\n-0.04\n0.9670\n\n\n\nTest: SLJ & Sex: Boys\n0.0092\n0.0090\n1.03\n0.3028\n\n\n\nTest: BPT & Sex: Boys\n0.0214\n0.0070\n3.07\n0.0022\n\n\n\na1 & Sex: Boys\n0.0714\n0.0474\n1.51\n0.1317\n\n\n\nTest: Star_r & a1 & Sex: Boys\n-0.0764\n0.0726\n-1.05\n0.2924\n\n\n\nTest: S20_r & a1 & Sex: Boys\n0.0284\n0.0430\n0.66\n0.5088\n\n\n\nTest: SLJ & a1 & Sex: Boys\n-0.0124\n0.0300\n-0.41\n0.6792\n\n\n\nTest: BPT & a1 & Sex: Boys\n-0.0021\n0.0233\n-0.09\n0.9286\n\n\n\nResidual\n0.6587\n\n\n\n\n\n\n\n\n\nWe forego a detailed discussion of the effects, but note that again none of the interactions between age x Sex with the four test contrasts was significant.\nThe default labeling of Helmert contrasts may lead to confusions with other contrasts. Therefore, we could provide our own labels:\nlabels=[\"c2.1\", \"c3.12\", \"c4.123\", \"c5.1234\"]\nOnce the order of levels is memorized the proposed labelling is very transparent.",
    "crumbs": [
      "Contrast coding and transformations",
      "Mixed Models Tutorial: Contrast Coding"
    ]
  },
  {
    "objectID": "contrasts_fggk21.html#hypothesiscoding-contr3",
    "href": "contrasts_fggk21.html#hypothesiscoding-contr3",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "2.3 HypothesisCoding: contr3",
    "text": "2.3 HypothesisCoding: contr3\nThe third set of contrasts uses HypothesisCoding. Hypothesis coding allows the user to specify their own a priori contrast matrix, subject to the mathematical constraint that the matrix has full rank. For example, sport scientists agree that the first four tests can be contrasted with BPT, because the difference is akin to a correction of overall physical fitness. However, they want to keep the pairwise comparisons for the first four tests.\n\nHyC1: BPT - mean(1,2,3,4)\nHyC2: Star_r - Run_r\nHyC3: Run_r - S20_r\nHyC4: S20_r - SLJ\n\n\ncontr3 = Dict(\n  :Sex =&gt; EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n  :Test =&gt; HypothesisCoding(\n    [\n      -1 -1 -1 -1 +4\n      -1 +1  0  0  0\n       0 -1 +1  0  0\n       0  0 -1 +1  0\n    ];\n    levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n    labels=[\"BPT-other\", \"Star-End\", \"S20-Star\", \"SLJ-S20\"],\n  ),\n);\n\n\nm_ovi_Hypo = lmm(f_ovi, dat; contrasts=contr3, progress)\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n(Intercept)\n-0.0189\n0.0142\n-1.33\n0.1830\n0.7098\n\n\nTest: BPT-other\n-0.0611\n0.1394\n-0.44\n0.6613\n\n\n\nTest: Star-End\n-0.0003\n0.0441\n-0.01\n0.9938\n\n\n\nTest: S20-Star\n0.0082\n0.0440\n0.19\n0.8516\n\n\n\nTest: SLJ-S20\n-0.0052\n0.0438\n-0.12\n0.9058\n\n\n\na1\n0.2652\n0.0474\n5.60\n&lt;1e-07\n\n\n\nSex: Boys\n0.2185\n0.0142\n15.39\n&lt;1e-52\n\n\n\nTest: BPT-other & a1\n0.5754\n0.4668\n1.23\n0.2177\n\n\n\nTest: Star-End & a1\n0.1224\n0.1451\n0.84\n0.3990\n\n\n\nTest: S20-Star & a1\n0.0049\n0.1489\n0.03\n0.9740\n\n\n\nTest: SLJ-S20 & a1\n-0.1032\n0.1484\n-0.70\n0.4870\n\n\n\nTest: BPT-other & Sex: Boys\n0.4277\n0.1394\n3.07\n0.0022\n\n\n\nTest: Star-End & Sex: Boys\n-0.1227\n0.0441\n-2.78\n0.0054\n\n\n\nTest: S20-Star & Sex: Boys\n0.0598\n0.0440\n1.36\n0.1745\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0380\n0.0438\n0.87\n0.3864\n\n\n\na1 & Sex: Boys\n0.0714\n0.0474\n1.51\n0.1317\n\n\n\nTest: BPT-other & a1 & Sex: Boys\n-0.0418\n0.4668\n-0.09\n0.9286\n\n\n\nTest: Star-End & a1 & Sex: Boys\n-0.1528\n0.1451\n-1.05\n0.2924\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.1616\n0.1489\n1.08\n0.2779\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.1063\n0.1484\n-0.72\n0.4737\n\n\n\nResidual\n0.6587\n\n\n\n\n\n\n\n\n\nWith HypothesisCoding we must generate our own labels for the contrasts. The default labeling of contrasts is usually not interpretable. Therefore, we provide our own.\nAnyway, none of the interactions between age x Sex with the four Test contrasts was significant for these contrasts.\n\ncontr1b = Dict(\n  :Sex =&gt; EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n  :Test =&gt; HypothesisCoding(\n    [\n      -1 +1  0  0  0\n       0 -1 +1  0  0\n       0  0 -1 +1  0\n       0  0  0 -1 +1\n    ];\n    levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n    labels=[\"Star-Run\", \"S20-Star\", \"SLJ-S20\", \"BPT-SLJ\"],\n  ),\n);\n\n\nm_ovi_SeqDiff_v2 = lmm(f_ovi, dat; contrasts=contr1b, progress)\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n(Intercept)\n-0.0189\n0.0142\n-1.33\n0.1830\n0.7098\n\n\nTest: Star-Run\n-0.0003\n0.0441\n-0.01\n0.9938\n\n\n\nTest: S20-Star\n0.0082\n0.0440\n0.19\n0.8516\n\n\n\nTest: SLJ-S20\n-0.0052\n0.0438\n-0.12\n0.9058\n\n\n\nTest: BPT-SLJ\n-0.0154\n0.0441\n-0.35\n0.7266\n\n\n\na1\n0.2652\n0.0474\n5.60\n&lt;1e-07\n\n\n\nSex: Boys\n0.2185\n0.0142\n15.39\n&lt;1e-52\n\n\n\nTest: Star-Run & a1\n0.1224\n0.1451\n0.84\n0.3990\n\n\n\nTest: S20-Star & a1\n0.0049\n0.1489\n0.03\n0.9740\n\n\n\nTest: SLJ-S20 & a1\n-0.1032\n0.1484\n-0.70\n0.4870\n\n\n\nTest: BPT-SLJ & a1\n0.1882\n0.1476\n1.28\n0.2023\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1227\n0.0441\n-2.78\n0.0054\n\n\n\nTest: S20-Star & Sex: Boys\n0.0598\n0.0440\n1.36\n0.1745\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0380\n0.0438\n0.87\n0.3864\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.0792\n0.0441\n1.80\n0.0723\n\n\n\na1 & Sex: Boys\n0.0714\n0.0474\n1.51\n0.1317\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n-0.1528\n0.1451\n-1.05\n0.2924\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.1616\n0.1489\n1.08\n0.2779\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.1063\n0.1484\n-0.72\n0.4737\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n0.0267\n0.1476\n0.18\n0.8564\n\n\n\nResidual\n0.6587\n\n\n\n\n\n\n\n\n\n\nm_zcp_SeqD = lmm(@formula(zScore ~ 1 + Test * a1 * Sex + zerocorr(1 + Test | Child)),\n                 dat; contrasts=contr1b, progress)\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n(Intercept)\n-0.0189\n0.0144\n-1.31\n0.1887\n0.8012\n\n\nTest: Star-Run\n-0.0404\n0.0433\n-0.93\n0.3510\n0.3754\n\n\nTest: S20-Star\n0.0341\n0.0421\n0.81\n0.4178\n0.7695\n\n\nTest: SLJ-S20\n-0.0253\n0.0415\n-0.61\n0.5429\n0.6663\n\n\nTest: BPT-SLJ\n0.0069\n0.0446\n0.16\n0.8763\n0.5233\n\n\na1\n0.2667\n0.0479\n5.57\n&lt;1e-07\n\n\n\nSex: Boys\n0.2183\n0.0144\n15.21\n&lt;1e-51\n\n\n\nTest: Star-Run & a1\n0.2908\n0.1414\n2.06\n0.0397\n\n\n\nTest: S20-Star & a1\n-0.0721\n0.1422\n-0.51\n0.6122\n\n\n\nTest: SLJ-S20 & a1\n-0.0768\n0.1406\n-0.55\n0.5850\n\n\n\nTest: BPT-SLJ & a1\n0.1405\n0.1496\n0.94\n0.3474\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1449\n0.0433\n-3.34\n0.0008\n\n\n\nTest: S20-Star & Sex: Boys\n0.0720\n0.0421\n1.71\n0.0875\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0374\n0.0415\n0.90\n0.3677\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.0759\n0.0446\n1.70\n0.0886\n\n\n\na1 & Sex: Boys\n0.0712\n0.0479\n1.49\n0.1372\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n-0.0404\n0.1414\n-0.29\n0.7749\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.0797\n0.1422\n0.56\n0.5753\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.0914\n0.1406\n-0.65\n0.5153\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n0.0453\n0.1496\n0.30\n0.7622\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nm_zcp_SeqD_2 = lmm(@formula(zScore ~ 1 + Test * a1 * Sex + (0 + Test | Child)),\n                   dat; contrasts=contr1b, progress)\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n(Intercept)\n-0.0190\n0.0141\n-1.34\n0.1793\n\n\n\nTest: Star-Run\n-0.0073\n0.0445\n-0.16\n0.8695\n\n\n\nTest: S20-Star\n0.0136\n0.0448\n0.30\n0.7615\n\n\n\nTest: SLJ-S20\n-0.0116\n0.0439\n-0.26\n0.7917\n\n\n\nTest: BPT-SLJ\n-0.0128\n0.0435\n-0.29\n0.7681\n\n\n\na1\n0.2690\n0.0472\n5.70\n&lt;1e-07\n\n\n\nSex: Boys\n0.2189\n0.0141\n15.49\n&lt;1e-53\n\n\n\nTest: Star-Run & a1\n0.1483\n0.1461\n1.01\n0.3102\n\n\n\nTest: S20-Star & a1\n-0.0269\n0.1512\n-0.18\n0.8586\n\n\n\nTest: SLJ-S20 & a1\n-0.0950\n0.1485\n-0.64\n0.5225\n\n\n\nTest: BPT-SLJ & a1\n0.1903\n0.1456\n1.31\n0.1911\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1214\n0.0445\n-2.73\n0.0063\n\n\n\nTest: S20-Star & Sex: Boys\n0.0567\n0.0448\n1.27\n0.2053\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0394\n0.0439\n0.90\n0.3689\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.0834\n0.0435\n1.92\n0.0552\n\n\n\na1 & Sex: Boys\n0.0693\n0.0472\n1.47\n0.1418\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n-0.1716\n0.1461\n-1.17\n0.2404\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.1681\n0.1512\n1.11\n0.2660\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.0967\n0.1485\n-0.65\n0.5151\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n0.0090\n0.1456\n0.06\n0.9506\n\n\n\nTest: Run\n\n\n\n\n0.9592\n\n\nTest: Star_r\n\n\n\n\n0.9874\n\n\nTest: S20_r\n\n\n\n\n0.9709\n\n\nTest: SLJ\n\n\n\n\n0.9677\n\n\nTest: BPT\n\n\n\n\n0.9398\n\n\nResidual\n0.0001\n\n\n\n\n\n\n\n\n\n\nm_cpx_0_SeqDiff = lmm(@formula(zScore ~ 1 + Test * a1 * Sex + (0 + Test | Child)),\n                      dat; contrasts=contr1b, progress)\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n(Intercept)\n-0.0190\n0.0141\n-1.34\n0.1793\n\n\n\nTest: Star-Run\n-0.0073\n0.0445\n-0.16\n0.8695\n\n\n\nTest: S20-Star\n0.0136\n0.0448\n0.30\n0.7615\n\n\n\nTest: SLJ-S20\n-0.0116\n0.0439\n-0.26\n0.7917\n\n\n\nTest: BPT-SLJ\n-0.0128\n0.0435\n-0.29\n0.7681\n\n\n\na1\n0.2690\n0.0472\n5.70\n&lt;1e-07\n\n\n\nSex: Boys\n0.2189\n0.0141\n15.49\n&lt;1e-53\n\n\n\nTest: Star-Run & a1\n0.1483\n0.1461\n1.01\n0.3102\n\n\n\nTest: S20-Star & a1\n-0.0269\n0.1512\n-0.18\n0.8586\n\n\n\nTest: SLJ-S20 & a1\n-0.0950\n0.1485\n-0.64\n0.5225\n\n\n\nTest: BPT-SLJ & a1\n0.1903\n0.1456\n1.31\n0.1911\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1214\n0.0445\n-2.73\n0.0063\n\n\n\nTest: S20-Star & Sex: Boys\n0.0567\n0.0448\n1.27\n0.2053\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0394\n0.0439\n0.90\n0.3689\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.0834\n0.0435\n1.92\n0.0552\n\n\n\na1 & Sex: Boys\n0.0693\n0.0472\n1.47\n0.1418\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n-0.1716\n0.1461\n-1.17\n0.2404\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.1681\n0.1512\n1.11\n0.2660\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.0967\n0.1485\n-0.65\n0.5151\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n0.0090\n0.1456\n0.06\n0.9506\n\n\n\nTest: Run\n\n\n\n\n0.9592\n\n\nTest: Star_r\n\n\n\n\n0.9874\n\n\nTest: S20_r\n\n\n\n\n0.9709\n\n\nTest: SLJ\n\n\n\n\n0.9677\n\n\nTest: BPT\n\n\n\n\n0.9398\n\n\nResidual\n0.0001\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_cpx_0_SeqDiff)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\nChild\nTest: Run\n0.920050551\n0.959192656\n\n\n\n\n\n\n\nTest: Star_r\n0.974948912\n0.987395013\n+0.51\n\n\n\n\n\n\nTest: S20_r\n0.942704181\n0.970929545\n+0.02\n-0.20\n\n\n\n\n\nTest: SLJ\n0.936426387\n0.967691266\n+0.29\n+0.55\n+0.65\n\n\n\n\nTest: BPT\n0.883247904\n0.939812696\n+0.44\n+0.71\n+0.05\n+0.39\n\n\nResidual\n\n0.000000015\n0.000123095\n\n\n\n\n\n\n\n\n\n\nm_cpx_0_SeqDiff.PCA\n\n(Child = \nPrincipal components based on correlation matrix\n Test: Run      1.0     .      .      .      .\n Test: Star_r   0.51   1.0     .      .      .\n Test: S20_r    0.02  -0.2    1.0     .      .\n Test: SLJ      0.29   0.55   0.65   1.0     .\n Test: BPT      0.44   0.71   0.05   0.39   1.0\n\nNormalized cumulative variances:\n[0.5003, 0.7898, 0.9144, 0.9972, 1.0]\n\nComponent loadings\n                 PC1    PC2    PC3    PC4    PC5\n Test: Run     -0.43   0.2   -0.88  -0.04   0.06\n Test: Star_r  -0.54   0.31   0.31  -0.4   -0.6\n Test: S20_r   -0.16  -0.78  -0.14   0.27  -0.51\n Test: SLJ     -0.48  -0.46   0.19  -0.43   0.58\n Test: BPT     -0.51   0.19   0.27   0.77   0.2,)\n\n\n\nf_cpx_1 = @formula(zScore ~ 1 + Test * a1 * Sex + (1 + Test | Child))\nm_cpx_1_SeqDiff = lmm(f_cpx_1, dat; contrasts=contr1b, progress)\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n(Intercept)\n-0.0188\n0.0142\n-1.32\n0.1857\n0.7466\n\n\nTest: Star-Run\n-0.0148\n0.0441\n-0.34\n0.7372\n0.6418\n\n\nTest: S20-Star\n0.0194\n0.0445\n0.44\n0.6634\n1.0510\n\n\nTest: SLJ-S20\n-0.0193\n0.0436\n-0.44\n0.6576\n0.7025\n\n\nTest: BPT-SLJ\n-0.0042\n0.0432\n-0.10\n0.9230\n0.7572\n\n\na1\n0.2680\n0.0474\n5.66\n&lt;1e-07\n\n\n\nSex: Boys\n0.2187\n0.0142\n15.41\n&lt;1e-52\n\n\n\nTest: Star-Run & a1\n0.1768\n0.1447\n1.22\n0.2219\n\n\n\nTest: S20-Star & a1\n-0.0288\n0.1504\n-0.19\n0.8481\n\n\n\nTest: SLJ-S20 & a1\n-0.0933\n0.1476\n-0.63\n0.5273\n\n\n\nTest: BPT-SLJ & a1\n0.1826\n0.1448\n1.26\n0.2073\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1280\n0.0441\n-2.90\n0.0037\n\n\n\nTest: S20-Star & Sex: Boys\n0.0622\n0.0445\n1.40\n0.1619\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0364\n0.0436\n0.84\n0.4035\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.0830\n0.0432\n1.92\n0.0550\n\n\n\na1 & Sex: Boys\n0.0706\n0.0474\n1.49\n0.1362\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n-0.1405\n0.1447\n-0.97\n0.3317\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.1337\n0.1504\n0.89\n0.3740\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.0809\n0.1476\n-0.55\n0.5836\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n0.0088\n0.1448\n0.06\n0.9515\n\n\n\nResidual\n0.0001\n\n\n\n\n\n\n\n\n\n\nm_cpx_1_SeqDiff.PCA\n\n(Child = \nPrincipal components based on correlation matrix\n (Intercept)      1.0     .      .     .      .\n Test: Star-Run   0.42   1.0     .     .      .\n Test: S20-Star  -0.21   0.03   1.0    .      .\n Test: SLJ-S20    0.22   0.26  -0.6   1.0     .\n Test: BPT-SLJ   -0.16  -0.02  -0.5   0.15   1.0\n\nNormalized cumulative variances:\n[0.3925, 0.6872, 0.8366, 0.9589, 1.0]\n\nComponent loadings\n                   PC1    PC2    PC3    PC4    PC5\n (Intercept)     -0.33   0.57   0.14   0.68   0.3\n Test: Star-Run  -0.28   0.56  -0.63  -0.33  -0.32\n Test: S20-Star   0.6    0.3   -0.24  -0.2    0.67\n Test: SLJ-S20   -0.58   0.03   0.35  -0.57   0.46\n Test: BPT-SLJ   -0.34  -0.52  -0.64   0.23   0.38,)",
    "crumbs": [
      "Contrast coding and transformations",
      "Mixed Models Tutorial: Contrast Coding"
    ]
  },
  {
    "objectID": "contrasts_fggk21.html#pca-based-hypothesiscoding-contr4",
    "href": "contrasts_fggk21.html#pca-based-hypothesiscoding-contr4",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "2.4 PCA-based HypothesisCoding: contr4",
    "text": "2.4 PCA-based HypothesisCoding: contr4\nThe fourth set of contrasts uses HypothesisCoding to specify the set of contrasts implementing the loadings of the four principal components of the published LMM based on test scores, not test effects (contrasts) ‚Äî coarse-grained, that is roughly according to their signs. This is actually a very interesting and plausible solution nobody had proposed a priori.\n\nPC1: BPT - Run_r\nPC2: (Star_r + S20_r + SLJ) - (BPT + Run_r)\nPC3: Star_r - (S20_r + SLJ)\nPC4: S20_r - SLJ\n\nPC1 contrasts the worst and the best indicator of physical health; PC2 contrasts these two against the core indicators of physical fitness; PC3 contrasts the cognitive and the physical tests within the narrow set of physical fitness components; and PC4, finally, contrasts two types of lower muscular fitness differing in speed and power.\n\ncontr4 = Dict(\n  :Sex =&gt; EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n  :Test =&gt; HypothesisCoding(\n    [\n      -1  0  0  0 +1\n      -3 +2 +2 +2 -3\n       0 +2 -1 -1  0\n       0  0 +1 -1  0\n    ];\n    levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n    labels=[\"c5.1\", \"c234.15\", \"c2.34\", \"c3.4\"],\n  ),\n);\n\n\nm_cpx_1_PC = lmm(f_cpx_1, dat; contrasts=contr4, progress)\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n(Intercept)\n-0.0191\n0.0142\n-1.34\n0.1790\n0.7527\n\n\nTest: c5.1\n-0.0146\n0.0436\n-0.33\n0.7386\n1.2292\n\n\nTest: c234.15\n0.0461\n0.1691\n0.27\n0.7850\n0.4860\n\n\nTest: c2.34\n-0.0182\n0.0767\n-0.24\n0.8127\n1.2339\n\n\nTest: c3.4\n0.0011\n0.0441\n0.02\n0.9808\n1.2935\n\n\na1\n0.2661\n0.0474\n5.61\n&lt;1e-07\n\n\n\nSex: Boys\n0.2185\n0.0142\n15.39\n&lt;1e-52\n\n\n\nTest: c5.1 & a1\n0.2222\n0.1440\n1.54\n0.1228\n\n\n\nTest: c234.15 & a1\n0.0043\n0.5620\n0.01\n0.9938\n\n\n\nTest: c2.34 & a1\n0.1420\n0.2576\n0.55\n0.5814\n\n\n\nTest: c3.4 & a1\n0.1032\n0.1494\n0.69\n0.4896\n\n\n\nTest: c5.1 & Sex: Boys\n0.0552\n0.0436\n1.27\n0.2059\n\n\n\nTest: c234.15 & Sex: Boys\n-0.6048\n0.1691\n-3.58\n0.0003\n\n\n\nTest: c2.34 & Sex: Boys\n-0.1553\n0.0767\n-2.02\n0.0430\n\n\n\nTest: c3.4 & Sex: Boys\n-0.0443\n0.0441\n-1.00\n0.3154\n\n\n\na1 & Sex: Boys\n0.0708\n0.0474\n1.49\n0.1351\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n-0.0790\n0.1440\n-0.55\n0.5830\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n-0.2611\n0.5620\n-0.46\n0.6422\n\n\n\nTest: c2.34 & a1 & Sex: Boys\n-0.1988\n0.2576\n-0.77\n0.4403\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n0.1219\n0.1494\n0.82\n0.4145\n\n\n\nResidual\n0.0001\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_cpx_1_PC)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\nChild\n(Intercept)\n0.56651366\n0.75267102\n\n\n\n\n\n\n\nTest: c5.1\n1.51095117\n1.22920754\n+0.02\n\n\n\n\n\n\nTest: c234.15\n0.23621476\n0.48601930\n+0.45\n+0.57\n\n\n\n\n\nTest: c2.34\n1.52250094\n1.23389665\n+0.37\n-0.09\n-0.11\n\n\n\n\nTest: c3.4\n1.67304027\n1.29346058\n-0.08\n-0.09\n-0.06\n-0.31\n\n\nResidual\n\n0.00000001\n0.00011939\n\n\n\n\n\n\n\n\n\n\nm_cpx_1_PC.PCA\n\n(Child = \nPrincipal components based on correlation matrix\n (Intercept)     1.0     .      .      .      .\n Test: c5.1      0.02   1.0     .      .      .\n Test: c234.15   0.45   0.57   1.0     .      .\n Test: c2.34     0.37  -0.09  -0.11   1.0     .\n Test: c3.4     -0.08  -0.09  -0.06  -0.31   1.0\n\nNormalized cumulative variances:\n[0.3552, 0.6475, 0.8423, 0.9565, 1.0]\n\nComponent loadings\n                  PC1    PC2    PC3    PC4    PC5\n (Intercept)    -0.5    0.34  -0.55   0.25   0.52\n Test: c5.1     -0.49  -0.4    0.36  -0.53   0.43\n Test: c234.15  -0.65  -0.28  -0.11   0.21  -0.66\n Test: c2.34    -0.18   0.69  -0.0   -0.62  -0.32\n Test: c3.4      0.23  -0.4   -0.74  -0.48  -0.08,)\n\n\nThere is a numerical interaction with a z-value &gt; 2.0 for the first PCA (i.e., BPT - Run_r). This interaction would really need to be replicated to be taken seriously. It is probably due to larger ‚Äúunfitness‚Äù gains in boys than girls (i.e., in BPT) relative to the slightly larger health-related ‚Äúfitness‚Äù gains of girls than boys (i.e., in Run_r).\n\ncontr4b = Dict(\n    :Sex =&gt; EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n    :Test =&gt; HypothesisCoding(\n      [\n        0.49 -0.04  0.20  0.03 -0.85\n        0.70 -0.56 -0.21 -0.13  0.37\n        0.31  0.68 -0.56 -0.35  0.00\n        0.04  0.08  0.61 -0.78  0.13\n      ];\n      levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n      labels=[\"c5.1\", \"c234.15\", \"c12.34\", \"c3.4\"],\n    )\n);\n\n\n# LN_NEWUOA does a poor job here\nm_cpx_1_PC_2 = lmm(f_cpx_1, dat; contrasts=contr4b, progress, optimizer=:LN_BOBYQA)\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n(Intercept)\n-0.0182\n0.0143\n-1.27\n0.2034\n0.7264\n\n\nTest: c5.1\n0.0149\n0.0310\n0.48\n0.6317\n0.6716\n\n\nTest: c234.15\n-0.0039\n0.0316\n-0.12\n0.9026\n0.7525\n\n\nTest: c12.34\n-0.0069\n0.0313\n-0.22\n0.8247\n0.9499\n\n\nTest: c3.4\n0.0111\n0.0305\n0.37\n0.7145\n0.3841\n\n\na1\n0.2639\n0.0477\n5.53\n&lt;1e-07\n\n\n\nSex: Boys\n0.2133\n0.0143\n14.93\n&lt;1e-49\n\n\n\nTest: c5.1 & a1\n-0.1454\n0.1032\n-1.41\n0.1588\n\n\n\nTest: c234.15 & a1\n-0.0489\n0.1043\n-0.47\n0.6391\n\n\n\nTest: c12.34 & a1\n0.0059\n0.1051\n0.06\n0.9551\n\n\n\nTest: c3.4 & a1\n0.0808\n0.1029\n0.79\n0.4319\n\n\n\nTest: c5.1 & Sex: Boys\n-0.0638\n0.0310\n-2.06\n0.0398\n\n\n\nTest: c234.15 & Sex: Boys\n0.1101\n0.0316\n3.48\n0.0005\n\n\n\nTest: c12.34 & Sex: Boys\n-0.0361\n0.0313\n-1.16\n0.2478\n\n\n\nTest: c3.4 & Sex: Boys\n-0.0175\n0.0305\n-0.57\n0.5656\n\n\n\na1 & Sex: Boys\n0.0692\n0.0477\n1.45\n0.1474\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n0.0631\n0.1032\n0.61\n0.5410\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n0.0885\n0.1043\n0.85\n0.3962\n\n\n\nTest: c12.34 & a1 & Sex: Boys\n-0.0679\n0.1051\n-0.65\n0.5186\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n0.0396\n0.1029\n0.39\n0.7002\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_cpx_1_PC_2)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\nChild\n(Intercept)\n0.52767098\n0.72640965\n\n\n\n\n\n\n\nTest: c5.1\n0.45104956\n0.67160223\n-0.15\n\n\n\n\n\n\nTest: c234.15\n0.56628817\n0.75252121\n-0.17\n+0.21\n\n\n\n\n\nTest: c12.34\n0.90228176\n0.94988513\n+0.02\n-0.13\n+0.49\n\n\n\n\nTest: c3.4\n0.14753671\n0.38410508\n-0.61\n-0.41\n-0.48\n-0.50\n\n\nResidual\n\n0.00000000\n0.00000325\n\n\n\n\n\n\n\n\n\n\nm_cpx_1_PC_2.PCA\n\n(Child = \nPrincipal components based on correlation matrix\n (Intercept)     1.0     .      .      .     .\n Test: c5.1     -0.15   1.0     .      .     .\n Test: c234.15  -0.17   0.21   1.0     .     .\n Test: c12.34    0.02  -0.13   0.49   1.0    .\n Test: c3.4     -0.61  -0.41  -0.48  -0.5   1.0\n\nNormalized cumulative variances:\n[0.4243, 0.6909, 0.9188, 0.9998, 1.0]\n\nComponent loadings\n                  PC1    PC2    PC3    PC4   PC5\n (Intercept)    -0.25   0.8   -0.07  -0.2   0.51\n Test: c5.1     -0.24  -0.3   -0.8    0.3   0.36\n Test: c234.15  -0.49  -0.45   0.14  -0.71  0.21\n Test: c12.34   -0.48  -0.14   0.55   0.61  0.29\n Test: c3.4      0.64  -0.24   0.19  -0.05  0.7,)\n\n\n\n# LN_BOBYQA does a poor job here\nm_zcp_1_PC_2 = lmm(@formula(zScore ~ 1 + Test*a1*Sex + zerocorr(1 + Test | Child)),\n                   dat; contrasts=contr4b, progress, optimizer=:LN_NEWUOA)\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n(Intercept)\n-0.0184\n0.0143\n-1.29\n0.1964\n0.7418\n\n\nTest: c5.1\n0.0138\n0.0313\n0.44\n0.6596\n0.6423\n\n\nTest: c234.15\n-0.0059\n0.0313\n-0.19\n0.8504\n0.7125\n\n\nTest: c12.34\n-0.0062\n0.0311\n-0.20\n0.8413\n0.6803\n\n\nTest: c3.4\n0.0003\n0.0313\n0.01\n0.9930\n0.7335\n\n\na1\n0.2615\n0.0477\n5.48\n&lt;1e-07\n\n\n\nSex: Boys\n0.2135\n0.0143\n14.95\n&lt;1e-49\n\n\n\nTest: c5.1 & a1\n-0.1404\n0.1042\n-1.35\n0.1779\n\n\n\nTest: c234.15 & a1\n-0.0360\n0.1035\n-0.35\n0.7277\n\n\n\nTest: c12.34 & a1\n-0.0010\n0.1046\n-0.01\n0.9921\n\n\n\nTest: c3.4 & a1\n0.0880\n0.1055\n0.83\n0.4042\n\n\n\nTest: c5.1 & Sex: Boys\n-0.0624\n0.0313\n-1.99\n0.0463\n\n\n\nTest: c234.15 & Sex: Boys\n0.1128\n0.0313\n3.60\n0.0003\n\n\n\nTest: c12.34 & Sex: Boys\n-0.0357\n0.0311\n-1.15\n0.2517\n\n\n\nTest: c3.4 & Sex: Boys\n-0.0216\n0.0313\n-0.69\n0.4894\n\n\n\na1 & Sex: Boys\n0.0684\n0.0477\n1.43\n0.1514\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n0.0564\n0.1042\n0.54\n0.5887\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n0.0855\n0.1035\n0.83\n0.4087\n\n\n\nTest: c12.34 & a1 & Sex: Boys\n-0.0727\n0.1046\n-0.69\n0.4873\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n0.0664\n0.1055\n0.63\n0.5292\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_zcp_1_PC_2)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\nChild\n(Intercept)\n0.55032486\n0.74183883\n\n\n\n\n\n\n\nTest: c5.1\n0.41249146\n0.64225498\n.\n\n\n\n\n\n\nTest: c234.15\n0.50764132\n0.71248953\n.\n.\n\n\n\n\n\nTest: c12.34\n0.46284655\n0.68032827\n.\n.\n.\n\n\n\n\nTest: c3.4\n0.53798322\n0.73347339\n.\n.\n.\n.\n\n\nResidual\n\n0.00000000\n0.00002321\n\n\n\n\n\n\n\n\n\n\nlikelihoodratiotest(m_zcp_1_PC_2, m_cpx_1_PC_2)\n\n\n\n\n\nmodel-dof\n-2 logLik\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + zerocorr(1 + Test | Child)\n26\n-13100\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n-12954\n146\n10\n&lt;1e-25",
    "crumbs": [
      "Contrast coding and transformations",
      "Mixed Models Tutorial: Contrast Coding"
    ]
  },
  {
    "objectID": "contrasts_fggk21.html#contrasts-are-re-parameterizations-of-the-same-model",
    "href": "contrasts_fggk21.html#contrasts-are-re-parameterizations-of-the-same-model",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "3.1 Contrasts are re-parameterizations of the same model",
    "text": "3.1 Contrasts are re-parameterizations of the same model\nThe choice of contrast does not affect the model objective, in other words, they all yield the same goodness of fit. It does not matter whether a contrast is orthogonal or not.\n\n[\n  objective(m_ovi_SeqDiff),\n  objective(m_ovi_Helmert),\n  objective(m_ovi_Hypo),\n]\n\n3-element Vector{Float64}:\n 13821.527421405826\n 13821.527421405826\n 13821.527421405826",
    "crumbs": [
      "Contrast coding and transformations",
      "Mixed Models Tutorial: Contrast Coding"
    ]
  },
  {
    "objectID": "contrasts_fggk21.html#vcs-and-cps-depend-on-contrast-coding",
    "href": "contrasts_fggk21.html#vcs-and-cps-depend-on-contrast-coding",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "3.2 VCs and CPs depend on contrast coding",
    "text": "3.2 VCs and CPs depend on contrast coding\nTrivially, the meaning of a contrast depends on its definition. Consequently, the contrast specification has a big effect on the random-effect structure. As an illustration, we refit the LMMs with variance components (VCs) and correlation parameters (CPs) for Child-related contrasts of Test. Unfortunately, it is not easy, actually rather quite difficult, to grasp the meaning of correlations of contrast-based effects; they represent two-way interactions.\n\n\nf_Child = @formula(zScore ~ 1 + Test * a1 * Sex + (1 + Test | Child))\nm_Child_SDC = lmm(f_Child, dat; contrasts=contr1, progress, optimizer=:LN_BOBYQA)\nm_Child_HeC = lmm(f_Child, dat; contrasts=contr2, progress, optimizer=:LN_NELDERMEAD)\nm_Child_HyC = lmm(f_Child, dat; contrasts=contr3, progress, optimizer=:LN_NELDERMEAD)\nm_Child_PCA = lmm(f_Child, dat; contrasts=contr4, progress, optimizer=:LN_NELDERMEAD)\n\n\nVarCorr(m_Child_SDC)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\nChild\n(Intercept)\n0.55499402\n0.74497921\n\n\n\n\n\n\n\nTest: Star_r\n0.63307744\n0.79566164\n+0.50\n\n\n\n\n\n\nTest: S20_r\n0.91425140\n0.95616494\n-0.31\n+0.40\n\n\n\n\n\nTest: SLJ\n0.36814993\n0.60675360\n+0.31\n-0.46\n-0.54\n\n\n\n\nTest: BPT\n0.37194511\n0.60987303\n-0.00\n-0.54\n-0.54\n+0.59\n\n\nResidual\n\n0.00000000\n0.00000369\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_Child_HeC)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\nChild\n(Intercept)\n0.06234925\n0.24969832\n\n\n\n\n\n\n\nTest: Star_r\n0.22234231\n0.47153188\n-0.19\n\n\n\n\n\n\nTest: S20_r\n0.25559719\n0.50556621\n+0.12\n-0.05\n\n\n\n\n\nTest: SLJ\n0.10514213\n0.32425628\n-0.38\n+0.17\n+0.68\n\n\n\n\nTest: BPT\n0.05665358\n0.23802012\n-0.14\n-0.29\n+0.13\n-0.29\n\n\nResidual\n\n0.00000000\n0.00000032\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_Child_HyC)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\nChild\n(Intercept)\n0.22635614\n0.47576900\n\n\n\n\n\n\n\nTest: BPT-other\n16.62936852\n4.07791227\n+0.05\n\n\n\n\n\n\nTest: Star-End\n0.57500656\n0.75829187\n+0.14\n-0.69\n\n\n\n\n\nTest: S20-Star\n2.40524329\n1.55088468\n-0.00\n+0.60\n-0.08\n\n\n\n\nTest: SLJ-S20\n0.83962174\n0.91630876\n+0.05\n-0.66\n+0.00\n-0.50\n\n\nResidual\n\n0.00000000\n0.00000032\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_Child_PCA)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\nChild\n(Intercept)\n0.47829129\n0.69158607\n\n\n\n\n\n\n\nTest: c5.1\n1.02921043\n1.01450009\n-0.00\n\n\n\n\n\n\nTest: c234.15\n4.85477599\n2.20335562\n-0.28\n-0.05\n\n\n\n\n\nTest: c2.34\n4.52641644\n2.12753765\n+0.07\n+0.20\n+0.23\n\n\n\n\nTest: c3.4\n1.69069088\n1.30026569\n-0.10\n-0.58\n-0.55\n-0.60\n\n\nResidual\n\n0.00000000\n0.00000027\n\n\n\n\n\n\n\n\n\nThe CPs for the various contrasts are in line with expectations. For the SDC we observe substantial negative CPs between neighboring contrasts. For the orthogonal HeC, all CPs are small; they are uncorrelated. HyC contains some of the SDC contrasts and we observe again the negative CPs. The (roughly) PCA-based contrasts are small with one exception; there is a sizeable CP of +.41 between GM and the core of adjusted physical fitness (c234.15).\nDo these differences in CPs imply that we can move to zcpLMMs when we have orthogonal contrasts? We pursue this question with by refitting the four LMMs with zerocorr() and compare the goodness of fit.\n\nf_Child0 = @formula(zScore ~ 1 + Test * a1 * Sex + zerocorr(1 + Test | Child))\nm_Child_SDC0 = lmm(f_Child0, dat; contrasts=contr1, progress, optimizer=:LN_NEWUOA)\nm_Child_HeC0 = lmm(f_Child0, dat; contrasts=contr2, progress, optimizer=:LN_NEWUOA)\nm_Child_HyC0 = lmm(f_Child0, dat; contrasts=contr3, progress, optimizer=:LN_NEWUOA)\nm_Child_PCA0 = lmm(f_Child0, dat; contrasts=contr4, progress, optimizer=:LN_NEWUOA)\n\n\nlikelihoodratiotest(m_Child_SDC0, m_Child_SDC)\n\n\n\n\n\nmodel-dof\n-2 logLik\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + zerocorr(1 + Test | Child)\n26\n-13157\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n-12987\n170\n10\n&lt;1e-30\n\n\n\n\n\n\nlikelihoodratiotest(m_Child_HeC0, m_Child_HeC)\n\n\n\n\n\nmodel-dof\n-2 logLik\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + zerocorr(1 + Test | Child)\n26\n-13108\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n-12807\n301\n10\n&lt;1e-57\n\n\n\n\n\n\nlikelihoodratiotest(m_Child_HyC0, m_Child_HyC)\n\n\n\n\n\nmodel-dof\n-2 logLik\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + zerocorr(1 + Test | Child)\n26\n-13137\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n-12772\n365\n10\n&lt;1e-71\n\n\n\n\n\n\nlikelihoodratiotest(m_Child_PCA0, m_Child_PCA)\n\n\n\n\n\nmodel-dof\n-2 logLik\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + zerocorr(1 + Test | Child)\n26\n-13062\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n-12759\n303\n10\n&lt;1e-58\n\n\n\n\n\nObviously, we can not drop CPs from any of the LMMs. The full LMMs all have the same objective, but we can compare the goodness-of-fit statistics of zcpLMMs more directly.\n\nzcpLMM = [\"SDC0\", \"HeC0\", \"HyC0\", \"PCA0\"]\nmods = [m_Child_SDC0, m_Child_HeC0, m_Child_HyC0, m_Child_PCA0]\ngof_summary = sort!(\n  DataFrame(;\n    zcpLMM=zcpLMM,\n    dof=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n  ),\n  :deviance,\n)\n\n4√ó5 DataFrame\n\n\n\nRow\nzcpLMM\ndof\ndeviance\nAIC\nBIC\n\n\n\nString\nInt64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nPCA0\n26\n13062.0\n13114.0\n13283.5\n\n\n2\nHeC0\n26\n13108.3\n13160.3\n13329.7\n\n\n3\nHyC0\n26\n13136.9\n13188.9\n13358.3\n\n\n4\nSDC0\n26\n13156.7\n13208.7\n13378.1\n\n\n\n\n\n\nThe best fit was obtained for the PCA-based zcpLMM. Somewhat surprisingly the second best fit was obtained for the SDC. The relatively poor performance of HeC-based zcpLMM is puzzling to me. I thought it might be related to imbalance in design in the present data, but this does not appear to be the case. The same comparison of SequentialDifferenceCoding and Helmert Coding also showed a worse fit for the zcp-HeC LMM than the zcp-SDC LMM.",
    "crumbs": [
      "Contrast coding and transformations",
      "Mixed Models Tutorial: Contrast Coding"
    ]
  },
  {
    "objectID": "contrasts_fggk21.html#vcs-and-cps-depend-on-random-factor",
    "href": "contrasts_fggk21.html#vcs-and-cps-depend-on-random-factor",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "3.3 VCs and CPs depend on random factor",
    "text": "3.3 VCs and CPs depend on random factor\nVCs and CPs resulting from a set of test contrasts can also be estimated for the random factor School. Of course, these VCs and CPs may look different from the ones we just estimated for Child.\nThe effect of age (i.e., developmental gain) varies within School. Therefore, we also include its VCs and CPs in this model; the school-related VC for Sex was not significant.\n\nf_School = @formula(zScore ~ 1 + Test * a1 * Sex + (1 + Test + a1 | School))\nm_School_SeqDiff = lmm(f_School, dat; contrasts=contr1, progress)\nm_School_Helmert = lmm(f_School, dat; contrasts=contr2, progress)\nm_School_Hypo = lmm(f_School, dat; contrasts=contr3, progress)\nm_School_PCA = lmm(f_School, dat; contrasts=contr4, progress)\n\n\nVarCorr(m_School_SeqDiff)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nSchool\n(Intercept)\n0.031571\n0.177681\n\n\n\n\n\n\n\n\nTest: Star_r\n0.114445\n0.338297\n-0.20\n\n\n\n\n\n\n\nTest: S20_r\n0.072370\n0.269017\n+0.23\n-0.79\n\n\n\n\n\n\nTest: SLJ\n0.105410\n0.324669\n-0.22\n+0.38\n-0.82\n\n\n\n\n\nTest: BPT\n0.091810\n0.303002\n-0.05\n-0.05\n+0.50\n-0.89\n\n\n\n\na1\n0.011235\n0.105995\n+0.48\n-0.09\n+0.16\n+0.15\n-0.52\n\n\nResidual\n\n0.874445\n0.935118\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_School_Helmert)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nSchool\n(Intercept)\n0.0315752\n0.1776940\n\n\n\n\n\n\n\n\nTest: Star_r\n0.0286125\n0.1691523\n-0.20\n\n\n\n\n\n\n\nTest: S20_r\n0.0032625\n0.0571182\n+0.15\n-0.25\n\n\n\n\n\n\nTest: SLJ\n0.0031817\n0.0564067\n-0.25\n+0.42\n-0.80\n\n\n\n\n\nTest: BPT\n0.0011214\n0.0334875\n-0.33\n+0.33\n+0.52\n-0.62\n\n\n\n\na1\n0.0112450\n0.1060424\n+0.48\n-0.09\n+0.16\n+0.30\n-0.64\n\n\nResidual\n\n0.8744416\n0.9351158\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_School_Hypo)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nSchool\n(Intercept)\n0.031574\n0.177689\n\n\n\n\n\n\n\n\nTest: BPT-other\n0.448362\n0.669599\n-0.33\n\n\n\n\n\n\n\nTest: Star-End\n0.114409\n0.338243\n-0.20\n+0.33\n\n\n\n\n\n\nTest: S20-Star\n0.072308\n0.268901\n+0.23\n+0.13\n-0.79\n\n\n\n\n\nTest: SLJ-S20\n0.105380\n0.324623\n-0.22\n-0.61\n+0.38\n-0.82\n\n\n\n\na1\n0.011223\n0.105940\n+0.48\n-0.64\n-0.08\n+0.16\n+0.15\n\n\nResidual\n\n0.874450\n0.935120\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_School_PCA)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nSchool\n(Intercept)\n0.031567\n0.177670\n\n\n\n\n\n\n\n\nTest: c5.1\n0.078039\n0.279354\n-0.34\n\n\n\n\n\n\n\nTest: c234.15\n0.822957\n0.907170\n-0.03\n+0.39\n\n\n\n\n\n\nTest: c2.34\n0.108701\n0.329699\n-0.15\n+0.77\n+0.60\n\n\n\n\n\nTest: c3.4\n0.105401\n0.324655\n+0.22\n+0.13\n-0.71\n-0.35\n\n\n\n\na1\n0.011150\n0.105595\n+0.48\n-0.34\n+0.41\n-0.40\n-0.15\n\n\nResidual\n\n0.874482\n0.935138\n\n\n\n\n\n\n\n\n\n\nWe compare again how much of the fit resides in the CPs.\n\nf_School0 = @formula(zScore ~ 1 + Test * a1 * Sex + zerocorr(1 + Test + a1 | School))\nm_School_SDC0 = lmm(f_School0, dat; contrasts=contr1, progress)\nm_School_HeC0 = lmm(f_School0, dat; contrasts=contr2, progress)\nm_School_HyC0 = lmm(f_School0, dat; contrasts=contr3, progress)\nm_School_PCA0 = lmm(f_School0, dat; contrasts=contr4, progress)\n\nzcpLMM2 = [\"SDC0\", \"HeC0\", \"HyC0\", \"PCA0\"]\nmods2 = [\n  m_School_SDC0, m_School_HeC0, m_School_HyC0, m_School_PCA0\n]\ngof_summary2 = sort!(\n  DataFrame(;\n    zcpLMM=zcpLMM2,\n    dof=dof.(mods2),\n    deviance=deviance.(mods2),\n    AIC=aic.(mods2),\n    BIC=bic.(mods2),\n  ),\n  :deviance,\n)\n\n4√ó5 DataFrame\n\n\n\nRow\nzcpLMM\ndof\ndeviance\nAIC\nBIC\n\n\n\nString\nInt64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nHeC0\n27\n13820.2\n13874.2\n14050.2\n\n\n2\nPCA0\n27\n13820.3\n13874.3\n14050.3\n\n\n3\nHyC0\n27\n13822.8\n13876.8\n14052.8\n\n\n4\nSDC0\n27\n13822.9\n13876.9\n14052.9\n\n\n\n\n\n\nFor the random factor School the Helmert contrast, followed by PCA-based contrasts have least information in the CPs; SDC has the largest contribution from CPs. Interesting.",
    "crumbs": [
      "Contrast coding and transformations",
      "Mixed Models Tutorial: Contrast Coding"
    ]
  },
  {
    "objectID": "fggk21.html",
    "href": "fggk21.html",
    "title": "Basics with Emotikon Project",
    "section": "",
    "text": "This script uses a subset of data reported in F√ºhner et al. (2021). To circumvent delays associated with model fitting we work with models that are less complex than those in the reference publication. All the data to reproduce the models in the publication are used here, too; the script requires only a few changes to specify the more complex models in the article.\nThe script is structured in four main sections:"
  },
  {
    "objectID": "fggk21.html#read-data",
    "href": "fggk21.html#read-data",
    "title": "Basics with Emotikon Project",
    "section": "3.1 Read data",
    "text": "3.1 Read data\n\ndf = DataFrame(dataset(:fggk21))\ntransform!(df,\n    :age =&gt; (x -&gt; x .- 8.5) =&gt; :a1,\n    :Sex =&gt; categorical =&gt; :Sex,\n    :Test =&gt; categorical =&gt; :Test,\n  )\nlevels!(df.Sex, [\"male\", \"female\"])\nrecode!(df.Sex, \"male\" =&gt; \"Boys\", \"female\" =&gt; \"Girls\")\nlevels!(df.Test, [\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"])\nrecode!(\n  df.Test,\n  \"Run\" =&gt; \"Endurance\",\n  \"Star_r\" =&gt; \"Coordination\",\n  \"S20_r\" =&gt; \"Speed\",\n  \"SLJ\" =&gt; \"PowerLOW\",\n  \"BPT\" =&gt; \"PowerUP\",\n)\ndescribe(df)\n\n8√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nDataType\n\n\n\n\n1\nCohort\n\n2011\n\n2019\n0\nString\n\n\n2\nSchool\n\nS100043\n\nS800200\n0\nString\n\n\n3\nChild\n\nC002352\n\nC117966\n0\nString\n\n\n4\nSex\n\nBoys\n\nGirls\n0\nCategoricalValue{String, UInt32}\n\n\n5\nage\n8.56073\n7.99452\n8.55852\n9.10609\n0\nFloat64\n\n\n6\nTest\n\nEndurance\n\nPowerUP\n0\nCategoricalValue{String, UInt32}\n\n\n7\nscore\n226.141\n1.14152\n4.65116\n1530.0\n0\nFloat64\n\n\n8\na1\n0.0607297\n-0.505476\n0.0585216\n0.606092\n0\nFloat64\n\n\n\n\n\n\n\n3.1.1 Transformations\nWe center age at 8.5 years and compute z-scores for each Test. With these variables the data frame df contains all variables used for the final model in the original publication.\n\nselect!(groupby(df, :Test),  Not(:score), :score =&gt; zscore =&gt; :zScore)\n\n525126√ó8 DataFrame525101 rows omitted\n\n\n\nRow\nTest\nCohort\nSchool\nChild\nSex\nage\na1\nzScore\n\n\n\nCat‚Ä¶\nString\nString\nString\nCat‚Ä¶\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nSpeed\n2013\nS100067\nC002352\nBoys\n7.99452\n-0.505476\n1.7913\n\n\n2\nPowerUP\n2013\nS100067\nC002352\nBoys\n7.99452\n-0.505476\n-0.0622317\n\n\n3\nPowerLOW\n2013\nS100067\nC002352\nBoys\n7.99452\n-0.505476\n-0.0336567\n\n\n4\nCoordination\n2013\nS100067\nC002352\nBoys\n7.99452\n-0.505476\n1.46874\n\n\n5\nEndurance\n2013\nS100067\nC002352\nBoys\n7.99452\n-0.505476\n0.331058\n\n\n6\nSpeed\n2013\nS100067\nC002353\nBoys\n7.99452\n-0.505476\n1.15471\n\n\n7\nPowerUP\n2013\nS100067\nC002353\nBoys\n7.99452\n-0.505476\n0.498354\n\n\n8\nPowerLOW\n2013\nS100067\nC002353\nBoys\n7.99452\n-0.505476\n-0.498822\n\n\n9\nCoordination\n2013\nS100067\nC002353\nBoys\n7.99452\n-0.505476\n-0.9773\n\n\n10\nEndurance\n2013\nS100067\nC002353\nBoys\n7.99452\n-0.505476\n0.574056\n\n\n11\nSpeed\n2013\nS100067\nC002354\nBoys\n7.99452\n-0.505476\n0.0551481\n\n\n12\nPowerUP\n2013\nS100067\nC002354\nBoys\n7.99452\n-0.505476\n0.218061\n\n\n13\nPowerLOW\n2013\nS100067\nC002354\nBoys\n7.99452\n-0.505476\n-0.757248\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n525115\nCoordination\n2018\nS401470\nC117964\nBoys\n9.10609\n0.606092\n-1.43175\n\n\n525116\nEndurance\n2018\nS401470\nC117964\nBoys\n9.10609\n0.606092\n-0.944681\n\n\n525117\nSpeed\n2018\nS401470\nC117965\nGirls\n9.10609\n0.606092\n0.31086\n\n\n525118\nPowerUP\n2018\nS401470\nC117965\nGirls\n9.10609\n0.606092\n0.0779146\n\n\n525119\nPowerLOW\n2018\nS401470\nC117965\nGirls\n9.10609\n0.606092\n-0.137027\n\n\n525120\nCoordination\n2018\nS401470\nC117965\nGirls\n9.10609\n0.606092\n-1.8077\n\n\n525121\nEndurance\n2018\nS401470\nC117965\nGirls\n9.10609\n0.606092\n0.513306\n\n\n525122\nSpeed\n2018\nS800200\nC117966\nBoys\n9.10609\n0.606092\n0.0551481\n\n\n525123\nPowerUP\n2018\nS800200\nC117966\nBoys\n9.10609\n0.606092\n0.0779146\n\n\n525124\nPowerLOW\n2018\nS800200\nC117966\nBoys\n9.10609\n0.606092\n-1.32578\n\n\n525125\nCoordination\n2018\nS800200\nC117966\nBoys\n9.10609\n0.606092\n0.473217\n\n\n525126\nEndurance\n2018\nS800200\nC117966\nBoys\n9.10609\n0.606092\n-0.0941883"
  },
  {
    "objectID": "fggk21.html#extract-a-stratified-subsample",
    "href": "fggk21.html#extract-a-stratified-subsample",
    "title": "Basics with Emotikon Project",
    "section": "3.2 Extract a stratified subsample",
    "text": "3.2 Extract a stratified subsample\nFor the prupose of the tutorial, we extract a random sample of 1000 boys and 1000 girls. Child, School, and Cohort are grouping variables. Traditionally, they are called random factors because the units (levels) of the factor are assumed to be a random sample from the population of their units (levels).\nCohort has only nine ‚Äúgroups‚Äù and could have been included as a set of polynomical fixed-effect contrasts rather than a random factor. This choice warrants a short excursion: The secular trends are very different for different tests and require the inclusion of interaction terms with Test contrasts (see Figure 4 in (F√ºhner et al., 2021). The authors opted to absorb these effects in cohort-related variance components for the Test contrasts and plan to address the details of secular changes in a separate analysis.\nFor complex designs, when they are in the theoretical focus of an article, factors and covariates should be specified as part of the fixed effects. If they are not in the theoretical focus, but serve as statistical control variables, they could be put in the RES - if supported by the data.\nStratified sampling: We generate a Child table with information about children. MersenneTwister(42) specifies 42 as the seed for the random number generator to ensure reproducibility of the stratification. For a different pattern of results choose, for example, 84. We randomly sample 1000 boys and 1000 girls from this table; they are stored in samp. Then, we extract the corresponding subset of these children‚Äôs test scores from df and store them dat.\n\nChild = unique(select(df, :Cohort, :School, :Child, :Sex, :age))\nsample = let\n  rng = MersenneTwister(42)\n  combine(\n    groupby(Child, :Sex), x -&gt; x[rand(rng, 1:nrow(x), 1000), :]\n  )\nend\ninsamp(x) = x ‚àà sample.Child\ndat = @subset(df, insamp(:Child))\n\n9621√ó8 DataFrame9596 rows omitted\n\n\n\nRow\nTest\nCohort\nSchool\nChild\nSex\nage\na1\nzScore\n\n\n\nCat‚Ä¶\nString\nString\nString\nCat‚Ä¶\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nSpeed\n2013\nS100547\nC002378\nBoys\n7.99452\n-0.505476\n0.578748\n\n\n2\nPowerUP\n2013\nS100547\nC002378\nBoys\n7.99452\n-0.505476\n-0.762963\n\n\n3\nPowerLOW\n2013\nS100547\nC002378\nBoys\n7.99452\n-0.505476\n-0.808933\n\n\n4\nCoordination\n2013\nS100547\nC002378\nBoys\n7.99452\n-0.505476\n-0.846558\n\n\n5\nEndurance\n2013\nS100547\nC002378\nBoys\n7.99452\n-0.505476\n-0.397936\n\n\n6\nSpeed\n2013\nS102489\nC002562\nGirls\n7.99452\n-0.505476\n-0.189199\n\n\n7\nPowerUP\n2013\nS102489\nC002562\nGirls\n7.99452\n-0.505476\n-0.622817\n\n\n8\nPowerLOW\n2013\nS102489\nC002562\nGirls\n7.99452\n-0.505476\n-1.63589\n\n\n9\nCoordination\n2013\nS102489\nC002562\nGirls\n7.99452\n-0.505476\n-1.62614\n\n\n10\nEndurance\n2013\nS102489\nC002562\nGirls\n7.99452\n-0.505476\n0.12181\n\n\n11\nSpeed\n2013\nS103226\nC002618\nBoys\n7.99452\n-0.505476\n-0.422921\n\n\n12\nPowerUP\n2013\nS103226\nC002618\nBoys\n7.99452\n-0.505476\n-0.482671\n\n\n13\nPowerLOW\n2013\nS103226\nC002618\nBoys\n7.99452\n-0.505476\n-0.292082\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n9610\nPowerLOW\n2018\nS111570\nC117818\nGirls\n9.10609\n0.606092\n0.483194\n\n\n9611\nCoordination\n2018\nS111570\nC117818\nGirls\n9.10609\n0.606092\n-0.31554\n\n\n9612\nEndurance\n2018\nS111570\nC117818\nGirls\n9.10609\n0.606092\n-0.0941883\n\n\n9613\nPowerUP\n2018\nS111739\nC117841\nGirls\n9.10609\n0.606092\n0.0779146\n\n\n9614\nPowerLOW\n2018\nS111739\nC117841\nGirls\n9.10609\n0.606092\n-1.11904\n\n\n9615\nCoordination\n2018\nS111739\nC117841\nGirls\n9.10609\n0.606092\n-0.95589\n\n\n9616\nEndurance\n2018\nS111739\nC117841\nGirls\n9.10609\n0.606092\n-2.58492\n\n\n9617\nSpeed\n2018\nS112999\nC117902\nGirls\n9.10609\n0.606092\n1.7913\n\n\n9618\nPowerUP\n2018\nS112999\nC117902\nGirls\n9.10609\n0.606092\n-0.0622317\n\n\n9619\nPowerLOW\n2018\nS112999\nC117902\nGirls\n9.10609\n0.606092\n0.483194\n\n\n9620\nCoordination\n2018\nS112999\nC117902\nGirls\n9.10609\n0.606092\n0.708474\n\n\n9621\nEndurance\n2018\nS112999\nC117902\nGirls\n9.10609\n0.606092\n0.574056\n\n\n\n\n\n\nDue to missing scores for some tests we have about 2% less than 10,000 observtions."
  },
  {
    "objectID": "fggk21.html#no-evidence-for-age-x-sex-x-test-interaction",
    "href": "fggk21.html#no-evidence-for-age-x-sex-x-test-interaction",
    "title": "Basics with Emotikon Project",
    "section": "3.3 No evidence for age x Sex x Test interaction",
    "text": "3.3 No evidence for age x Sex x Test interaction\nThe main results are captured in the figure constructed in this section. We build it both for the full data and the stratified subset.\n\ndf2 = combine(\n  groupby(\n    select(df, :, :age =&gt; ByRow(x -&gt; round(x; digits=1)) =&gt; :age),\n    [:Sex, :Test, :age],\n  ),\n  :zScore =&gt; mean =&gt; :zScore,\n  :zScore =&gt; length =&gt; :n,\n)\n\n120√ó5 DataFrame95 rows omitted\n\n\n\nRow\nSex\nTest\nage\nzScore\nn\n\n\n\nCat‚Ä¶\nCat‚Ä¶\nFloat64\nFloat64\nInt64\n\n\n\n\n1\nBoys\nSpeed\n8.0\n-0.0265138\n1223\n\n\n2\nBoys\nPowerUP\n8.0\n0.026973\n1227\n\n\n3\nBoys\nPowerLOW\n8.0\n0.121609\n1227\n\n\n4\nBoys\nCoordination\n8.0\n-0.0571726\n1186\n\n\n5\nBoys\nEndurance\n8.0\n0.292695\n1210\n\n\n6\nGirls\nSpeed\n8.0\n-0.35164\n1411\n\n\n7\nGirls\nPowerUP\n8.0\n-0.610355\n1417\n\n\n8\nGirls\nPowerLOW\n8.0\n-0.279872\n1418\n\n\n9\nGirls\nCoordination\n8.0\n-0.268221\n1381\n\n\n10\nGirls\nEndurance\n8.0\n-0.245573\n1387\n\n\n11\nBoys\nSpeed\n8.1\n0.0608397\n3042\n\n\n12\nBoys\nPowerUP\n8.1\n0.0955413\n3069\n\n\n13\nBoys\nPowerLOW\n8.1\n0.123099\n3069\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n109\nBoys\nCoordination\n9.0\n0.254973\n4049\n\n\n110\nBoys\nEndurance\n9.0\n0.258082\n4034\n\n\n111\nGirls\nSpeed\n9.1\n-0.0286172\n1154\n\n\n112\nGirls\nPowerUP\n9.1\n-0.0752301\n1186\n\n\n113\nGirls\nPowerLOW\n9.1\n-0.094587\n1174\n\n\n114\nGirls\nCoordination\n9.1\n0.00276252\n1162\n\n\n115\nGirls\nEndurance\n9.1\n-0.235591\n1150\n\n\n116\nBoys\nSpeed\n9.1\n0.325745\n1303\n\n\n117\nBoys\nPowerUP\n9.1\n0.616416\n1320\n\n\n118\nBoys\nPowerLOW\n9.1\n0.267577\n1310\n\n\n119\nBoys\nCoordination\n9.1\n0.254342\n1297\n\n\n120\nBoys\nEndurance\n9.1\n0.251045\n1294\n\n\n\n\n\n\n\n3.3.1 Figure(s) of interaction\nThe core results of the article are reported in Figure 2 of F√ºhner et al. (2021). In summary:\n\nMain effects of age and Sex: There are developmental gains in the ninth year of life; boys outperform girls. There is no main effect of Test because of z-scoring.\nInteractions of Test and age: Tests differ in how much children improve during the year (i.e., the magnitude of developmental gain), that is slopes depend on Test.\nInteractions of Test and Sex: The sex difference is test dependent, that is the difference between the slopes depends on Test.\nThe most distinctive result is the absence of evidence for an age x Sex x Test interaction, that is the slopes for boys and girls are statistically parallel for each of the five tests.\n\n\n\nCode\nlet\n  design1 = mapping(:age, :zScore; color=:Sex, col=:Test)\n  lines1 = design1 * linear()\n  means1 = design1 * visual(Scatter; markersize=5)\n  draw(data(df2) * means1 + data(df) * lines1;)\nend\n\n\n\n\n\n\n\nFigure¬†1: Age trends by sex for each Test for the full data set\n\n\n\n\nFigure¬†1 shows performance differences for the full set of data between 8.0 and 9.2 years by sex in the five physical fitness tests presented as z-transformed data computed separately for each test.\n\nEndurance = cardiorespiratory endurance (i.e., 6-min-run test),\nCoordination = star-run test,\nSpeed = 20-m linear sprint test,\nPowerLOW = power of lower limbs (i.e., standing long jump test),\nPowerUP = power of upper limbs (i.e., ball push test),\nSD = standard deviation. Points are binned observed child means; lines are simple regression fits to the observations.\n\nWhat do the results look like for the stratified subsample? Here the parallelism is much less clear. In the final LMM we test whether the two regression lines in each of the five panels are statistically parallel for this subset of data. That is, we test the interaction of Sex and age as nested within the levels of Test. Most people want to know the signficance of these five Sex x age interactions.\nThe theoretical focus of the article, however, is on comparisons between tests displayed next to each other. We ask whether the degree of parallelism is statistically the same for Endurance and Coordination (H1), Coordination and Speed (H2), Speed and PowerLOW (H3), and PowerLow and PowerUP (H4). Hypotheses H1 to H4 require Sequential Difference contrasts c1 to c4 for Test; they are tested as fixed effects for`H1 x age x Sex, H2 x age x Sex, H3 x age x Sex, and H4 x age x Sex.\n\n\nCode\ndat2 = combine(\n  groupby(\n    select(dat, :, :age =&gt; ByRow(x -&gt; round(x; digits=1)) =&gt; :age),\n    [:Sex, :Test, :age],\n  ),\n  :zScore =&gt; mean =&gt; :zScore,\n  :zScore =&gt; length =&gt; :n,\n)\n\n\n120√ó5 DataFrame95 rows omitted\n\n\n\nRow\nSex\nTest\nage\nzScore\nn\n\n\n\nCat‚Ä¶\nCat‚Ä¶\nFloat64\nFloat64\nInt64\n\n\n\n\n1\nBoys\nSpeed\n8.0\n0.0108467\n20\n\n\n2\nBoys\nPowerUP\n8.0\n0.0779146\n20\n\n\n3\nBoys\nPowerLOW\n8.0\n0.0800505\n20\n\n\n4\nBoys\nCoordination\n8.0\n0.0843624\n19\n\n\n5\nBoys\nEndurance\n8.0\n0.287361\n19\n\n\n6\nGirls\nSpeed\n8.0\n-0.430054\n27\n\n\n7\nGirls\nPowerUP\n8.0\n-0.799298\n27\n\n\n8\nGirls\nPowerLOW\n8.0\n-0.408374\n28\n\n\n9\nGirls\nCoordination\n8.0\n-0.323287\n28\n\n\n10\nGirls\nEndurance\n8.0\n-0.380386\n25\n\n\n11\nGirls\nSpeed\n8.1\n-0.461211\n55\n\n\n12\nGirls\nPowerUP\n8.1\n-0.566758\n55\n\n\n13\nGirls\nPowerLOW\n8.1\n-0.309937\n55\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n109\nGirls\nCoordination\n9.0\n-0.0750398\n57\n\n\n110\nGirls\nEndurance\n9.0\n-0.42162\n57\n\n\n111\nBoys\nSpeed\n9.1\n0.119451\n29\n\n\n112\nBoys\nPowerUP\n9.1\n0.618479\n28\n\n\n113\nBoys\nPowerLOW\n9.1\n-0.131489\n28\n\n\n114\nBoys\nCoordination\n9.1\n0.145517\n28\n\n\n115\nBoys\nEndurance\n9.1\n-0.0891883\n27\n\n\n116\nGirls\nSpeed\n9.1\n-0.0435788\n20\n\n\n117\nGirls\nPowerUP\n9.1\n0.230801\n22\n\n\n118\nGirls\nPowerLOW\n9.1\n-0.0538813\n23\n\n\n119\nGirls\nCoordination\n9.1\n0.282882\n23\n\n\n120\nGirls\nEndurance\n9.1\n-0.0865179\n22\n\n\n\n\n\n\n\n\nCode\nlet\n  design2 = mapping(:age, :zScore; color=:Sex, col=:Test)\n  lines2 = design2 * linear()\n  means2 = design2 * visual(Scatter; markersize=5)\n  draw(data(dat2) * means2 + data(dat) * lines2;)\nend\n\n\n\n\n\n\n\nFigure¬†2: Age trends by sex for each Test for the stratified sample\n\n\n\n\nFigure¬†2 Performance differences for subset of data between 8.0 and 9.2 years by sex in the five physical fitness tests presented as z-transformed data computed separately for each test.\n\nEndurance = cardiorespiratory endurance (i.e., 6-min-run test),\nCoordination = star-run test,\nSpeed = 20-m linear sprint test,\nPowerLOW = power of lower limbs (i.e., standing long jump test),\nPowerUP = power of upper limbs (i.e., ball push test),\nSD = standard deviation. Points are binned observed child means; lines are simple regression fits to the observations.\n\n\n\n3.3.2 Regression on age by Sex for each Test\nAnother set of relevant statistics are the slopes for the regression of performance on age for boys and girls in each of the five tests. The lines in Figures 1 and 2, however, are computed directly from the raw data with the linear() command.\n\ncombine(\n  groupby(df, [:Sex, :Test]),\n  [:age, :zScore] =&gt; simplelinreg =&gt; :coef,\n)\n\n10√ó3 DataFrame\n\n\n\nRow\nSex\nTest\ncoef\n\n\n\nCat‚Ä¶\nCat‚Ä¶\nTuple‚Ä¶\n\n\n\n\n1\nBoys\nEndurance\n(0.00256718, 0.0291899)\n\n\n2\nBoys\nCoordination\n(-2.47279, 0.302819)\n\n\n3\nBoys\nSpeed\n(-2.12689, 0.267153)\n\n\n4\nBoys\nPowerLOW\n(-1.4307, 0.189659)\n\n\n5\nBoys\nPowerUP\n(-4.35864, 0.549005)\n\n\n6\nGirls\nEndurance\n(-0.692022, 0.0523217)\n\n\n7\nGirls\nCoordination\n(-2.50524, 0.279119)\n\n\n8\nGirls\nSpeed\n(-2.34431, 0.255687)\n\n\n9\nGirls\nPowerLOW\n(-1.87241, 0.196917)\n\n\n10\nGirls\nPowerUP\n(-4.82271, 0.524799)\n\n\n\n\n\n\n\ncombine(\n  groupby(dat, [:Sex, :Test]),\n  [:age, :zScore] =&gt; simplelinreg =&gt; :coef,\n)\n\n10√ó3 DataFrame\n\n\n\nRow\nSex\nTest\ncoef\n\n\n\nCat‚Ä¶\nCat‚Ä¶\nTuple‚Ä¶\n\n\n\n\n1\nBoys\nEndurance\n(-0.681676, 0.111322)\n\n\n2\nBoys\nCoordination\n(-1.11067, 0.141854)\n\n\n3\nBoys\nSpeed\n(-1.37849, 0.178035)\n\n\n4\nBoys\nPowerLOW\n(-1.44439, 0.194219)\n\n\n5\nBoys\nPowerUP\n(-3.26371, 0.420349)\n\n\n6\nGirls\nEndurance\n(-0.350538, 0.0141756)\n\n\n7\nGirls\nCoordination\n(-2.08942, 0.234567)\n\n\n8\nGirls\nSpeed\n(-1.93461, 0.213441)\n\n\n9\nGirls\nPowerLOW\n(0.0629858, -0.0240463)\n\n\n10\nGirls\nPowerUP\n(-4.30863, 0.469259)"
  },
  {
    "objectID": "fggk21.html#seqdiffcoding-of-test",
    "href": "fggk21.html#seqdiffcoding-of-test",
    "title": "Basics with Emotikon Project",
    "section": "3.4 SeqDiffCoding of Test",
    "text": "3.4 SeqDiffCoding of Test\nSeqDiffCoding was used in the publication. This specification tests pairwise differences between the five neighboring levels of Test, that is:\n\nH1: Star_r - Run (2-1)\nH2: S20_r - Star_r (3-2)\nH3: SLJ - S20_r (4-3)\nH4: BPT - SLJ (5-4)\n\nThe levels were sorted such that these contrasts map onto four a priori hypotheses; in other words, they are theoretically motivated pairwise comparisons. The motivation also encompasses theoretically motivated interactions with Sex. The order of levels can also be explicitly specified during contrast construction. This is very useful if levels are in a different order in the dataframe.\nNote that random factors Child, School, and Cohort are declared as Grouping variables. Technically, this specification is required for variables with a very large number of levels (e.g., 100K+ children). We recommend the explicit specification for all random factors as a general coding style.\nThe first command recodes names indicating the physical fitness components used in the above figures and tables back to the shorter actual test names. This reduces clutter in LMM outputs.\n\nrecode!(\n  dat.Test,\n  \"Endurance\" =&gt; \"Run\",\n  \"Coordination\" =&gt; \"Star_r\",\n  \"Speed\" =&gt; \"S20_r\",\n  \"PowerLOW\" =&gt; \"SLJ\",\n  \"PowerUP\" =&gt; \"BMT\",\n)\ncontrasts = Dict{Symbol,Any}(nm =&gt; SeqDiffCoding() for nm in (:Test, :Sex))\n\nDict{Symbol, Any} with 2 entries:\n  :Test =&gt; SeqDiffCoding(nothing)\n  :Sex  =&gt; SeqDiffCoding(nothing)\n\n\nThe statistical disadvantage of SeqDiffCoding is that the contrasts are not orthogonal, that is the contrasts are correlated. This is obvious from the fact that levels 2, 3, and 4 are all used in two contrasts. One consequence of this is that correlation parameters estimated between neighboring contrasts (e.g., 2-1 and 3-2) are difficult to interpret. Usually, they will be negative because assuming some practical limitations on the overall range (e.g., between levels 1 and 3), a small ‚Äú2-1‚Äù effect ‚Äúcorrelates‚Äù negatively with a larger ‚Äú3-2‚Äù effect for mathematical reasons.\nObviously, the tradeoff between theoretical motivation and statistical purity is something that must be considered carefully when planning the analysis.\nVarious options for contrast coding are the topic of the MixedModelsTutorial_contrasts_emotikon.jl and MixedModelsTutorial_contrasts_kwdyz.jl notebooks."
  },
  {
    "objectID": "fggk21.html#lmm-m_ovi",
    "href": "fggk21.html#lmm-m_ovi",
    "title": "Basics with Emotikon Project",
    "section": "4.1 LMM m_ovi",
    "text": "4.1 LMM m_ovi\nIn its random-effect structure (RES) we only vary intercepts (i.e., Grand Means) for School (LMM m_ovi), that is we allow that the schools differ in the average fitness of its children, average over the five tests.\nIt is well known that such a simple RES is likely to be anti-conservative with respect to fixed-effect test statistics.\n\nm_ovi = let\n  f = @formula zScore ~ 1 + Test * Sex * a1 + (1 | School)\n  fit(MixedModel, f, dat; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_School\n\n\n(Intercept)\n-0.0079\n0.0195\n-0.40\n0.6858\n0.3367\n\n\nTest: Star_r\n-0.0208\n0.0303\n-0.68\n0.4934\n\n\n\nTest: S20_r\n0.0097\n0.0302\n0.32\n0.7492\n\n\n\nTest: SLJ\n0.0245\n0.0301\n0.81\n0.4162\n\n\n\nTest: BMT\n-0.0363\n0.0301\n-1.21\n0.2271\n\n\n\nSex: Girls\n-0.3839\n0.0208\n-18.49\n&lt;1e-75\n\n\n\na1\n0.1901\n0.0357\n5.32\n&lt;1e-06\n\n\n\nTest: Star_r & Sex: Girls\n0.3042\n0.0606\n5.02\n&lt;1e-06\n\n\n\nTest: S20_r & Sex: Girls\n-0.0679\n0.0603\n-1.12\n0.2608\n\n\n\nTest: SLJ & Sex: Girls\n-0.0898\n0.0602\n-1.49\n0.1355\n\n\n\nTest: BMT & Sex: Girls\n-0.2812\n0.0602\n-4.67\n&lt;1e-05\n\n\n\nTest: Star_r & a1\n0.1230\n0.1037\n1.19\n0.2356\n\n\n\nTest: S20_r & a1\n0.0113\n0.1032\n0.11\n0.9129\n\n\n\nTest: SLJ & a1\n-0.1133\n0.1029\n-1.10\n0.2708\n\n\n\nTest: BMT & a1\n0.3544\n0.1027\n3.45\n0.0006\n\n\n\nSex: Girls & a1\n0.0626\n0.0717\n0.87\n0.3826\n\n\n\nTest: Star_r & Sex: Girls & a1\n0.1758\n0.2074\n0.85\n0.3965\n\n\n\nTest: S20_r & Sex: Girls & a1\n-0.0452\n0.2063\n-0.22\n0.8265\n\n\n\nTest: SLJ & Sex: Girls & a1\n-0.2598\n0.2058\n-1.26\n0.2068\n\n\n\nTest: BMT & Sex: Girls & a1\n0.2645\n0.2054\n1.29\n0.1980\n\n\n\nResidual\n0.9170\n\n\n\n\n\n\n\n\n\nIs the model singular (overparameterized, degenerate)? In other words: Is the model not supported by the data?\n\nissingular(m_ovi)\n\nfalse\n\n\nModels varying only in intercepts are almost always supported by the data."
  },
  {
    "objectID": "fggk21.html#lmm-m_zcp",
    "href": "fggk21.html#lmm-m_zcp",
    "title": "Basics with Emotikon Project",
    "section": "4.2 LMM m_zcp",
    "text": "4.2 LMM m_zcp\nIn this LMM we allow that schools differ not only in GM, but also in the size of the four contrasts defined for Test, in the difference between boys and girls (Sex) and the developmental gain children achieve within the third grade (age).\nWe assume that there is covariance associated with these CPs beyond residual noise, that is we assume that there is no detectable evidence in the data that the CPs are different from zero.\n\nm_zcp = let\n  f = @formula(\n    zScore ~\n      1 + Test * Sex * a1 + zerocorr(1 + Test + Sex + a1 | School)\n  )\n  fit(MixedModel, f, dat; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_School\n\n\n(Intercept)\n0.0002\n0.0196\n0.01\n0.9916\n0.3150\n\n\nTest: Star_r\n-0.0134\n0.0308\n-0.44\n0.6628\n0.2033\n\n\nTest: S20_r\n0.0112\n0.0288\n0.39\n0.6959\n0.0700\n\n\nTest: SLJ\n0.0239\n0.0284\n0.84\n0.4008\n0.0000\n\n\nTest: BMT\n-0.0277\n0.0302\n-0.92\n0.3584\n0.1813\n\n\nSex: Girls\n-0.3806\n0.0341\n-11.17\n&lt;1e-28\n0.4879\n\n\na1\n0.1864\n0.0596\n3.13\n0.0018\n0.8207\n\n\nTest: Star_r & Sex: Girls\n0.3107\n0.0579\n5.37\n&lt;1e-07\n\n\n\nTest: S20_r & Sex: Girls\n-0.0686\n0.0571\n-1.20\n0.2292\n\n\n\nTest: SLJ & Sex: Girls\n-0.0869\n0.0568\n-1.53\n0.1263\n\n\n\nTest: BMT & Sex: Girls\n-0.2854\n0.0573\n-4.98\n&lt;1e-06\n\n\n\nTest: Star_r & a1\n0.1138\n0.0992\n1.15\n0.2513\n\n\n\nTest: S20_r & a1\n0.0174\n0.0976\n0.18\n0.8582\n\n\n\nTest: SLJ & a1\n-0.1120\n0.0971\n-1.15\n0.2490\n\n\n\nTest: BMT & a1\n0.3461\n0.0980\n3.53\n0.0004\n\n\n\nSex: Girls & a1\n0.0991\n0.0820\n1.21\n0.2269\n\n\n\nTest: Star_r & Sex: Girls & a1\n0.1113\n0.1985\n0.56\n0.5749\n\n\n\nTest: S20_r & Sex: Girls & a1\n-0.0511\n0.1952\n-0.26\n0.7934\n\n\n\nTest: SLJ & Sex: Girls & a1\n-0.2599\n0.1943\n-1.34\n0.1810\n\n\n\nTest: BMT & Sex: Girls & a1\n0.2592\n0.1960\n1.32\n0.1860\n\n\n\nResidual\n0.8651\n\n\n\n\n\n\n\n\n\nDepending on sampling, this model estimating variance components for School may or may not be supported by the data.\n\nissingular(m_zcp)\n\ntrue"
  },
  {
    "objectID": "fggk21.html#lmm-m_cpx",
    "href": "fggk21.html#lmm-m_cpx",
    "title": "Basics with Emotikon Project",
    "section": "4.3 LMM m_cpx",
    "text": "4.3 LMM m_cpx\nIn the complex LMM investigated in this sequence we give up the assumption of zero-correlation between VCs.\n\nm_cpx = let\n  f = @formula(\n    zScore ~ 1 + Test * Sex * a1 + (1 + Test + Sex + a1 | School)\n  )\n  fit(MixedModel, f, dat; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_School\n\n\n(Intercept)\n-0.0019\n0.0196\n-0.10\n0.9227\n0.3184\n\n\nTest: Star_r\n-0.0146\n0.0319\n-0.46\n0.6462\n0.2545\n\n\nTest: S20_r\n0.0097\n0.0298\n0.33\n0.7440\n0.1604\n\n\nTest: SLJ\n0.0223\n0.0284\n0.78\n0.4329\n0.0450\n\n\nTest: BMT\n-0.0229\n0.0306\n-0.75\n0.4545\n0.2113\n\n\nSex: Girls\n-0.3714\n0.0341\n-10.90\n&lt;1e-26\n0.4924\n\n\na1\n0.1716\n0.0583\n2.94\n0.0032\n0.7978\n\n\nTest: Star_r & Sex: Girls\n0.3109\n0.0581\n5.35\n&lt;1e-07\n\n\n\nTest: S20_r & Sex: Girls\n-0.0671\n0.0573\n-1.17\n0.2416\n\n\n\nTest: SLJ & Sex: Girls\n-0.0868\n0.0567\n-1.53\n0.1258\n\n\n\nTest: BMT & Sex: Girls\n-0.2879\n0.0572\n-5.03\n&lt;1e-06\n\n\n\nTest: Star_r & a1\n0.1207\n0.0995\n1.21\n0.2252\n\n\n\nTest: S20_r & a1\n0.0173\n0.0980\n0.18\n0.8595\n\n\n\nTest: SLJ & a1\n-0.1145\n0.0969\n-1.18\n0.2377\n\n\n\nTest: BMT & a1\n0.3478\n0.0977\n3.56\n0.0004\n\n\n\nSex: Girls & a1\n0.0907\n0.0808\n1.12\n0.2618\n\n\n\nTest: Star_r & Sex: Girls & a1\n0.1175\n0.1993\n0.59\n0.5556\n\n\n\nTest: S20_r & Sex: Girls & a1\n-0.0537\n0.1960\n-0.27\n0.7842\n\n\n\nTest: SLJ & Sex: Girls & a1\n-0.2489\n0.1939\n-1.28\n0.1993\n\n\n\nTest: BMT & Sex: Girls & a1\n0.2424\n0.1955\n1.24\n0.2149\n\n\n\nResidual\n0.8628\n\n\n\n\n\n\n\n\n\nWe also need to see the VCs and CPs of the random-effect structure (RES).\n\nVarCorr(m_cpx)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nSchool\n(Intercept)\n0.1013614\n0.3183731\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.0647725\n0.2545045\n+0.15\n\n\n\n\n\n\n\n\nTest: S20_r\n0.0257395\n0.1604353\n+0.03\n-0.37\n\n\n\n\n\n\n\nTest: SLJ\n0.0020213\n0.0449591\n+0.04\n-0.65\n-0.35\n\n\n\n\n\n\nTest: BMT\n0.0446358\n0.2112718\n-0.57\n-0.07\n-0.34\n+0.43\n\n\n\n\n\nSex: Girls\n0.2424276\n0.4923694\n-0.42\n-0.04\n-0.06\n+0.19\n+0.36\n\n\n\n\na1\n0.6364160\n0.7977569\n+0.18\n+0.01\n+0.09\n-0.02\n-0.14\n+0.01\n\n\nResidual\n\n0.7444391\n0.8628088\n\n\n\n\n\n\n\n\n\n\n\n\nissingular(m_cpx)\n\nfalse\n\n\nThe complex model may or may not be supported by the data."
  },
  {
    "objectID": "fggk21.html#model-comparisons",
    "href": "fggk21.html#model-comparisons",
    "title": "Basics with Emotikon Project",
    "section": "4.4 Model comparisons",
    "text": "4.4 Model comparisons\nThe checks of model singularity indicate that the three models are supported by the data. Does model complexification also increase the goodness of fit or are we only fitting noise?\n\n4.4.1 LRT and goodness-of-fit statistics\nAs the thee models are strictly hierarchically nested, we compare them with a likelihood-ratio tests (LRT) and AIC and BIC goodness-of-fit statistics derived from them.\n\nMixedModels.likelihoodratiotest(m_ovi, m_zcp, m_cpx)\n\n\n\n\n\nmodel-dof\n-2 logLik\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\nzScore ~ 1 + Test + Sex + a1 + Test & Sex + Test & a1 + Sex & a1 + Test & Sex & a1 + (1 | School)\n22\n-26190\n\n\n\n\n\nzScore ~ 1 + Test + Sex + a1 + Test & Sex + Test & a1 + Sex & a1 + Test & Sex & a1 + zerocorr(1 + Test + Sex + a1 | School)\n28\n-25862\n328\n6\n&lt;1e-67\n\n\nzScore ~ 1 + Test + Sex + a1 + Test & Sex + Test & a1 + Sex & a1 + Test & Sex & a1 + (1 + Test + Sex + a1 | School)\n49\n-25814\n48\n21\n0.0006\n\n\n\n\n\n\n\nCode\ngof_summary = let\n  nms = [:m_ovi, :m_zcp, :m_cpx]\n  mods = eval.(nms)\n  DataFrame(;\n    name=nms,\n    dof=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    AICc=aicc.(mods),\n    BIC=bic.(mods),\n  )\nend\n\n\n3√ó6 DataFrame\n\n\n\nRow\nname\ndof\ndeviance\nAIC\nAICc\nBIC\n\n\n\nSymbol\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nm_ovi\n22\n26189.8\n26233.8\n26234.0\n26391.6\n\n\n2\nm_zcp\n28\n25862.2\n25918.2\n25918.4\n26119.0\n\n\n3\nm_cpx\n49\n25813.8\n25911.8\n25912.4\n26263.3\n\n\n\n\n\n\nThese statistics will depend on sampling. In general, smaller deviance, AIC, and BIC indicate an improvement in goodness of fit. Usually, œá¬≤ should be larger than the associated degrees of freedom; for AIC and BIC the decrease should amount to more than 5, according to some literature. Severity of meeting these criteria increases from deviance to AIC to BIC. Therefore, it is not always the case that the criteria are unanimous in their verdict. Basically, the more confirmatory the analysis, the more one may go with deviance and AIC; for exploratory analyses the BIC is probably a better guide. There are grey zones here.\n\n\n4.4.2 Comparing fixed effects of m_ovi, m_zcp, and m_cpx\nWe check whether enriching the RES changed the significance of fixed effects in the final model.\n\n\nCode\nm_ovi_fe = DataFrame(coeftable(m_ovi));\nm_zcp_fe = DataFrame(coeftable(m_zcp));\nm_cpx_fe = DataFrame(coeftable(m_cpx));\nm_all = hcat(\n  m_ovi_fe[:, [1, 2, 4]],\n  leftjoin(\n    m_zcp_fe[:, [1, 2, 4]],\n    m_cpx_fe[:, [1, 2, 4]];\n    on=:Name,\n    makeunique=true,\n  );\n  makeunique=true,\n)\nrename!(\n  m_all,\n  \"Coef.\" =&gt; \"b_ovi\",\n  \"Coef._2\" =&gt; \"b_zcp\",\n  \"Coef._1\" =&gt; \"b_cpx\",\n  \"z\" =&gt; \"z_ovi\",\n  \"z_2\" =&gt; \"z_zcp\",\n  \"z_1\" =&gt; \"z_cpx\",\n)\nm_all2 =\n  round.(\n    m_all[:, [:b_ovi, :b_zcp, :b_cpx, :z_ovi, :z_zcp, :z_cpx]],\n    digits=2,\n  )\nm_all3 = hcat(m_all.Name, m_all2)\n\n\n20√ó7 DataFrame\n\n\n\nRow\nx1\nb_ovi\nb_zcp\nb_cpx\nz_ovi\nz_zcp\nz_cpx\n\n\n\nString\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n(Intercept)\n-0.01\n0.0\n-0.0\n-0.4\n0.01\n-0.1\n\n\n2\nTest: Star_r\n-0.02\n-0.01\n-0.01\n-0.68\n-0.44\n-0.46\n\n\n3\nTest: S20_r\n0.01\n0.01\n0.01\n0.32\n0.39\n0.33\n\n\n4\nTest: SLJ\n0.02\n0.02\n0.02\n0.81\n0.84\n0.78\n\n\n5\nTest: BMT\n-0.04\n-0.03\n-0.02\n-1.21\n-0.92\n-0.75\n\n\n6\nSex: Girls\n-0.38\n-0.38\n-0.37\n-18.49\n-11.17\n-10.9\n\n\n7\na1\n0.19\n0.19\n0.17\n5.32\n3.13\n2.94\n\n\n8\nTest: Star_r & Sex: Girls\n0.3\n0.31\n0.31\n5.02\n5.37\n5.35\n\n\n9\nTest: S20_r & Sex: Girls\n-0.07\n-0.07\n-0.07\n-1.12\n-1.2\n-1.17\n\n\n10\nTest: SLJ & Sex: Girls\n-0.09\n-0.09\n-0.09\n-1.49\n-1.53\n-1.53\n\n\n11\nTest: BMT & Sex: Girls\n-0.28\n-0.29\n-0.29\n-4.67\n-4.98\n-5.03\n\n\n12\nTest: Star_r & a1\n0.12\n0.11\n0.12\n1.19\n1.15\n1.21\n\n\n13\nTest: S20_r & a1\n0.01\n0.02\n0.02\n0.11\n0.18\n0.18\n\n\n14\nTest: SLJ & a1\n-0.11\n-0.11\n-0.11\n-1.1\n-1.15\n-1.18\n\n\n15\nTest: BMT & a1\n0.35\n0.35\n0.35\n3.45\n3.53\n3.56\n\n\n16\nSex: Girls & a1\n0.06\n0.1\n0.09\n0.87\n1.21\n1.12\n\n\n17\nTest: Star_r & Sex: Girls & a1\n0.18\n0.11\n0.12\n0.85\n0.56\n0.59\n\n\n18\nTest: S20_r & Sex: Girls & a1\n-0.05\n-0.05\n-0.05\n-0.22\n-0.26\n-0.27\n\n\n19\nTest: SLJ & Sex: Girls & a1\n-0.26\n-0.26\n-0.25\n-1.26\n-1.34\n-1.28\n\n\n20\nTest: BMT & Sex: Girls & a1\n0.26\n0.26\n0.24\n1.29\n1.32\n1.24\n\n\n\n\n\n\nThe three models usually do not differ in fixed-effect estimates. For main effects of age and Sex, z-values decrease strongly with the complexity of the model (i.e., standard errors are larger). For other coefficients, the changes are not very large and not consistent.\nIn general, dropping significant variance components and/or correlation parameters may lead to anti-conservative estimates of fixed effects (e.g., Schielzeth & Forstmeier, 2008). Basically, some of the variance allocated to age and Sex in LMM m_ovi could also be due to differences between schools. This ambiguity increased the uncertainty of the respective fixed effects in the other two LMMs."
  },
  {
    "objectID": "fggk21.html#fitting-an-overparameterized-lmm",
    "href": "fggk21.html#fitting-an-overparameterized-lmm",
    "title": "Basics with Emotikon Project",
    "section": "4.5 Fitting an overparameterized LMM",
    "text": "4.5 Fitting an overparameterized LMM\nThe complex LMM was not overparameterized with respect to School, because there are over 400 schools in the data. When the number of units (levels) of a grouping factor is small relative to the number of parameters we are trying to estimate, we often end up with an overparameterized / degenerate random-effect structure.\nAs an illustration, we fit a full CP matrix for the Cohort. As there are only nine cohorts in the data, we may be asking too much to estimate 5*6/2 = 15 VC/CP parameters.\n\nm_cpxCohort = let\n  f = @formula zScore ~ 1 + Test * a1 * Sex + (1 + Test | Cohort)\n  fit(MixedModel, f, dat; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Cohort\n\n\n(Intercept)\n0.0108\n0.0150\n0.72\n0.4710\n0.0333\n\n\nTest: Star_r\n-0.0215\n0.0383\n-0.56\n0.5738\n0.0627\n\n\nTest: S20_r\n0.0060\n0.0385\n0.16\n0.8766\n0.0647\n\n\nTest: SLJ\n0.0293\n0.0354\n0.83\n0.4072\n0.0467\n\n\nTest: BMT\n-0.0399\n0.0338\n-1.18\n0.2378\n0.0345\n\n\na1\n0.1910\n0.0349\n5.47\n&lt;1e-07\n\n\n\nSex: Girls\n-0.3818\n0.0201\n-18.95\n&lt;1e-79\n\n\n\nTest: Star_r & a1\n0.1041\n0.1101\n0.95\n0.3445\n\n\n\nTest: S20_r & a1\n0.0154\n0.1093\n0.14\n0.8879\n\n\n\nTest: SLJ & a1\n-0.1028\n0.1089\n-0.94\n0.3451\n\n\n\nTest: BMT & a1\n0.3500\n0.1085\n3.22\n0.0013\n\n\n\nTest: Star_r & Sex: Girls\n0.3033\n0.0639\n4.74\n&lt;1e-05\n\n\n\nTest: S20_r & Sex: Girls\n-0.0678\n0.0636\n-1.07\n0.2866\n\n\n\nTest: SLJ & Sex: Girls\n-0.0899\n0.0635\n-1.42\n0.1568\n\n\n\nTest: BMT & Sex: Girls\n-0.2810\n0.0634\n-4.43\n&lt;1e-05\n\n\n\na1 & Sex: Girls\n-0.0287\n0.0688\n-0.42\n0.6767\n\n\n\nTest: Star_r & a1 & Sex: Girls\n0.1905\n0.2187\n0.87\n0.3837\n\n\n\nTest: S20_r & a1 & Sex: Girls\n-0.0557\n0.2175\n-0.26\n0.7979\n\n\n\nTest: SLJ & a1 & Sex: Girls\n-0.2549\n0.2169\n-1.17\n0.2400\n\n\n\nTest: BMT & a1 & Sex: Girls\n0.2671\n0.2166\n1.23\n0.2175\n\n\n\nResidual\n0.9669\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_cpxCohort)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\nCohort\n(Intercept)\n0.0011061\n0.0332581\n\n\n\n\n\n\n\nTest: Star_r\n0.0039348\n0.0627278\n+0.52\n\n\n\n\n\n\nTest: S20_r\n0.0041821\n0.0646691\n-1.00\n-0.50\n\n\n\n\n\nTest: SLJ\n0.0021834\n0.0467264\n+0.58\n-0.38\n-0.60\n\n\n\n\nTest: BMT\n0.0011913\n0.0345149\n+0.67\n+0.94\n-0.67\n-0.18\n\n\nResidual\n\n0.9349896\n0.9669486\n\n\n\n\n\n\n\n\n\n\nissingular(m_cpxCohort)\n\ntrue\n\n\nThe model is overparameterized with several CPs estimated between |.98| and |1.00|. How about the zero-correlation parameter (zcp) version of this LMM?\n\nm_zcpCohort = let\n  f = @formula(\n    zScore ~ 1 + Test * a1 * Sex + zerocorr(1 + Test | Cohort)\n  )\n  fit(MixedModel, f, dat; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Cohort\n\n\n(Intercept)\n0.0109\n0.0142\n0.76\n0.4461\n0.0300\n\n\nTest: Star_r\n-0.0184\n0.0330\n-0.56\n0.5771\n0.0245\n\n\nTest: S20_r\n0.0074\n0.0318\n0.23\n0.8171\n0.0000\n\n\nTest: SLJ\n0.0252\n0.0317\n0.80\n0.4266\n0.0000\n\n\nTest: BMT\n-0.0378\n0.0317\n-1.19\n0.2334\n0.0000\n\n\na1\n0.1904\n0.0350\n5.44\n&lt;1e-07\n\n\n\nSex: Girls\n-0.3821\n0.0202\n-18.95\n&lt;1e-79\n\n\n\nTest: Star_r & a1\n0.1203\n0.1095\n1.10\n0.2720\n\n\n\nTest: S20_r & a1\n0.0076\n0.1088\n0.07\n0.9443\n\n\n\nTest: SLJ & a1\n-0.1109\n0.1085\n-1.02\n0.3066\n\n\n\nTest: BMT & a1\n0.3599\n0.1084\n3.32\n0.0009\n\n\n\nTest: Star_r & Sex: Girls\n0.3035\n0.0639\n4.75\n&lt;1e-05\n\n\n\nTest: S20_r & Sex: Girls\n-0.0646\n0.0636\n-1.02\n0.3099\n\n\n\nTest: SLJ & Sex: Girls\n-0.0928\n0.0635\n-1.46\n0.1437\n\n\n\nTest: BMT & Sex: Girls\n-0.2811\n0.0635\n-4.43\n&lt;1e-05\n\n\n\na1 & Sex: Girls\n-0.0300\n0.0689\n-0.44\n0.6633\n\n\n\nTest: Star_r & a1 & Sex: Girls\n0.1896\n0.2187\n0.87\n0.3859\n\n\n\nTest: S20_r & a1 & Sex: Girls\n-0.0571\n0.2176\n-0.26\n0.7929\n\n\n\nTest: SLJ & a1 & Sex: Girls\n-0.2533\n0.2170\n-1.17\n0.2430\n\n\n\nTest: BMT & a1 & Sex: Girls\n0.2670\n0.2167\n1.23\n0.2180\n\n\n\nResidual\n0.9675\n\n\n\n\n\n\n\n\n\n\nissingular(m_zcpCohort)\n\ntrue\n\n\nThis zcpLMM is also singular. Three of the five VCs are estimated as zero. This raises the possibility that LMM m_oviCohort might fit as well as LMM m_zcpCohort.\n\nm_oviCohort = let\n  f = @formula zScore ~ 1 + Test * a1 * Sex + (1 | Cohort)\n  fit(MixedModel, f, dat; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Cohort\n\n\n(Intercept)\n0.0109\n0.0143\n0.76\n0.4459\n0.0300\n\n\nTest: Star_r\n-0.0174\n0.0320\n-0.54\n0.5871\n\n\n\nTest: S20_r\n0.0074\n0.0318\n0.23\n0.8172\n\n\n\nTest: SLJ\n0.0252\n0.0317\n0.80\n0.4264\n\n\n\nTest: BMT\n-0.0378\n0.0317\n-1.19\n0.2333\n\n\n\na1\n0.1904\n0.0350\n5.44\n&lt;1e-07\n\n\n\nSex: Girls\n-0.3821\n0.0202\n-18.95\n&lt;1e-79\n\n\n\nTest: Star_r & a1\n0.1254\n0.1093\n1.15\n0.2515\n\n\n\nTest: S20_r & a1\n0.0076\n0.1088\n0.07\n0.9444\n\n\n\nTest: SLJ & a1\n-0.1109\n0.1085\n-1.02\n0.3065\n\n\n\nTest: BMT & a1\n0.3599\n0.1084\n3.32\n0.0009\n\n\n\nTest: Star_r & Sex: Girls\n0.3039\n0.0639\n4.75\n&lt;1e-05\n\n\n\nTest: S20_r & Sex: Girls\n-0.0646\n0.0636\n-1.02\n0.3099\n\n\n\nTest: SLJ & Sex: Girls\n-0.0928\n0.0635\n-1.46\n0.1438\n\n\n\nTest: BMT & Sex: Girls\n-0.2811\n0.0635\n-4.43\n&lt;1e-05\n\n\n\na1 & Sex: Girls\n-0.0300\n0.0689\n-0.43\n0.6636\n\n\n\nTest: Star_r & a1 & Sex: Girls\n0.1894\n0.2187\n0.87\n0.3864\n\n\n\nTest: S20_r & a1 & Sex: Girls\n-0.0571\n0.2176\n-0.26\n0.7930\n\n\n\nTest: SLJ & a1 & Sex: Girls\n-0.2534\n0.2170\n-1.17\n0.2430\n\n\n\nTest: BMT & a1 & Sex: Girls\n0.2669\n0.2167\n1.23\n0.2180\n\n\n\nResidual\n0.9675\n\n\n\n\n\n\n\n\n\n\nissingular(m_oviCohort)\n\nfalse\n\n\nThis solves the problem with singularity, but does LMM m_zcpCohort fit noise relative to the LMM m_oviCohort?\n\nMixedModels.likelihoodratiotest(m_oviCohort, m_zcpCohort)\n\n\n\n\n\nmodel-dof\n-2 logLik\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 | Cohort)\n22\n-26674\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + zerocorr(1 + Test | Cohort)\n26\n-26674\n0\n4\n0.9997\n\n\n\n\n\n\ngof_summary2 = let\n  mods = [m_oviCohort, m_zcpCohort, m_cpxCohort]\n  DataFrame(;\n    dof=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    AICc=aicc.(mods),\n    BIC=bic.(mods),\n  )\nend\n\n3√ó5 DataFrame\n\n\n\nRow\ndof\ndeviance\nAIC\nAICc\nBIC\n\n\n\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n22\n26674.0\n26718.0\n26718.1\n26875.8\n\n\n2\n26\n26674.0\n26726.0\n26726.1\n26912.4\n\n\n3\n36\n26669.6\n26741.6\n26741.9\n26999.8\n\n\n\n\n\n\nIndeed, adding VCs is fitting noise. Again, the goodness of fit statistics unanimously favor the selection of the LMM m_oviCohort.\nNot shown here, but the Cohort-related VCs for the Test contrasts could be estimated reliably for the full data. Thus, the small number of cohorts does not necessarily prevent the determination of reliable differences between tests across cohorts. What if we include VCs and CPs related to random factors Child and School?"
  },
  {
    "objectID": "fggk21.html#fitting-the-published-lmm-m1-to-the-reduced-data",
    "href": "fggk21.html#fitting-the-published-lmm-m1-to-the-reduced-data",
    "title": "Basics with Emotikon Project",
    "section": "4.6 Fitting the published LMM m1 to the reduced data",
    "text": "4.6 Fitting the published LMM m1 to the reduced data\n\n\n\n\n\n\nWarning\n\n\n\nThe following LMMs m1, m2, etc. take a bit longer (e.g., close to 6 minutes in the Pluto notebook, close to 3 minutes in the REPL on a MacBook Pro).\n\n\nLMM m1 reported in F√ºhner et al. (2021) included random factors for School, Child, and Cohort. The RES for School was specified like in LMM m_cpx. The RES for Child included VCs and CPs for Test, but not for linear developmental gain in the ninth year of life a1 or Sex; they are between-Child effects.\nThe RES for Cohort included only VCs, no CPs for Test. The parsimony was due to the small number of nine levels for this grouping factor.\nHere we fit this LMM m1 for the reduced data. For a different subset of similar size on MacBook Pro [13 | 15 | 16] this took [303 | 250 | 244 ] s; for LMM m1a (i.e., dropping 1 school-relate VC for Sex), times are [212 | 165 | 160] s. The corresponding lme4 times for LMM m1 are [397 | 348 | 195].\nFinally, times for fitting the full set of data ‚Äìnot in this script‚Äì, for LMM m1are [60 | 62 | 85] minutes (!); for LMM m1a the times were [46 | 48 | 34] minutes. It was not possible to fit the full set of data with lme4; after about 13 to 18 minutes the program stopped with: Error in eval_f(x, ...) : Downdated VtV is not positive definite.\n\nm1 = let\n  f = @formula(\n    zScore ~\n      1 +\n      Test * a1 * Sex +\n      (1 + Test + a1 + Sex | School) +\n      (1 + Test | Child) +\n      zerocorr(1 + Test | Cohort)\n  )\n  fit(MixedModel, f, dat; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\nœÉ_School\nœÉ_Cohort\n\n\n(Intercept)\n0.0005\n0.0198\n0.02\n0.9813\n0.5935\n0.2027\n0.0201\n\n\nTest: Star_r\n-0.0213\n0.0390\n-0.55\n0.5849\n0.5954\n0.3872\n0.0685\n\n\nTest: S20_r\n0.0191\n0.0323\n0.59\n0.5546\n0.5796\n0.3234\n0.0409\n\n\nTest: SLJ\n0.0161\n0.0260\n0.62\n0.5340\n0.4643\n0.2500\n0.0086\n\n\nTest: BMT\n-0.0187\n0.0309\n-0.60\n0.5455\n0.6520\n0.2901\n0.0313\n\n\na1\n0.1805\n0.0543\n3.32\n0.0009\n\n0.1663\n\n\n\nSex: Girls\n-0.3743\n0.0329\n-11.38\n&lt;1e-29\n\n0.2132\n\n\n\nTest: Star_r & a1\n0.1202\n0.0858\n1.40\n0.1613\n\n\n\n\n\nTest: S20_r & a1\n0.0061\n0.0832\n0.07\n0.9416\n\n\n\n\n\nTest: SLJ & a1\n-0.1208\n0.0767\n-1.58\n0.1152\n\n\n\n\n\nTest: BMT & a1\n0.3397\n0.0855\n3.97\n&lt;1e-04\n\n\n\n\n\nTest: Star_r & Sex: Girls\n0.3113\n0.0494\n6.30\n&lt;1e-09\n\n\n\n\n\nTest: S20_r & Sex: Girls\n-0.0634\n0.0482\n-1.31\n0.1890\n\n\n\n\n\nTest: SLJ & Sex: Girls\n-0.0892\n0.0447\n-1.99\n0.0463\n\n\n\n\n\nTest: BMT & Sex: Girls\n-0.2890\n0.0498\n-5.81\n&lt;1e-08\n\n\n\n\n\na1 & Sex: Girls\n0.0233\n0.1066\n0.22\n0.8269\n\n\n\n\n\nTest: Star_r & a1 & Sex: Girls\n0.0481\n0.1700\n0.28\n0.7772\n\n\n\n\n\nTest: S20_r & a1 & Sex: Girls\n-0.0716\n0.1656\n-0.43\n0.6656\n\n\n\n\n\nTest: SLJ & a1 & Sex: Girls\n-0.2182\n0.1535\n-1.42\n0.1551\n\n\n\n\n\nTest: BMT & a1 & Sex: Girls\n0.2560\n0.1705\n1.50\n0.1333\n\n\n\n\n\nResidual\n0.5755\n\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m1)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.35227800\n0.59353011\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.35445882\n0.59536445\n+0.05\n\n\n\n\n\n\n\n\nTest: S20_r\n0.33590975\n0.57957722\n+0.12\n-0.49\n\n\n\n\n\n\n\nTest: SLJ\n0.21561932\n0.46434828\n+0.06\n-0.03\n-0.39\n\n\n\n\n\n\nTest: BMT\n0.42508792\n0.65198767\n-0.34\n+0.20\n-0.20\n-0.25\n\n\n\n\nSchool\n(Intercept)\n0.04110450\n0.20274244\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.14992396\n0.38720016\n+0.16\n\n\n\n\n\n\n\n\nTest: S20_r\n0.10460074\n0.32342038\n-0.20\n-0.48\n\n\n\n\n\n\n\nTest: SLJ\n0.06249624\n0.24999249\n-0.03\n-0.14\n-0.46\n\n\n\n\n\n\nTest: BMT\n0.08413899\n0.29006721\n-0.14\n-0.09\n-0.11\n-0.28\n\n\n\n\n\na1\n0.02767101\n0.16634607\n+0.81\n-0.08\n+0.39\n-0.19\n-0.19\n\n\n\n\nSex: Girls\n0.04546040\n0.21321444\n-0.97\n-0.18\n+0.09\n+0.23\n+0.18\n-0.81\n\n\nCohort\n(Intercept)\n0.00040452\n0.02011268\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.00469025\n0.06848541\n.\n\n\n\n\n\n\n\n\nTest: S20_r\n0.00167528\n0.04093017\n.\n.\n\n\n\n\n\n\n\nTest: SLJ\n0.00007345\n0.00857052\n.\n.\n.\n\n\n\n\n\n\nTest: BMT\n0.00098160\n0.03133056\n.\n.\n.\n.\n\n\n\n\nResidual\n\n0.33123917\n0.57553381\n\n\n\n\n\n\n\n\n\n\n\n\nissingular(m1)\n\ntrue\n\n\nDepending on the random number for stratified samplign, LMM m1 may or may not be supported by the data.\nWe also fit an alternative parameterization, estimating VCs and CPs for Test scores rather than Test effects by replacing the 1 + ... in the RE terms with 0 + ....\n\nm2 = let\n  f = @formula(\n    zScore ~\n      1 +\n      Test * a1 * Sex +\n      (0 + Test + a1 + Sex | School) +\n      (0 + Test | Child) +\n      zerocorr(0 + Test | Cohort)\n  )\n  fit(MixedModel, f, dat; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\nœÉ_School\nœÉ_Cohort\n\n\n(Intercept)\n0.0000\n0.0199\n0.00\n0.9985\n\n\n\n\n\nTest: Star_r\n-0.0223\n0.0424\n-0.53\n0.5990\n0.7345\n0.3453\n0.0681\n\n\nTest: S20_r\n0.0188\n0.0401\n0.47\n0.6392\n0.7795\n0.2831\n0.0450\n\n\nTest: SLJ\n0.0212\n0.0315\n0.67\n0.5003\n0.7897\n0.2497\n0.0289\n\n\nTest: BMT\n-0.0205\n0.0321\n-0.64\n0.5233\n0.7046\n0.2450\n0.0289\n\n\na1\n0.1802\n0.0542\n3.32\n0.0009\n\n0.1655\n\n\n\nSex: Girls\n-0.3743\n0.0329\n-11.38\n&lt;1e-29\n\n0.2131\n\n\n\nTest: Star_r & a1\n0.1185\n0.0861\n1.38\n0.1687\n\n\n\n\n\nTest: S20_r & a1\n0.0111\n0.0839\n0.13\n0.8943\n\n\n\n\n\nTest: SLJ & a1\n-0.1095\n0.0774\n-1.41\n0.1572\n\n\n\n\n\nTest: BMT & a1\n0.3384\n0.0857\n3.95\n&lt;1e-04\n\n\n\n\n\nTest: Star_r & Sex: Girls\n0.3117\n0.0494\n6.31\n&lt;1e-09\n\n\n\n\n\nTest: S20_r & Sex: Girls\n-0.0653\n0.0482\n-1.35\n0.1757\n\n\n\n\n\nTest: SLJ & Sex: Girls\n-0.0864\n0.0447\n-1.93\n0.0535\n\n\n\n\n\nTest: BMT & Sex: Girls\n-0.2900\n0.0498\n-5.83\n&lt;1e-08\n\n\n\n\n\na1 & Sex: Girls\n0.0237\n0.1066\n0.22\n0.8244\n\n\n\n\n\nTest: Star_r & a1 & Sex: Girls\n0.0510\n0.1699\n0.30\n0.7640\n\n\n\n\n\nTest: S20_r & a1 & Sex: Girls\n-0.0757\n0.1655\n-0.46\n0.6472\n\n\n\n\n\nTest: SLJ & a1 & Sex: Girls\n-0.2174\n0.1534\n-1.42\n0.1563\n\n\n\n\n\nTest: BMT & a1 & Sex: Girls\n0.2571\n0.1705\n1.51\n0.1315\n\n\n\n\n\nTest: Run\n\n\n\n\n0.7369\n0.3070\n0.0494\n\n\nResidual\n0.5275\n\n\n\n\n\n\n\n\n\n\n\n\nissingular(m2)\n\ntrue\n\n\nDepending on the random number generator seed, the model may or may not be supported in the alternative parameterization of scores. The fixed-effects profile is not affected (see 2.8 below).\n\n\n\n\n\n\nCaution\n\n\n\nRK: The order of RE terms is critical. In formula f2 the zerocorr() term must be placed last as shown. If it is placed first, School-related and Child-related CPs are estimated/reported (?) as zero. This was not the case for formula m1. Thus, it appears to be related to the 0-intercepts in School and Child terms. Need a reprex.\n\n\n\nVarCorr(m2)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\nTest: Run\n0.5430797\n0.7369394\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.5395289\n0.7345263\n+0.58\n\n\n\n\n\n\n\n\nTest: S20_r\n0.6076051\n0.7794903\n+0.60\n+0.62\n\n\n\n\n\n\n\nTest: SLJ\n0.6235756\n0.7896680\n+0.62\n+0.62\n+0.74\n\n\n\n\n\n\nTest: BMT\n0.4963980\n0.7045552\n+0.30\n+0.45\n+0.46\n+0.53\n\n\n\n\nSchool\nTest: Run\n0.0942444\n0.3069926\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.1192138\n0.3452735\n+0.29\n\n\n\n\n\n\n\n\nTest: S20_r\n0.0801661\n0.2831362\n+0.24\n+0.47\n\n\n\n\n\n\n\nTest: SLJ\n0.0623265\n0.2496527\n+0.40\n+0.50\n+0.56\n\n\n\n\n\n\nTest: BMT\n0.0600015\n0.2449521\n+0.38\n+0.36\n+0.24\n+0.31\n\n\n\n\n\na1\n0.0273810\n0.1654720\n+0.46\n+0.34\n+0.84\n+0.76\n+0.56\n\n\n\n\nSex: Girls\n0.0454275\n0.2131373\n-0.63\n-0.76\n-0.83\n-0.70\n-0.50\n-0.82\n\n\nCohort\nTest: Run\n0.0024427\n0.0494241\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.0046398\n0.0681159\n.\n\n\n\n\n\n\n\n\nTest: S20_r\n0.0020206\n0.0449509\n.\n.\n\n\n\n\n\n\n\nTest: SLJ\n0.0008351\n0.0288986\n.\n.\n.\n\n\n\n\n\n\nTest: BMT\n0.0008347\n0.0288920\n.\n.\n.\n.\n\n\n\n\nResidual\n\n0.2782288\n0.5274740"
  },
  {
    "objectID": "fggk21.html#principle-component-analysis-of-random-effect-structure-repca",
    "href": "fggk21.html#principle-component-analysis-of-random-effect-structure-repca",
    "title": "Basics with Emotikon Project",
    "section": "4.7 Principle Component Analysis of Random Effect Structure (rePCA)",
    "text": "4.7 Principle Component Analysis of Random Effect Structure (rePCA)\nThe √¨ssingular() command is sort of a shortcut for a quick inspection of the principle components (PCs) of the variance-covariance matrix of the RES. With the MixedModels.PCA() command, we also obtain information about the amount of cumulative variance accounted for as we add PCs.\nThe output also provides PC loadings which may facilitate interpretation of the CP matrices (if estimated). This topic will be picked uo in a separate vignette. See also F√ºhner et al. (2021) for an application."
  },
  {
    "objectID": "fggk21.html#effects-in-res",
    "href": "fggk21.html#effects-in-res",
    "title": "Basics with Emotikon Project",
    "section": "4.8 Effects in RES",
    "text": "4.8 Effects in RES\nFor every random factor, MixedModels.PCA() extracts as many PCs as there are VCs. Therefore, the cumulation of variance across PCs within a random factor will always add up to 100% ‚Äì at the latest with the last VC, but, in the case of overparameterized LMMs, the ceiling will be reached earlier. The final PCs are usually quite small.\nPCs are extracted in the order of the amount of unique variance they account for. The first PC accounts for the largest and the final PC for the least amount of variance. The number the PCs with percent variance above a certain threshold indicates the number of weighted composites needed and reflects the dimensionality of the orthogonal space within which (almost) all the variance can be accounted for. The weights for forming composite scores are the listed loadings. For ease of interpretation it is often useful to change the sign of some composite scores.\nThe PCA for LMM m1 shows that each of the five PCs for Child accounts for a non-zero percent of unique variance.\nFor School fewer than seven PCs have unique variance. The exact number depends on sampling. The overparameterization of School might be resolved when the CPs for Sex are dropped from the LMM.\nCohort was estimated with CPs forced to zero. Therefore, the VCs were forced to be orthogonal; they already represent the PCA solution. However, depending on sampling, not all PCs may be identified for this random factor either.\nImportantly, again depending on sampling, a non-singular fit does not imply that unique variance is associated with all PCs (i.e., not for last PC for School). Embrace uncertainty!\n\nMixedModels.PCA(m1)\n\n(Child = \nPrincipal components based on correlation matrix\n (Intercept)    1.0     .      .      .      .\n Test: Star_r   0.05   1.0     .      .      .\n Test: S20_r    0.12  -0.49   1.0     .      .\n Test: SLJ      0.06  -0.03  -0.39   1.0     .\n Test: BMT     -0.34   0.2   -0.2   -0.25   1.0\n\nNormalized cumulative variances:\n[0.3394, 0.6247, 0.8283, 0.9369, 1.0]\n\nComponent loadings\n                 PC1    PC2    PC3    PC4    PC5\n (Intercept)   -0.27  -0.44   0.66  -0.54  -0.1\n Test: Star_r   0.55  -0.08   0.56   0.39   0.47\n Test: S20_r   -0.65   0.3    0.08   0.04   0.7\n Test: SLJ      0.17  -0.64  -0.5   -0.26   0.5\n Test: BMT      0.42   0.55  -0.01  -0.7    0.19, School = \nPrincipal components based on correlation matrix\n (Intercept)    1.0     .      .      .      .      .      .\n Test: Star_r   0.16   1.0     .      .      .      .      .\n Test: S20_r   -0.2   -0.48   1.0     .      .      .      .\n Test: SLJ     -0.03  -0.14  -0.46   1.0     .      .      .\n Test: BMT     -0.14  -0.09  -0.11  -0.28   1.0     .      .\n a1             0.81  -0.08   0.39  -0.19  -0.19   1.0     .\n Sex: Girls    -0.97  -0.18   0.09   0.23   0.18  -0.81   1.0\n\nNormalized cumulative variances:\n[0.4037, 0.6536, 0.8347, 0.9628, 1.0, 1.0, 1.0]\n\nComponent loadings\n                 PC1    PC2    PC3    PC4    PC5    PC6    PC7\n (Intercept)   -0.56   0.18  -0.01   0.22   0.07  -0.51   0.58\n Test: Star_r  -0.09   0.48   0.43  -0.56  -0.49   0.09   0.13\n Test: S20_r   -0.05  -0.71  -0.12  -0.25  -0.34   0.22   0.5\n Test: SLJ      0.14   0.41  -0.64   0.32  -0.45   0.27   0.17\n Test: BMT      0.14  -0.1    0.62   0.68  -0.32   0.13   0.08\n a1            -0.54  -0.22  -0.12   0.07  -0.49  -0.19  -0.6\n Sex: Girls     0.58  -0.09  -0.07  -0.07  -0.3   -0.74  -0.0, Cohort = \nPrincipal components based on correlation matrix\n (Intercept)   1.0  .    .    .    .\n Test: Star_r  0.0  1.0  .    .    .\n Test: S20_r   0.0  0.0  1.0  .    .\n Test: SLJ     0.0  0.0  0.0  1.0  .\n Test: BMT     0.0  0.0  0.0  0.0  1.0\n\nNormalized cumulative variances:\n[0.2, 0.4, 0.6, 0.8, 1.0]\n\nComponent loadings\n                PC1   PC2   PC3   PC4   PC5\n (Intercept)   1.0   0.0   0.0   0.0   0.0\n Test: Star_r  0.0   1.0   0.0   0.0   0.0\n Test: S20_r   0.0   0.0   1.0   0.0   0.0\n Test: SLJ     0.0   0.0   0.0   1.0   0.0\n Test: BMT     0.0   0.0   0.0   0.0   1.0)\n\n\n\n4.8.1 Scores in RES\nNow lets looks at the PCA results for the alternative parameterization of LMM m2. It is important to note that the reparameterization to base estimates of VCs and CPs on scores rather than effects applies only to the Test factor (i.e., the first factor in the formula); VCs for Sex and age refer to the associated effects.\nDepending on sampling, the difference between LMM m1 and LMM m2 may show that overparameterization according to PCs may depend on the specification chosen for the other the random-effect structure.\n\n\n\n\n\n\nNote\n\n\n\nFor the complete data, all PCs had unique variance associated with them.\n\n\n\nMixedModels.PCA(m2)\n\n(Child = \nPrincipal components based on correlation matrix\n Test: Run     1.0    .     .     .     .\n Test: Star_r  0.58  1.0    .     .     .\n Test: S20_r   0.6   0.62  1.0    .     .\n Test: SLJ     0.62  0.62  0.74  1.0    .\n Test: BMT     0.3   0.45  0.46  0.53  1.0\n\nNormalized cumulative variances:\n[0.6476, 0.7909, 0.8745, 0.9499, 1.0]\n\nComponent loadings\n                 PC1    PC2    PC3    PC4    PC5\n Test: Run     -0.43   0.52   0.04  -0.73  -0.12\n Test: Star_r  -0.45   0.09  -0.84   0.28   0.06\n Test: S20_r   -0.48   0.1    0.39   0.48  -0.61\n Test: SLJ     -0.49  -0.01   0.38   0.18   0.76\n Test: BMT     -0.37  -0.84  -0.02  -0.36  -0.15, School = \nPrincipal components based on correlation matrix\n Test: Run      1.0     .      .      .      .      .      .\n Test: Star_r   0.29   1.0     .      .      .      .      .\n Test: S20_r    0.24   0.47   1.0     .      .      .      .\n Test: SLJ      0.4    0.5    0.56   1.0     .      .      .\n Test: BMT      0.38   0.36   0.24   0.31   1.0     .      .\n a1             0.46   0.34   0.84   0.76   0.56   1.0     .\n Sex: Girls    -0.63  -0.76  -0.83  -0.7   -0.5   -0.82   1.0\n\nNormalized cumulative variances:\n[0.6069, 0.7374, 0.843, 0.9369, 1.0, 1.0, 1.0]\n\nComponent loadings\n                 PC1    PC2    PC3    PC4    PC5    PC6    PC7\n Test: Run     -0.29   0.58  -0.09   0.68  -0.19  -0.26   0.08\n Test: Star_r  -0.33  -0.05   0.84  -0.08   0.04  -0.33  -0.24\n Test: S20_r   -0.39  -0.49  -0.16  -0.08  -0.46  -0.33   0.49\n Test: SLJ     -0.39  -0.18  -0.1    0.17   0.83   0.0    0.29\n Test: BMT     -0.29   0.61  -0.08  -0.69   0.04  -0.03   0.26\n a1            -0.44  -0.11  -0.46  -0.13  -0.0   -0.15  -0.74\n Sex: Girls     0.47   0.03  -0.16  -0.1    0.23  -0.83   0.0, Cohort = \nPrincipal components based on correlation matrix\n Test: Run     1.0  .    .    .    .\n Test: Star_r  0.0  1.0  .    .    .\n Test: S20_r   0.0  0.0  1.0  .    .\n Test: SLJ     0.0  0.0  0.0  1.0  .\n Test: BMT     0.0  0.0  0.0  0.0  1.0\n\nNormalized cumulative variances:\n[0.2, 0.4, 0.6, 0.8, 1.0]\n\nComponent loadings\n                PC1   PC2   PC3   PC4   PC5\n Test: Run     1.0   0.0   0.0   0.0   0.0\n Test: Star_r  0.0   1.0   0.0   0.0   0.0\n Test: S20_r   0.0   0.0   0.0   0.0   1.0\n Test: SLJ     0.0   0.0   1.0   0.0   0.0\n Test: BMT     0.0   0.0   0.0   1.0   0.0)"
  },
  {
    "objectID": "fggk21.html#summary-of-results-for-stratified-subset-of-data",
    "href": "fggk21.html#summary-of-results-for-stratified-subset-of-data",
    "title": "Basics with Emotikon Project",
    "section": "4.9 Summary of results for stratified subset of data",
    "text": "4.9 Summary of results for stratified subset of data\nReturning to the theoretical focus of the article, the significant main effects of age and Sex, the interactions between age and c1 and c4 contrasts and the interactions between Sex and three test contrasts (c1, c2, c4) are replicated. Obviously, the subset of data is much noisier than the full set."
  },
  {
    "objectID": "fggk21.html#overall-summary-statistics",
    "href": "fggk21.html#overall-summary-statistics",
    "title": "Basics with Emotikon Project",
    "section": "6.1 Overall summary statistics",
    "text": "6.1 Overall summary statistics\n+ julia&gt; m1.optsum         # MixedModels.OptSummary:  gets all info\n+ julia&gt; loglikelihood(m1) # StatsBase.loglikelihood: return loglikelihood\n                             of the model\n+ julia&gt; deviance(m1)      # StatsBase.deviance: negative twice the log-likelihood\n                             relative to saturated model\n+ julia&gt; objective(m1)     # MixedModels.objective: saturated model not clear:\n                             negative twice the log-likelihood\n+ julia&gt; nobs(m1)          # n of observations; they are not independent\n+ julia&gt; dof(m1)           # n of degrees of freedom is number of model parameters\n+ julia&gt; aic(m1)           # objective(m1) + 2*dof(m1)\n+ julia&gt; bic(m1)           # objective(m1) + dof(m1)*log(nobs(m1))\n\nm1.optsum            # MixedModels.OptSummary:  gets all info\n\n\n\n\n\n\n\n\nInitialization\n\n\n\nInitial parameter vector\n[1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n\n\nInitial objective value\n25653.67035388993\n\n\nOptimizer settings\n\n\n\nOptimizer\nLN_NEWUOA\n\n\nBackend\nnlopt\n\n\nftol_rel\n1.0e-12\n\n\nftol_abs\n1.0e-8\n\n\nxtol_rel\n0.0\n\n\nxtol_abs\n[1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10]\n\n\ninitial_step\n[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n\n\nmaxfeval\n-1\n\n\nmaxtime\n-1.0\n\n\nxtol_zero_abs\n0.001\n\n\nftol_zero_abs\n1.0e-5\n\n\nResult\n\n\n\nFunction evaluations\n1587\n\n\nFinal parameter vector\n[1.0313, 0.0512, 0.1243, 0.0453, -0.3849, 1.0332, -0.4987, -0.0281, 0.2501, 0.866, -0.3895, -0.0665, 0.7046, -0.3239, 0.9815, 0.3523, 0.1105, -0.1123, -0.0133, -0.0707, 0.2328, -0.3599, 0.6636, -0.2572, -0.0605, -0.0342, -0.0631, -0.0078, 0.4869, -0.2645, -0.0972, 0.15, -0.0503, 0.3389, -0.2655, 0.0436, 0.0565, 0.4098, 0.0307, 0.0437, 0.0, -0.0003, 0.0, 0.0349, 0.119, 0.0711, 0.0149, 0.0544]\n\n\nFinal objective value\n24467.6721\n\n\nReturn code\nFTOL_REACHED\n\n\n\n\n\n\nloglikelihood(m1) # StatsBase.loglikelihood: return loglikelihood of the model\n\n-12233.83606486928\n\n\n\ndeviance(m1)      # StatsBase.deviance: negative twice the log-likelihood relative to saturated mode`\n\n24467.67212973856\n\n\n\nobjective(m1)    # MixedModels.objective: saturated model not clear: negative twice the log-likelihood\n\n24467.67212973856\n\n\n\nnobs(m1) # n of observations; they are not independent\n\n9621\n\n\n\nn_, p_, q_, k_ = size(m1)\n\n(9621, 20, 13072, 3)\n\n\n\ndof(m1)  # n of degrees of freedom is number of model parameters\n\n69\n\n\n\ngeom_df = sum(leverage(m1)) # trace of hat / rank of model matrix / geom dof\n\n4680.6256904278625\n\n\n\nresid_df = nobs(m1) - geom_df  # eff. residual degrees of freedom\n\n4940.3743095721375\n\n\n\naic(m1)  # objective(m1) + 2*dof(m1)\n\n24605.67212973856\n\n\n\nbic(m1)  # objective(m1) + dof(m1)*log(nobs(m1))\n\n25100.519670435486"
  },
  {
    "objectID": "fggk21.html#fixed-effect-statistics",
    "href": "fggk21.html#fixed-effect-statistics",
    "title": "Basics with Emotikon Project",
    "section": "6.2 Fixed-effect statistics",
    "text": "6.2 Fixed-effect statistics\n+ julia&gt; coeftable(m1)     # StatsBase.coeftable: fixed-effects statiscs;\n                             default level=0.95\n+ julia&gt; Arrow.write(\"./data/m_cpx_fe.arrow\", DataFrame(coeftable(m1)));\n+ julia&gt; coef(m1)          # StatsBase.coef - parts of the table\n+ julia&gt; fixef(m1)         # MixedModels.fixef: not the same as coef()\n                             for rank-deficient case\n+ julia&gt; m1.beta           # alternative extractor\n+ julia&gt; fixefnames(m1)    # works also for coefnames(m1)\n+ julia&gt; vcov(m1)          # StatsBase.vcov: var-cov matrix of fixed-effects coef.\n+ julia&gt; stderror(m1)      # StatsBase.stderror: SE for fixed-effects coefficients\n+ julia&gt; propertynames(m1) # names of available extractors\n\ncoeftable(m1) # StatsBase.coeftable: fixed-effects statiscs; default level=0.95\n\n\n\n\n\n\n\n\n\n\n\n\nCoef.\nStd. Error\nz\nPr(&gt;\n\n\n\n\n(Intercept)\n0.000465304\n0.0198409\n0.02\n0.9813\n\n\nTest: Star_r\n-0.0213273\n0.0390442\n-0.55\n0.5849\n\n\nTest: S20_r\n0.0190929\n0.0323132\n0.59\n0.5546\n\n\nTest: SLJ\n0.016146\n0.0259607\n0.62\n0.5340\n\n\nTest: BMT\n-0.0186595\n0.0308638\n-0.60\n0.5455\n\n\na1\n0.180455\n0.0542781\n3.32\n0.0009\n\n\nSex: Girls\n-0.374298\n0.0328958\n-11.38\n&lt;1e-29\n\n\nTest: Star_r & a1\n0.120156\n0.0857869\n1.40\n0.1613\n\n\nTest: S20_r & a1\n0.00609509\n0.0831569\n0.07\n0.9416\n\n\nTest: SLJ & a1\n-0.120753\n0.0766581\n-1.58\n0.1152\n\n\nTest: BMT & a1\n0.339747\n0.0855172\n3.97\n&lt;1e-04\n\n\nTest: Star_r & Sex: Girls\n0.311303\n0.0493988\n6.30\n&lt;1e-09\n\n\nTest: S20_r & Sex: Girls\n-0.0633748\n0.0482497\n-1.31\n0.1890\n\n\nTest: SLJ & Sex: Girls\n-0.0891776\n0.0447464\n-1.99\n0.0463\n\n\nTest: BMT & Sex: Girls\n-0.289\n0.0497777\n-5.81\n&lt;1e-08\n\n\na1 & Sex: Girls\n0.0233155\n0.106646\n0.22\n0.8269\n\n\nTest: Star_r & a1 & Sex: Girls\n0.0481143\n0.169995\n0.28\n0.7772\n\n\nTest: S20_r & a1 & Sex: Girls\n-0.0715654\n0.165604\n-0.43\n0.6656\n\n\nTest: SLJ & a1 & Sex: Girls\n-0.218187\n0.153476\n-1.42\n0.1551\n\n\nTest: BMT & a1 & Sex: Girls\n0.255996\n0.170515\n1.50\n0.1333\n\n\n\n\n\n\n#Arrow.write(\"./data/m_cpx_fe.arrow\", DataFrame(coeftable(m1)));\n\n\ncoef(m1)              # StatsBase.coef; parts of the table\n\n20-element Vector{Float64}:\n  0.00046530425895310504\n -0.02132734944527947\n  0.019092899076631104\n  0.016145993028277343\n -0.0186594775046434\n  0.18045454153299173\n -0.37429786163205453\n  0.12015639592283825\n  0.006095086301361594\n -0.1207531863803185\n  0.33974667468981923\n  0.3113031640617854\n -0.0633748306370217\n -0.08917761409455151\n -0.2889999839503716\n  0.023315467818972813\n  0.04811427223187357\n -0.07156542243281178\n -0.21818694798397062\n  0.25599592504974333\n\n\n\nfixef(m1)    # MixedModels.fixef: not the same as coef() for rank-deficient case\n\n20-element Vector{Float64}:\n  0.00046530425895310504\n -0.02132734944527947\n  0.019092899076631104\n  0.016145993028277343\n -0.0186594775046434\n  0.18045454153299173\n -0.37429786163205453\n  0.12015639592283825\n  0.006095086301361594\n -0.1207531863803185\n  0.33974667468981923\n  0.3113031640617854\n -0.0633748306370217\n -0.08917761409455151\n -0.2889999839503716\n  0.023315467818972813\n  0.04811427223187357\n -0.07156542243281178\n -0.21818694798397062\n  0.25599592504974333\n\n\n\nm1.Œ≤                  # alternative extractor\n\n20-element Vector{Float64}:\n  0.00046530425895310504\n -0.02132734944527947\n  0.019092899076631104\n  0.016145993028277343\n -0.0186594775046434\n  0.18045454153299173\n -0.37429786163205453\n  0.12015639592283825\n  0.006095086301361594\n -0.1207531863803185\n  0.33974667468981923\n  0.3113031640617854\n -0.0633748306370217\n -0.08917761409455151\n -0.2889999839503716\n  0.023315467818972813\n  0.04811427223187357\n -0.07156542243281178\n -0.21818694798397062\n  0.25599592504974333\n\n\n\nfixefnames(m1)        # works also for coefnames(m1)\n\n20-element Vector{String}:\n \"(Intercept)\"\n \"Test: Star_r\"\n \"Test: S20_r\"\n \"Test: SLJ\"\n \"Test: BMT\"\n \"a1\"\n \"Sex: Girls\"\n \"Test: Star_r & a1\"\n \"Test: S20_r & a1\"\n \"Test: SLJ & a1\"\n \"Test: BMT & a1\"\n \"Test: Star_r & Sex: Girls\"\n \"Test: S20_r & Sex: Girls\"\n \"Test: SLJ & Sex: Girls\"\n \"Test: BMT & Sex: Girls\"\n \"a1 & Sex: Girls\"\n \"Test: Star_r & a1 & Sex: Girls\"\n \"Test: S20_r & a1 & Sex: Girls\"\n \"Test: SLJ & a1 & Sex: Girls\"\n \"Test: BMT & a1 & Sex: Girls\"\n\n\n\nvcov(m1)   # StatsBase.vcov: var-cov matrix of fixed-effects coefficients\n\n20√ó20 Matrix{Float64}:\n  0.000393662   4.46252e-5   -1.51224e-5   ‚Ä¶  -4.12743e-6   -1.85067e-5\n  4.46252e-5    0.00152445   -0.000453875     -1.94799e-6    7.35738e-6\n -1.51224e-5   -0.000453875   0.00104414      -5.94449e-5   -1.07865e-5\n  4.64558e-6   -4.25019e-5   -0.000347571      0.000108168  -4.50744e-5\n -9.40888e-5    1.60233e-5   -7.05322e-5      -4.47354e-5    0.000144068\n -7.23849e-5   -2.70234e-5    5.4519e-5    ‚Ä¶   4.27064e-5   -1.43779e-5\n -0.000145993  -4.41265e-5    1.72165e-5      -3.62865e-5    0.00018052\n -1.17407e-5   -0.000403606   0.00020028      -6.19795e-6    3.32412e-5\n -1.01666e-6    0.000199514  -0.000380394     -0.00033458   -4.06918e-5\n -9.74376e-6    6.10785e-6    0.000160753      0.000562624  -0.000208961\n  4.42106e-5   -2.57744e-5    2.95519e-5   ‚Ä¶  -0.000208581   0.000564765\n -7.1246e-6    -3.0567e-5     2.04268e-5       2.29863e-5   -0.000103515\n  3.03612e-6    2.04247e-5   -3.0706e-5        0.000655823   0.000123673\n  4.76297e-6    8.62193e-7    8.65298e-6      -0.00132596    0.000614758\n  5.91086e-6   -4.72786e-7    2.0174e-6        0.000616022  -0.00169325\n  6.03446e-5    1.31897e-5   -6.2552e-6    ‚Ä¶   0.000357987  -0.00338054\n  1.1554e-5     0.000165277  -9.46663e-5      -0.000389046   0.00187169\n -2.12962e-6   -9.4651e-5     0.000161573     -0.0118661    -0.00206647\n -4.12743e-6   -1.94799e-6   -5.94449e-5       0.023555     -0.0107439\n -1.85067e-5    7.35738e-6   -1.07865e-5      -0.0107439     0.0290755\n\n\n\nvcov(m1; corr=true) # StatsBase.vcov: correlation matrix of fixed-effects coefficients\n\n20√ó20 Matrix{Float64}:\n  1.0           0.0576053    -0.0235874   ‚Ä¶  -0.00135543   -0.0054702\n  0.0576053     1.0          -0.35975        -0.000325079   0.0011051\n -0.0235874    -0.35975       1.0            -0.0119865    -0.00195767\n  0.00901907   -0.0419311    -0.414332        0.0271482    -0.0101824\n -0.153648      0.0132968    -0.0707227      -0.00944411    0.027375\n -0.0672142    -0.0127514     0.0310845   ‚Ä¶   0.00512657   -0.00155349\n -0.223681     -0.034356      0.0161967      -0.00718725    0.0321826\n -0.00689783   -0.120498      0.0722497      -0.000470744   0.00227244\n -0.000616194   0.0614496    -0.141565       -0.0262156    -0.00286976\n -0.00640629    0.00204068    0.0648964       0.047821     -0.0159861\n  0.0260562    -0.00771932    0.0106943   ‚Ä¶  -0.015892      0.0387303\n -0.00726913   -0.0158482     0.0127969       0.00303188   -0.0122892\n  0.00317148    0.0108419    -0.0196947       0.0885626     0.015032\n  0.00536485    0.000493504   0.00598451     -0.193077      0.0805718\n  0.00598485   -0.000243262   0.00125423      0.0806344    -0.19949\n  0.0285188     0.00316761   -0.00181516  ‚Ä¶   0.0218716    -0.185899\n  0.00342559    0.0249013    -0.0172338      -0.0149116     0.0645706\n -0.00064814   -0.0146385     0.0301939      -0.466869     -0.0731803\n -0.00135543   -0.000325079  -0.0119865       1.0          -0.410542\n -0.0054702     0.0011051    -0.00195767     -0.410542      1.0\n\n\n\nstderror(m1)       # StatsBase.stderror: SE for fixed-effects coefficients\n\n20-element Vector{Float64}:\n 0.019840924188817957\n 0.03904417206594245\n 0.03231315016726891\n 0.025960679390621813\n 0.030863800842472117\n 0.05427813934076953\n 0.03289580142807546\n 0.08578693489571423\n 0.08315687074710586\n 0.07665812859145309\n 0.08551720566085513\n 0.04939876104482131\n 0.048249725655134655\n 0.044746390099495034\n 0.049777730904703034\n 0.10664627286129956\n 0.16999457607411267\n 0.16560414601368895\n 0.15347630428647727\n 0.17051529896959966\n\n\n\npropertynames(m1)  # names of available extractors\n\n(:formula, :reterms, :Xymat, :feterm, :sqrtwts, :parmap, :dims, :A, :L, :optsum, :Œ∏, :theta, :Œ≤, :beta, :Œ≤s, :betas, :Œª, :lambda, :stderror, :œÉ, :sigma, :œÉs, :sigmas, :œÉœÅs, :sigmarhos, :b, :u, :X, :y, :corr, :vcov, :PCA, :rePCA, :objective, :pvalues)"
  },
  {
    "objectID": "fggk21.html#covariance-parameter-estimates",
    "href": "fggk21.html#covariance-parameter-estimates",
    "title": "Basics with Emotikon Project",
    "section": "6.3 Covariance parameter estimates",
    "text": "6.3 Covariance parameter estimates\nThese commands inform us about the model parameters associated with the RES.\n+ julia&gt; issingular(m1)        # Test singularity for param. vector m1.theta\n+ julia&gt; VarCorr(m1)           # MixedModels.VarCorr: est. of RES\n+ julia&gt; propertynames(m1)\n+ julia&gt; m1.œÉ                  # residual; or: m1.sigma\n+ julia&gt; m1.œÉs                 # VCs; m1.sigmas\n+ julia&gt; m1.Œ∏                  # Parameter vector for RES (w/o residual); m1.theta\n+ julia&gt; MixedModels.sdest(m1) #  prsqrt(MixedModels.varest(m1))\n+ julia&gt; BlockDescription(m1)  #  Description of blocks of A and L in an LMM\n\nissingular(m1) # Test if model is singular for parameter vector m1.theta (default)\n\ntrue\n\n\n\nissingular(m2)\n\ntrue\n\n\n\nVarCorr(m1) # MixedModels.VarCorr: estimates of random-effect structure (RES)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.35227800\n0.59353011\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.35445882\n0.59536445\n+0.05\n\n\n\n\n\n\n\n\nTest: S20_r\n0.33590975\n0.57957722\n+0.12\n-0.49\n\n\n\n\n\n\n\nTest: SLJ\n0.21561932\n0.46434828\n+0.06\n-0.03\n-0.39\n\n\n\n\n\n\nTest: BMT\n0.42508792\n0.65198767\n-0.34\n+0.20\n-0.20\n-0.25\n\n\n\n\nSchool\n(Intercept)\n0.04110450\n0.20274244\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.14992396\n0.38720016\n+0.16\n\n\n\n\n\n\n\n\nTest: S20_r\n0.10460074\n0.32342038\n-0.20\n-0.48\n\n\n\n\n\n\n\nTest: SLJ\n0.06249624\n0.24999249\n-0.03\n-0.14\n-0.46\n\n\n\n\n\n\nTest: BMT\n0.08413899\n0.29006721\n-0.14\n-0.09\n-0.11\n-0.28\n\n\n\n\n\na1\n0.02767101\n0.16634607\n+0.81\n-0.08\n+0.39\n-0.19\n-0.19\n\n\n\n\nSex: Girls\n0.04546040\n0.21321444\n-0.97\n-0.18\n+0.09\n+0.23\n+0.18\n-0.81\n\n\nCohort\n(Intercept)\n0.00040452\n0.02011268\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.00469025\n0.06848541\n.\n\n\n\n\n\n\n\n\nTest: S20_r\n0.00167528\n0.04093017\n.\n.\n\n\n\n\n\n\n\nTest: SLJ\n0.00007345\n0.00857052\n.\n.\n.\n\n\n\n\n\n\nTest: BMT\n0.00098160\n0.03133056\n.\n.\n.\n.\n\n\n\n\nResidual\n\n0.33123917\n0.57553381\n\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m2)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\nTest: Run\n0.5430797\n0.7369394\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.5395289\n0.7345263\n+0.58\n\n\n\n\n\n\n\n\nTest: S20_r\n0.6076051\n0.7794903\n+0.60\n+0.62\n\n\n\n\n\n\n\nTest: SLJ\n0.6235756\n0.7896680\n+0.62\n+0.62\n+0.74\n\n\n\n\n\n\nTest: BMT\n0.4963980\n0.7045552\n+0.30\n+0.45\n+0.46\n+0.53\n\n\n\n\nSchool\nTest: Run\n0.0942444\n0.3069926\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.1192138\n0.3452735\n+0.29\n\n\n\n\n\n\n\n\nTest: S20_r\n0.0801661\n0.2831362\n+0.24\n+0.47\n\n\n\n\n\n\n\nTest: SLJ\n0.0623265\n0.2496527\n+0.40\n+0.50\n+0.56\n\n\n\n\n\n\nTest: BMT\n0.0600015\n0.2449521\n+0.38\n+0.36\n+0.24\n+0.31\n\n\n\n\n\na1\n0.0273810\n0.1654720\n+0.46\n+0.34\n+0.84\n+0.76\n+0.56\n\n\n\n\nSex: Girls\n0.0454275\n0.2131373\n-0.63\n-0.76\n-0.83\n-0.70\n-0.50\n-0.82\n\n\nCohort\nTest: Run\n0.0024427\n0.0494241\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.0046398\n0.0681159\n.\n\n\n\n\n\n\n\n\nTest: S20_r\n0.0020206\n0.0449509\n.\n.\n\n\n\n\n\n\n\nTest: SLJ\n0.0008351\n0.0288986\n.\n.\n.\n\n\n\n\n\n\nTest: BMT\n0.0008347\n0.0288920\n.\n.\n.\n.\n\n\n\n\nResidual\n\n0.2782288\n0.5274740\n\n\n\n\n\n\n\n\n\n\n\n\nm1.œÉs      # VCs; m1.sigmas\n\n(Child = (var\"(Intercept)\" = 0.5935301147462484, var\"Test: Star_r\" = 0.5953644468070622, var\"Test: S20_r\" = 0.5795772177999251, var\"Test: SLJ\" = 0.46434827660886474, var\"Test: BMT\" = 0.6519876666567095), School = (var\"(Intercept)\" = 0.20274244433314623, var\"Test: Star_r\" = 0.3872001591173718, var\"Test: S20_r\" = 0.32342037995885253, var\"Test: SLJ\" = 0.24999248690520504, var\"Test: BMT\" = 0.2900672146932753, a1 = 0.16634606912268482, var\"Sex: Girls\" = 0.21321443901251663), Cohort = (var\"(Intercept)\" = 0.02011268394449711, var\"Test: Star_r\" = 0.0684854120402093, var\"Test: S20_r\" = 0.04093017328069726, var\"Test: SLJ\" = 0.008570523763137093, var\"Test: BMT\" = 0.03133055603107415))\n\n\n\nm1.Œ∏       # Parameter vector for RES (w/o residual); m1.theta\n\n48-element Vector{Float64}:\n  1.0312688906382135\n  0.05115969834750499\n  0.12425987922187859\n  0.04527068874545405\n -0.3848714352048221\n  1.0331902318967632\n -0.49869764932784505\n -0.028094792185299065\n  0.2501080543108952\n  0.8660025830374609\n  ‚ãÆ\n  0.043656196641391434\n  0.0\n -0.00027385710181746284\n  0.0\n  0.034946137936179\n  0.1189945937786201\n  0.07111688749080665\n  0.014891434004452433\n  0.05443738566670338\n\n\n\nBlockDescription(m1) #  Description of blocks of A and L in a LinearMixedModel\n\n\n\n\nrows\nChild\nSchool\nCohort\nfixed\n\n\n9905\nBlkDiag\n\n\n\n\n\n3122\nSparse\nBlkDiag\n\n\n\n\n45\nDense\nDense\nBlkDiag/Dense\n\n\n\n21\nDense\nDense\nDense\nDense\n\n\n\n\n\n\nm2.Œ∏\n\n48-element Vector{Float64}:\n  1.3971103194150511\n  0.803356346571519\n  0.889381166617073\n  0.9261261154383268\n  0.40222756176832014\n  1.137441689829539\n  0.49276830305269104\n  0.4842451997824754\n  0.45625782297092576\n  1.0723865231356826\n  ‚ãÆ\n -0.03768070864112477\n  0.0\n -9.633114426763902e-6\n  0.0\n  0.09369968452648784\n  0.12913605128592756\n  0.08521909918535757\n  0.05478676671428523\n  0.054774179254079625\n\n\n\nBlockDescription(m2)\n\n\n\n\nrows\nChild\nSchool\nCohort\nfixed\n\n\n9905\nBlkDiag\n\n\n\n\n\n3122\nSparse\nBlkDiag\n\n\n\n\n45\nDense\nDense\nBlkDiag/Dense\n\n\n\n21\nDense\nDense\nDense\nDense"
  },
  {
    "objectID": "fggk21.html#model-predictions",
    "href": "fggk21.html#model-predictions",
    "title": "Basics with Emotikon Project",
    "section": "6.4 Model ‚Äúpredictions‚Äù",
    "text": "6.4 Model ‚Äúpredictions‚Äù\nThese commands inform us about extracion of conditional modes/means and (co-)variances, that using the model parameters to improve the predictions for units (levels) of the grouping (random) factors. We need this information, e.g., for partial-effect response profiles (e.g., facet plot) or effect profiles (e.g., caterpillar plot), or visualizing the borrowing-strength effect for correlation parameters (e.g., shrinkage plots). We are using the fit of LMM m2.\njulia&gt; condVar(m2)\nSome plotting functions are currently available from the MixedModelsMakie package or via custom functions.\n+ julia&gt; caterpillar!(m2)\n+ julia&gt; shrinkage!(m2)\n\n6.4.1 Conditional covariances\n\ncondVar(m1)\n\n3-element Vector{Array{Float64, 3}}:\n [0.07863055913344927 0.006257728936204379 ‚Ä¶ 0.002807244708397474 -0.021428987471224605; 0.006257728936204379 0.23665553388656713 ‚Ä¶ -0.007137841336072538 0.03387179518699708; ‚Ä¶ ; 0.002807244708397474 -0.007137841336072538 ‚Ä¶ 0.163957120906934 -0.0645980120693267; -0.021428987471224605 0.03387179518699708 ‚Ä¶ -0.0645980120693267 0.24054127866339528;;; 0.05688990623830173 0.004074204243496961 ‚Ä¶ 0.0035571011266920796 -0.013382648569179775; 0.004074204243496961 0.23466427691433892 ‚Ä¶ -0.006948104774146393 0.0342747802984983; ‚Ä¶ ; 0.0035571011266920796 -0.006948104774146393 ‚Ä¶ 0.16367113896405985 -0.06466416884241825; -0.013382648569179775 0.0342747802984983 ‚Ä¶ -0.06466416884241825 0.23672692976904974;;; 0.07975488593844791 0.0065194340842176105 ‚Ä¶ 0.002771066124870564 -0.02192192838393739; 0.0065194340842176105 0.23742825061386044 ‚Ä¶ -0.007209898076777342 0.03388736798981914; ‚Ä¶ ; 0.002771066124870564 -0.007209898076777342 ‚Ä¶ 0.16411272436336932 -0.06456274157657949; -0.02192192838393739 0.03388736798981914 ‚Ä¶ -0.06456274157657949 0.24119492201982043;;; ‚Ä¶ ;;; 0.07101878819405304 0.0035735936613861307 ‚Ä¶ 0.003263944083989713 -0.01895985689654255; 0.0035735936613861307 0.2371406303011173 ‚Ä¶ -0.0072984068542646 0.034632524487959046; ‚Ä¶ ; 0.003263944083989713 -0.0072984068542646 ‚Ä¶ 0.16417044224323987 -0.06464848883677196; -0.01895985689654255 0.034632524487959046 ‚Ä¶ -0.06464848883677196 0.2402619426009521;;; 0.08897154389886881 0.003258608198234842 ‚Ä¶ -0.021146483995604846 -0.030412412159318634; 0.003258608198234842 0.23717861399689127 ‚Ä¶ -0.006808774285077546 0.03485013432742127; ‚Ä¶ ; -0.021146483995604846 -0.006808774285077546 ‚Ä¶ 0.19835413971389218 -0.04886097468360213; -0.030412412159318634 0.03485013432742127 ‚Ä¶ -0.04886097468360213 0.24762754936126022;;; 0.06943996413788546 0.0028907027643269812 ‚Ä¶ 0.003353760095210695 -0.018309225068336522; 0.0028907027643269812 0.2365350726362062 ‚Ä¶ -0.0072432342097661015 0.034751977172979905; ‚Ä¶ ; 0.003353760095210695 -0.0072432342097661015 ‚Ä¶ 0.16389731989998213 -0.06470610035719401; -0.018309225068336522 0.034751977172979905 ‚Ä¶ -0.06470610035719401 0.23920144467736396]\n [0.027817388441617175 0.007387590578894039 ‚Ä¶ 0.019042932015690548 -0.028226445784808304; 0.007387590578894039 0.11441929261527704 ‚Ä¶ -0.006164311375937795 -0.008358308314680471; ‚Ä¶ ; 0.019042932015690548 -0.006164311375937795 ‚Ä¶ 0.021144713837013367 -0.020162959250659168; -0.028226445784808304 -0.008358308314680471 ‚Ä¶ -0.020162959250659168 0.030837211669225487;;; 0.028686844624263574 0.007310377472721044 ‚Ä¶ 0.019827172893097066 -0.029132312144644068; 0.007310377472721044 0.11438276975667708 ‚Ä¶ -0.0061932751293762465 -0.00828119248929339; ‚Ä¶ ; 0.019827172893097066 -0.0061932751293762465 ‚Ä¶ 0.021816264097811625 -0.0209771256783798; -0.029132312144644068 -0.00828119248929339 ‚Ä¶ -0.0209771256783798 0.031780777518025036;;; 0.03895542526613115 0.00833193175422314 ‚Ä¶ 0.02700959922771861 -0.03969790804850063; 0.00833193175422314 0.10289898223917786 ‚Ä¶ -0.004470909684883572 -0.00908777432147014; ‚Ä¶ ; 0.02700959922771861 -0.004470909684883572 ‚Ä¶ 0.026124347017010812 -0.02822148150731481; -0.03969790804850063 -0.00908777432147014 ‚Ä¶ -0.02822148150731481 0.04248556418672119;;; ‚Ä¶ ;;; 0.030604856172918304 0.009637023743445555 ‚Ä¶ 0.02009657737361203 -0.03109203384193495; 0.009637023743445555 0.12980472966547998 ‚Ä¶ -0.006005254569091734 -0.01101277658543406; ‚Ä¶ ; 0.02009657737361203 -0.006005254569091734 ‚Ä¶ 0.02201970795401663 -0.02135664683847035; -0.03109203384193495 -0.01101277658543406 ‚Ä¶ -0.02135664683847035 0.033941862763799534;;; 0.02970879738297442 0.008192688782410607 ‚Ä¶ 0.020148409366859382 -0.030242224340043686; 0.008192688782410607 0.10431241165661297 ‚Ä¶ -0.0046607112724289185 -0.008846886424849855; ‚Ä¶ ; 0.020148409366859382 -0.0046607112724289185 ‚Ä¶ 0.02094351893437889 -0.021183437132644357; -0.030242224340043686 -0.008846886424849855 ‚Ä¶ -0.021183437132644357 0.03270842388651754;;; 0.029824800138703454 0.007827163711563603 ‚Ä¶ 0.019753329832308632 -0.030294921799274465; 0.007827163711563603 0.1230742645622283 ‚Ä¶ -0.005620981477568017 -0.008849008063510548; ‚Ä¶ ; 0.019753329832308632 -0.005620981477568017 ‚Ä¶ 0.021039760287484038 -0.020822020378027894; -0.030294921799274465 -0.008849008063510548 ‚Ä¶ -0.020822020378027894 0.03282275613725856]\n [0.00035644045136612077 1.118339427728724e-5 ‚Ä¶ -2.1686870370282198e-7 -1.3232215234585795e-5; 1.118339427728724e-5 0.0026572044210832284 ‚Ä¶ -1.0319831777589507e-5 -3.427051176599017e-5; ‚Ä¶ ; -2.1686870370282198e-7 -1.0319831777589507e-5 ‚Ä¶ 7.215819577550748e-5 -6.179263245190656e-6; -1.3232215234585795e-5 -3.427051176599017e-5 ‚Ä¶ -6.179263245190656e-6 0.0008429709194202059;;; 0.0003473684307087321 1.2629152604154411e-5 ‚Ä¶ -2.400164202255046e-7 -1.5025316011698084e-5; 1.2629152604154411e-5 0.0024201354961587997 ‚Ä¶ -1.0791300341680005e-5 -3.232633447065208e-5; ‚Ä¶ ; -2.400164202255046e-7 -1.0791300341680005e-5 ‚Ä¶ 7.19064575231302e-5 -7.0824380864623695e-6; -1.5025316011698084e-5 -3.232633447065208e-5 ‚Ä¶ -7.0824380864623695e-6 0.0008187074824714124;;; 0.000339744493928228 1.4947377215304058e-5 ‚Ä¶ -2.596390018217046e-7 -1.67081067299837e-5; 1.4947377215304058e-5 0.0022242072298914216 ‚Ä¶ -1.1016246468088464e-5 -3.125217619469785e-5; ‚Ä¶ ; -2.596390018217046e-7 -1.1016246468088464e-5 ‚Ä¶ 7.167809699588528e-5 -8.039713278532149e-6; -1.67081067299837e-5 -3.125217619469785e-5 ‚Ä¶ -8.039713278532149e-6 0.0007958084286414547;;; 0.00034825084603987524 1.3809135909244489e-5 ‚Ä¶ -2.515874233043251e-7 -1.5014035243051027e-5; 1.3809135909244489e-5 0.0024222363584031183 ‚Ä¶ -1.0799540316498638e-5 -3.326628991544328e-5; ‚Ä¶ ; -2.515874233043251e-7 -1.0799540316498638e-5 ‚Ä¶ 7.191876699683024e-5 -7.1087741880281e-6; -1.5014035243051027e-5 -3.326628991544328e-5 ‚Ä¶ -7.1087741880281e-6 0.0008196404552873635;;; 0.00034068130351609595 1.4788352402541657e-5 ‚Ä¶ -2.8794431458386357e-7 -1.676165931037454e-5; 1.4788352402541657e-5 0.0022581068155001775 ‚Ä¶ -1.1188280880563895e-5 -3.205421966481175e-5; ‚Ä¶ ; -2.8794431458386357e-7 -1.1188280880563895e-5 ‚Ä¶ 7.1712258094887e-5 -7.866932128985891e-6; -1.676165931037454e-5 -3.205421966481175e-5 ‚Ä¶ -7.866932128985891e-6 0.0007982666962034134;;; 0.0003386439618747802 1.5157376199521329e-5 ‚Ä¶ -2.7505886706978413e-7 -1.6819060216265967e-5; 1.5157376199521329e-5 0.0022184752613332312 ‚Ä¶ -1.1127000235199859e-5 -3.2540653526667593e-5; ‚Ä¶ ; -2.7505886706978413e-7 -1.1127000235199859e-5 ‚Ä¶ 7.164988520837927e-5 -8.098614509745355e-6; -1.6819060216265967e-5 -3.2540653526667593e-5 ‚Ä¶ -8.098614509745355e-6 0.0007931394399706686;;; 0.0003355737559395007 1.3673902414463906e-5 ‚Ä¶ -3.6426325269784853e-7 -1.7691545711223047e-5; 1.3673902414463906e-5 0.0021628883414762674 ‚Ä¶ -1.1364109322796006e-5 -3.1454682023520614e-5; ‚Ä¶ ; -3.6426325269784853e-7 -1.1364109322796006e-5 ‚Ä¶ 7.153438028181438e-5 -8.37182523807883e-6; -1.7691545711223047e-5 -3.1454682023520614e-5 ‚Ä¶ -8.37182523807883e-6 0.0007822931305374102;;; 0.0003286191120672152 1.4228600352517084e-5 ‚Ä¶ -3.2600141151667164e-7 -1.8950073357740682e-5; 1.4228600352517084e-5 0.002041233342802819 ‚Ä¶ -1.1264509816227406e-5 -3.0193353042461853e-5; ‚Ä¶ ; -3.2600141151667164e-7 -1.1264509816227406e-5 ‚Ä¶ 7.136189098608551e-5 -9.069633772888744e-6; -1.8950073357740682e-5 -3.0193353042461853e-5 ‚Ä¶ -9.069633772888744e-6 0.0007655978064625865;;; 0.0003163697525884593 1.3324746583364404e-5 ‚Ä¶ -3.9856188164730807e-7 -2.1413489059313608e-5; 1.3324746583364404e-5 0.0018850670631649893 ‚Ä¶ -1.1186075346609143e-5 -2.7605181090178752e-5; ‚Ä¶ ; -3.9856188164730807e-7 -1.1186075346609143e-5 ‚Ä¶ 7.102434040761308e-5 -1.0152662086865305e-5; -2.1413489059313608e-5 -2.7605181090178752e-5 ‚Ä¶ -1.0152662086865305e-5 0.0007346345276310783]\n\n\n\ncondVar(m2)\n\n3-element Vector{Array{Float64, 3}}:\n [0.1835001431437919 0.052362461187019524 ‚Ä¶ 0.05339916026138972 0.013923123659454872; 0.052362461187019524 0.18606882138848577 ‚Ä¶ 0.052537631014878704 0.03698993187011031; ‚Ä¶ ; 0.05339916026138972 0.052537631014878704 ‚Ä¶ 0.17226520227407593 0.04257225305867498; 0.013923123659454872 0.03698993187011031 ‚Ä¶ 0.04257225305867498 0.17875235440113763;;; 0.16000901416587507 0.027988825367257934 ‚Ä¶ 0.02936430932588277 -0.0022205577490522627; 0.027988825367257934 0.1575306301862166 ‚Ä¶ 0.02597380704486902 0.01873044094547151; ‚Ä¶ ; 0.02936430932588277 0.02597380704486902 ‚Ä¶ 0.14589016916623235 0.025340657079790555; -0.0022205577490522627 0.01873044094547151 ‚Ä¶ 0.025340657079790555 0.1665327636837247;;; 0.18503854342288534 0.05353270902844885 ‚Ä¶ 0.05455269949893274 0.014551816382064655; 0.05353270902844885 0.18818540743271164 ‚Ä¶ 0.05401678626284685 0.03792133886224371; ‚Ä¶ ; 0.05455269949893274 0.05401678626284685 ‚Ä¶ 0.17378686701640988 0.04330258803655117; 0.014551816382064655 0.03792133886224371 ‚Ä¶ 0.04330258803655117 0.17960331949528155;;; ‚Ä¶ ;;; 0.17329631356648986 0.03844459064093947 ‚Ä¶ 0.04515882892629477 0.008277842858652917; 0.03844459064093947 0.1694584844887934 ‚Ä¶ 0.04089294576482905 0.028642007901135667; ‚Ä¶ ; 0.04515882892629477 0.04089294576482905 ‚Ä¶ 0.16772514645418998 0.03997011273933929; 0.008277842858652917 0.028642007901135667 ‚Ä¶ 0.03997011273933929 0.17819486637156584;;; 0.17885101484563234 0.04382162222416236 ‚Ä¶ 0.0536255708191783 0.011555599720037293; 0.04382162222416236 0.17473722403965433 ‚Ä¶ 0.049138998874481664 0.031861066311365996; ‚Ä¶ ; 0.0536255708191783 0.049138998874481664 ‚Ä¶ 0.180787497877876 0.04493707265826943; 0.011555599720037293 0.031861066311365996 ‚Ä¶ 0.04493707265826943 0.1801728139176115;;; 0.1723707683762156 0.03676527055363047 ‚Ä¶ 0.04369963643182943 0.007452389668158451; 0.03676527055363047 0.16601522330341562 ‚Ä¶ 0.03876536043442933 0.027310780530726688; ‚Ä¶ ; 0.04369963643182943 0.03876536043442933 ‚Ä¶ 0.1654167080171395 0.03884397367029101; 0.007452389668158451 0.027310780530726688 ‚Ä¶ 0.03884397367029101 0.1767098024931638]\n [0.06935262897946186 0.018484141616399483 ‚Ä¶ 0.015669150134245927 -0.02779242653743672; 0.018484141616399483 0.08269633649202793 ‚Ä¶ 0.010554714414613453 -0.03588864789163317; ‚Ä¶ ; 0.015669150134245927 0.010554714414613453 ‚Ä¶ 0.02082377246157113 -0.020313851621264808; -0.02779242653743672 -0.03588864789163317 ‚Ä¶ -0.020313851621264808 0.03080079682487133;;; 0.07011301049876431 0.01917361666590066 ‚Ä¶ 0.016421798838368442 -0.02865075177354453; 0.01917361666590066 0.08328565445797546 ‚Ä¶ 0.011294715055333651 -0.03669652320871563; ‚Ä¶ ; 0.016421798838368442 0.011294715055333651 ‚Ä¶ 0.021488517711964163 -0.021124540950429534; -0.02865075177354453 -0.03669652320871563 ‚Ä¶ -0.021124540950429534 0.031751358468024966;;; 0.07816809954562845 0.034364100511522895 ‚Ä¶ 0.024508724005969495 -0.040348772255025096; 0.034364100511522895 0.09402002138931224 ‚Ä¶ 0.021004387615427396 -0.04918800917390768; ‚Ä¶ ; 0.024508724005969495 0.021004387615427396 ‚Ä¶ 0.025898824281255135 -0.028406134240691317; -0.040348772255025096 -0.04918800917390768 ‚Ä¶ -0.028406134240691317 0.04243142160994624;;; ‚Ä¶ ;;; 0.07749583170747879 0.0212279795426477 ‚Ä¶ 0.01669058189565822 -0.03057284886749437; 0.0212279795426477 0.09556961200920702 ‚Ä¶ 0.011786980242716731 -0.041227064122388746; ‚Ä¶ ; 0.01669058189565822 0.011786980242716731 ‚Ä¶ 0.021719196702437493 -0.021549187113314175; -0.03057284886749437 -0.041227064122388746 ‚Ä¶ -0.021549187113314175 0.03387941240184749;;; 0.06776927193892422 0.022830498853856955 ‚Ä¶ 0.0172329894578496 -0.030143456352972522; 0.022830498853856955 0.0828331997331298 ‚Ä¶ 0.013526018425622503 -0.038692901184855674; ‚Ä¶ ; 0.0172329894578496 0.013526018425622503 ‚Ä¶ 0.020700738814047086 -0.021349074006958315; -0.030143456352972522 -0.038692901184855674 ‚Ä¶ -0.021349074006958315 0.03263300184820812;;; 0.07745874824907986 0.021073436718914543 ‚Ä¶ 0.016874690753658616 -0.030809140281248277; 0.021073436718914543 0.08840836526588497 ‚Ä¶ 0.012337289465195227 -0.03932256233151305; ‚Ä¶ ; 0.016874690753658616 0.012337289465195227 ‚Ä¶ 0.020769658213351184 -0.020995492575951576; -0.030809140281248277 -0.03932256233151305 ‚Ä¶ -0.020995492575951576 0.03275071330488253]\n [0.0016570093831594494 0.0001605165081361211 ‚Ä¶ 6.26449498563687e-5 1.012101930456336e-5; 0.0001605165081361211 0.002389018142602151 ‚Ä¶ 8.118197626224069e-5 5.4285617348541746e-5; ‚Ä¶ ; 6.26449498563687e-5 8.118197626224069e-5 ‚Ä¶ 0.0007077304538191391 2.40675517991294e-5; 1.012101930456336e-5 5.4285617348541746e-5 ‚Ä¶ 2.40675517991294e-5 0.000714051122838681;;; 0.0015394575161252642 0.00017258640859948987 ‚Ä¶ 7.242211576723615e-5 1.2420976154459829e-5; 0.00017258640859948987 0.0021334337225968657 ‚Ä¶ 9.115235296073128e-5 5.955520757128931e-5; ‚Ä¶ ; 7.242211576723615e-5 9.115235296073128e-5 ‚Ä¶ 0.0006845812728493166 2.8370292190531164e-5; 1.2420976154459829e-5 5.955520757128931e-5 ‚Ä¶ 2.8370292190531164e-5 0.0006929082777478652;;; 0.0014353324224963642 0.000181570887395417 ‚Ä¶ 7.965181578808337e-5 1.5898481293945134e-5; 0.000181570887395417 0.001967559409669854 ‚Ä¶ 9.644732649530252e-5 6.474503204243903e-5; ‚Ä¶ ; 7.965181578808337e-5 9.644732649530252e-5 ‚Ä¶ 0.0006665332668399944 3.18296286588633e-5; 1.5898481293945134e-5 6.474503204243903e-5 ‚Ä¶ 3.18296286588633e-5 0.0006730646384418408;;; 0.0015391110750365507 0.00017398728902796724 ‚Ä¶ 7.166734276607453e-5 1.3014602339309143e-5; 0.00017398728902796724 0.0021609729481794444 ‚Ä¶ 8.951376674860006e-5 5.9936266355353196e-5; ‚Ä¶ ; 7.166734276607453e-5 8.951376674860006e-5 ‚Ä¶ 0.0006866635564611906 2.7925615721531105e-5; 1.3014602339309143e-5 5.9936266355353196e-5 ‚Ä¶ 2.7925615721531105e-5 0.0006937682889276523;;; 0.001450834975643842 0.00017641445153016236 ‚Ä¶ 7.920479324923204e-5 1.5232424203072496e-5; 0.00017641445153016236 0.0020075034663078577 ‚Ä¶ 9.544567055053978e-5 6.294300514751181e-5; ‚Ä¶ ; 7.920479324923204e-5 9.544567055053978e-5 ‚Ä¶ 0.0006681050252635448 3.180867221148738e-5; 1.5232424203072496e-5 6.294300514751181e-5 ‚Ä¶ 3.180867221148738e-5 0.0006750666836022721;;; 0.0014291203800345827 0.00017639219162405643 ‚Ä¶ 7.954482172948655e-5 1.6380970159991346e-5; 0.00017639219162405643 0.0019574998303365297 ‚Ä¶ 9.712503108636175e-5 6.373298211868371e-5; ‚Ä¶ ; 7.954482172948655e-5 9.712503108636175e-5 ‚Ä¶ 0.0006636179390229105 3.2495283823568136e-5; 1.6380970159991346e-5 6.373298211868371e-5 ‚Ä¶ 3.2495283823568136e-5 0.0006707825115150795;;; 0.0014016300567330261 0.0001783784597863537 ‚Ä¶ 8.384398933706971e-5 1.772303098145762e-5; 0.0001783784597863537 0.0018916540528781734 ‚Ä¶ 0.00010189657450799072 6.59655233590099e-5; ‚Ä¶ ; 8.384398933706971e-5 0.00010189657450799072 ‚Ä¶ 0.0006520328959623462 3.474581246863309e-5; 1.772303098145762e-5 6.59655233590099e-5 ‚Ä¶ 3.474581246863309e-5 0.0006614401172016982;;; 0.001333603126818329 0.00017795808514037758 ‚Ä¶ 8.747970991945111e-5 1.9495259460074227e-5; 0.00017795808514037758 0.0017601698598955224 ‚Ä¶ 0.00010494805885946578 6.786290227588929e-5; ‚Ä¶ ; 8.747970991945111e-5 0.00010494805885946578 ‚Ä¶ 0.0006397123530838859 3.6866810196614546e-5; 1.9495259460074227e-5 6.786290227588929e-5 ‚Ä¶ 3.6866810196614546e-5 0.0006465074650638469;;; 0.0012451965621625906 0.00017749126130467494 ‚Ä¶ 9.258257619507684e-5 2.1708303893508674e-5; 0.00017749126130467494 0.0015949014547548084 ‚Ä¶ 0.00011027161033741918 7.127259634664323e-5; ‚Ä¶ ; 9.258257619507684e-5 0.00011027161033741918 ‚Ä¶ 0.0006137501376265905 4.1045140839672455e-5; 2.1708303893508674e-5 7.127259634664323e-5 ‚Ä¶ 4.1045140839672455e-5 0.0006186835419614991]\n\n\nThey are hard to look at. Let‚Äôs take pictures.\n\n\n6.4.2 Caterpillar plots\n\n\nCode\ncaterpillar!(\n  Figure(; resolution=(800, 400)), ranefinfo(m1, :Cohort)\n)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†3: Prediction intervals of the random effects for Cohort in model m1\n\n\n\n\n\n\n6.4.3 Shrinkage plots\nThese are just teasers. We will pick this up in a separate tutorial. Enjoy!\n\n\nCode\nshrinkageplot!(Figure(; resolution=(800, 800)), m1, :Cohort)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†4: Shrinkage plot of the random effects for Cohort in model m1\n\n\n\n\n\n\nCode\nshrinkageplot!(Figure(; resolution=(800, 800)), m2, :Cohort)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†5: Shrinkage plot of the random effects for Cohort in model m2\n\n\n\n\n\n\nF√ºhner, T., Granacher, U., Golle, K., & Kliegl, R. (2021). Age and sex effects in physical fitness components of 108,295 third graders including 515 primary schools and 9 cohorts. Scientific Reports, 11(1). https://doi.org/10.1038/s41598-021-97000-4\n\n\nSchielzeth, H., & Forstmeier, W. (2008). Conclusions beyond support: Overconfident estimates in mixed models. Behavioral Ecology, 20(2), 416‚Äì420. https://doi.org/10.1093/beheco/arn145\n\n\nThis page was rendered from git revision fb99b3c\n using Quarto 1.7.33."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tenth Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "",
    "text": "This site provides materials for the Advanced frequentist methods stream of the Summer School on Statistical Methods to be held at the University of Potsdam, 24‚Äì28 August 2026."
  },
  {
    "objectID": "index.html#git",
    "href": "index.html#git",
    "title": "Tenth Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "1.1 git",
    "text": "1.1 git\nWe will assume that you have git installed and are able to clone a repository from github. If not, Happy Git with R is a good place to learn about git for data science.\nThe example data stored in the data folder is stored with git-lfs. You will need to install git-lfs to access this data, but then everything will ‚Äújust‚Äù work.\nThis website is built using quarto, described below, from the repository. Clone this repository with, e.g.\ngit clone https://github.com/RePsychLing/SMLP2026"
  },
  {
    "objectID": "index.html#julia-programming-language",
    "href": "index.html#julia-programming-language",
    "title": "Tenth Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "1.2 Julia Programming Language",
    "text": "1.2 Julia Programming Language\nWe will use Julia v1.11.6. We recommend using Juliaup to install and manage Julia versions. Juliaup makes it trivial to upgrade to new Julia releases or even use old ones. Alternatively, you can download the version appropriate for your setup from here: Julia Programming Language"
  },
  {
    "objectID": "index.html#quarto",
    "href": "index.html#quarto",
    "title": "Tenth Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "1.3 Quarto",
    "text": "1.3 Quarto\nThe web site and other documents for this course are rendered using a knitr-like system called Quarto. You can download the version appropriate for your setup from here: quarto"
  },
  {
    "objectID": "index.html#integrated-development-environment-ide",
    "href": "index.html#integrated-development-environment-ide",
    "title": "Tenth Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "1.4 Integrated Development Environment (IDE)",
    "text": "1.4 Integrated Development Environment (IDE)\nAn integrated development environment (IDE) makes editing and running Julia and Quarto source files easier. The ‚Äúintegrated development‚Äù portion is a clue to what makes them different than a text editor (even a text editor with syntax highlighting): they have tools for suggesting code completion, interacting with and running code and much more.\nPopular ones for R and Python include RStudio and Spyder. For Julia, we recommend VS Code or Positron. VS Code is widely used in the Julia community, while Positron is a new fork of VS Code aimed at data scientists and developed by the Posit, the folks behind RStudio and Quarto.\nWhatever IDE you use, make sure to install the necessary extensions for Quarto and Julia."
  },
  {
    "objectID": "kkl15.html",
    "href": "kkl15.html",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "",
    "text": "Kliegl et al. (2015) is a follow-up to Kliegl et al. (2011) (see also script kwdyz11.qmd) from an experiment looking at a variety of effects of visual cueing under four different cue-target relations (CTRs). In this experiment two rectangles are displayed (1) in horizontal orientation , (2) in vertical orientation, (3) in left diagonal orientation, or in (4) right diagonal orientation relative to a central fixation point. Subjects react to the onset of a small or a large visual target occurring at one of the four ends of the two rectangles. The target is cued validly on 70% of trials by a brief flash of the corner of the rectangle at which it appears; it is cued invalidly at the three other locations 10% of the trials each. This implies a latent imbalance in design that is not visible in the repeated-measures ANOVA, but we will show its effect in the random-effect structure and conditional modes.\nThere are a couple of differences between the first and this follow-up experiment, rendering it more a conceptual than a direct replication. First, the original experiment was carried out at Peking University and this follow-up at Potsdam University. Second, diagonal orientations of rectangles and large target sizes were not part of the design of Kliegl et al. (2011).\nWe specify three contrasts for the four-level factor CTR that are derived from spatial, object-based, and attractor-like features of attention. They map onto sequential differences between appropriately ordered factor levels. Replicating Kliegl et al. (2011), the attraction effect was not significant as a fixed effect, but yielded a highly reliable variance component (VC; i.e., reliable individual differences in positive and negative attraction effects cancel the fixed effect). Moreover, these individual differences in the attraction effect were negatively correlated with those in the spatial effect.\nThis comparison is of interest because a few years after the publication of Kliegl et al. (2011), the theoretically critical correlation parameter (CP) between the spatial effect and the attraction effect was determined as the source of a non-singular LMM in that paper. The present study served the purpose to estimate this parameter with a larger sample and a wider variety of experimental conditions.\nHere we also include two additional experimental manipulations of target size and orientation of cue rectangle. A similar analysis was reported in the parsimonious mixed-model paper (Bates et al., 2015); it was also used in a paper of GAMEMs (Baayen et al., 2017). Data and R scripts of those analyses are also available in R-package RePsychLing.\nThe analysis is based on reaction times rt to maintain compatibility with Kliegl et al. (2011).\nIn this vignette we focus on the reduction of model complexity. And we start with a quote:\n‚ÄúNeither the [maximal] nor the [minimal] linear mixed models are appropriate for most repeated measures analysis. Using the [maximal] model is generally wasteful and costly in terms of statiscal power for testing hypotheses. On the other hand, the [minimal] model fails to account for nontrivial correlation among repeated measurements. This results in inflated [T]ype I error rates when non-negligible correlation does in fact exist. We can usually find middle ground, a covariance model that adequately accounts for correlation but is more parsimonious than the [maximal] model. Doing so allows us full control over [T]ype I error rates without needlessly sacrificing power.‚Äù\nStroup, W. W. (2012, p.¬†185). Generalized linear mixed models: Modern concepts, methods and applica?ons. CRC Press, Boca Raton.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#zero-correlation-parameter-lmm-1",
    "href": "kkl15.html#zero-correlation-parameter-lmm-1",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "6.1 Zero-correlation parameter LMM (1)",
    "text": "6.1 Zero-correlation parameter LMM (1)\nForce CPs to zero. Reduction strategy 1 is more suited for reducing model w/o theoretical expectations about CPs. The better reduction strategy for the present experiment with an a priori interest in CPs is described as Reduction strategy 2.\n\nm_zcp1 = let\n  form = @formula rt ~ 1 + CTR * cardinal * size +\n                   zerocorr(1 + CTR * cardinal | Subj)\n  fit(MixedModel, form, dat; contrasts)\nend;\n\n\nissingular(m_zcp1)\n\ntrue\n\n\n\nonly(MixedModels.PCA(m_zcp1))\n\n\nPrincipal components based on correlation matrix\n (Intercept)                    1.0  .    .    .    .    .    .    .\n CTR: sod                       0.0  1.0  .    .    .    .    .    .\n CTR: dos                       0.0  0.0  1.0  .    .    .    .    .\n CTR: dod                       0.0  0.0  0.0  1.0  .    .    .    .\n cardinal: diagonal             0.0  0.0  0.0  0.0  1.0  .    .    .\n CTR: sod & cardinal: diagonal  0.0  0.0  0.0  0.0  0.0  1.0  .    .\n CTR: dos & cardinal: diagonal  0.0  0.0  0.0  0.0  0.0  0.0  0.0  .\n CTR: dod & cardinal: diagonal  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n\nNormalized cumulative variances:\n[0.1429, 0.2857, 0.4286, 0.5714, 0.7143, 0.8571, 1.0, 1.0]\n\nComponent loadings\n                                 PC1  ‚Ä¶   PC3   PC4   PC5   PC6   PC7   PC8\n (Intercept)                    1.0      0.0   0.0   0.0   0.0   0.0   0.0\n CTR: sod                       0.0      0.0   0.0   0.0   0.0   0.0   0.0\n CTR: dos                       0.0      1.0   0.0   0.0   0.0   0.0   0.0\n CTR: dod                       0.0      0.0   0.0   0.0   0.0   1.0   0.0\n cardinal: diagonal             0.0   ‚Ä¶  0.0   1.0   0.0   0.0   0.0   0.0\n CTR: sod & cardinal: diagonal  0.0      0.0   0.0   1.0   0.0   0.0   0.0\n CTR: dos & cardinal: diagonal  0.0      0.0   0.0   0.0   0.0   0.0   1.0\n CTR: dod & cardinal: diagonal  0.0      0.0   0.0   0.0   1.0   0.0   0.0\n\n\n\nVarCorr(m_zcp1)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\nSubj\n(Intercept)\n2875.36834\n53.62246\n\n\n\n\n\n\n\n\n\n\nCTR: sod\n482.88548\n21.97466\n.\n\n\n\n\n\n\n\n\n\nCTR: dos\n79.81159\n8.93373\n.\n.\n\n\n\n\n\n\n\n\nCTR: dod\n217.17364\n14.73681\n.\n.\n.\n\n\n\n\n\n\n\ncardinal: diagonal\n250.29104\n15.82059\n.\n.\n.\n.\n\n\n\n\n\n\nCTR: sod & cardinal: diagonal\n35.96505\n5.99709\n.\n.\n.\n.\n.\n\n\n\n\n\nCTR: dos & cardinal: diagonal\n0.00000\n0.00000\n.\n.\n.\n.\n.\n.\n\n\n\n\nCTR: dod & cardinal: diagonal\n6.85703\n2.61859\n.\n.\n.\n.\n.\n.\n.\n\n\nResidual\n\n3972.38091\n63.02683\n\n\n\n\n\n\n\n\n\n\n\n\nThe LMM m_zcp1 is also overparameterized, but now there is clear evidence for absence of evidence for the VC of one of the interactions and the other two interaction-based VCs are also very small.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#reduced-zcp-lmm",
    "href": "kkl15.html#reduced-zcp-lmm",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "6.2 Reduced zcp LMM",
    "text": "6.2 Reduced zcp LMM\nTake out VCs for interactions.\n\nm_zcp1_rdc = let\n  form = @formula rt ~ 1 + CTR * cardinal * size +\n                   zerocorr(1 + CTR + cardinal | Subj)\n  fit(MixedModel, form, dat; contrasts)\nend;\n\n\nissingular(m_zcp1_rdc)\n\nfalse\n\n\n\nonly(MixedModels.PCA(m_zcp1_rdc))\n\n\nPrincipal components based on correlation matrix\n (Intercept)         1.0  .    .    .    .\n CTR: sod            0.0  1.0  .    .    .\n CTR: dos            0.0  0.0  1.0  .    .\n CTR: dod            0.0  0.0  0.0  1.0  .\n cardinal: diagonal  0.0  0.0  0.0  0.0  1.0\n\nNormalized cumulative variances:\n[0.2, 0.4, 0.6, 0.8, 1.0]\n\nComponent loadings\n                      PC1   PC2   PC3   PC4   PC5\n (Intercept)         0.0   0.0   0.0   1.0   0.0\n CTR: sod            1.0   0.0   0.0   0.0   0.0\n CTR: dos            0.0   1.0   0.0   0.0   0.0\n CTR: dod            0.0   0.0   1.0   0.0   0.0\n cardinal: diagonal  0.0   0.0   0.0   0.0   1.0\n\n\n\nVarCorr(m_zcp1_rdc)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\nSubj\n(Intercept)\n2880.90084\n53.67402\n\n\n\n\n\n\n\nCTR: sod\n484.39077\n22.00888\n.\n\n\n\n\n\n\nCTR: dos\n79.50606\n8.91662\n.\n.\n\n\n\n\n\nCTR: dod\n216.56629\n14.71619\n.\n.\n.\n\n\n\n\ncardinal: diagonal\n244.93221\n15.65031\n.\n.\n.\n.\n\n\nResidual\n\n3980.88042\n63.09422\n\n\n\n\n\n\n\n\n\nLMM m_zcp_rdc is ok . We add in CPs.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#parsimonious-lmm-1",
    "href": "kkl15.html#parsimonious-lmm-1",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "6.3 Parsimonious LMM (1)",
    "text": "6.3 Parsimonious LMM (1)\nExtend zcp-reduced LMM with CPs\n\nm_prm1 = let\n  form = @formula rt ~ 1 + CTR * cardinal * size +\n                           (1 + CTR + cardinal | Subj)\n  fit(MixedModel, form, dat; contrasts)\nend;\n\n\nissingular(m_prm1)\n\nfalse\n\n\n\nonly(MixedModels.PCA(m_prm1))\n\n\nPrincipal components based on correlation matrix\n (Intercept)          1.0     .      .      .      .\n CTR: sod             0.6    1.0     .      .      .\n CTR: dos             0.45   0.19   1.0     .      .\n CTR: dod             0.37   0.54   0.3    1.0     .\n cardinal: diagonal  -0.1   -0.05  -0.09   0.13   1.0\n\nNormalized cumulative variances:\n[0.4489, 0.667, 0.8308, 0.9439, 1.0]\n\nComponent loadings\n                       PC1    PC2    PC3    PC4    PC5\n (Intercept)         -0.55  -0.16   0.0    0.58  -0.57\n CTR: sod            -0.54   0.09  -0.49   0.16   0.66\n CTR: dos            -0.4   -0.25   0.8   -0.12   0.34\n CTR: dod            -0.49   0.36  -0.09  -0.71  -0.35\n cardinal: diagonal   0.05   0.88   0.32   0.34   0.07\n\n\nLMM m_zcp_rdc is ok . We add in CPs.\n\nVarCorr(m_prm1)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\nSubj\n(Intercept)\n2923.31335\n54.06767\n\n\n\n\n\n\n\nCTR: sod\n454.67288\n21.32306\n+0.60\n\n\n\n\n\n\nCTR: dos\n56.63260\n7.52546\n+0.45\n+0.19\n\n\n\n\n\nCTR: dod\n188.56029\n13.73173\n+0.37\n+0.54\n+0.30\n\n\n\n\ncardinal: diagonal\n245.42177\n15.66594\n-0.10\n-0.05\n-0.09\n+0.13\n\n\nResidual\n\n3981.72067\n63.10088\n\n\n\n\n\n\n\n\n\nWe note that the critical correlation parameter between spatial (sod) and attraction (dod) is now estimated at .54 ‚Äì not that close to the 1.0 boundary that caused singularity in Kliegl et al. (2011).",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#model-comparison-1",
    "href": "kkl15.html#model-comparison-1",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "6.4 Model comparison 1",
    "text": "6.4 Model comparison 1\n\ngof_summary = let\n  nms = [:m_zcp1_rdc, :m_prm1, :m_max]\n  mods = eval.(nms)\n  lrt = MixedModels.likelihoodratiotest(m_zcp1_rdc, m_prm1, m_max)\n  DataFrame(;\n    name = nms,\n    dof=dof.(mods),\n    deviance=round.(deviance.(mods), digits=0),\n    AIC=round.(aic.(mods),digits=0),\n    AICc=round.(aicc.(mods),digits=0),\n    BIC=round.(bic.(mods),digits=0),\n    œá¬≤=vcat(:., round.(Int, diff(collect(lrt.lrt.deviance)))),\n    œá¬≤_dof=vcat(:., diff(collect(lrt.lrt.dof))),\n    # StatsBase.PValue includes some pretty-printing methods\n    pvalue=vcat(:., StatsBase.PValue.(collect(lrt.lrt.pval[2:end])))\n  )\nend\n\n3√ó9 DataFrame\n\n\n\nRow\nname\ndof\ndeviance\nAIC\nAICc\nBIC\nœá¬≤\nœá¬≤_dof\npvalue\n\n\n\nSymbol\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nAny\nAny\nAny\n\n\n\n\n1\nm_zcp1_rdc\n22\n599486.0\n599530.0\n599530.0\n599725.0\n.\n.\n.\n\n\n2\nm_prm1\n32\n599418.0\n599482.0\n599482.0\n599766.0\n-68\n10\n&lt;1e-09\n\n\n3\nm_max\n53\n599359.0\n599465.0\n599465.0\n599936.0\n-59\n21\n&lt;1e-04\n\n\n\n\n\n\nAIC prefers LMM m_prm1 over m_zcp1_rdc; BIC LMM m_zcp1_rdc. As the CPs were one reason for conducting this experiment, AIC is the criterion of choice.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#complex-lmm",
    "href": "kkl15.html#complex-lmm",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "7.1 Complex LMM",
    "text": "7.1 Complex LMM\nRelative to LMM m_max, first we take out interaction VCs and associated CPs, because these VCs are very small. This is the same as LMM m_prm1 above.\n\nm_cpx = let\n  form = @formula rt ~ 1 + CTR * cardinal * size +\n                      (1 + CTR + cardinal | Subj)\n  fit(MixedModel, form, dat; contrasts)\nend;",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#zero-correlation-parameter-lmm-2",
    "href": "kkl15.html#zero-correlation-parameter-lmm-2",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "7.2 Zero-correlation parameter LMM (2)",
    "text": "7.2 Zero-correlation parameter LMM (2)\nNow we check the significance of ensemble of CPs.\n\nm_zcp2 = let\n  form = @formula rt ~ 1 + CTR * cardinal * size  +\n              zerocorr(1 + CTR + cardinal | Subj)\n  fit(MixedModel, form, dat; contrasts)\nend;\n\n\nVarCorr(m_zcp2)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\nSubj\n(Intercept)\n2880.90084\n53.67402\n\n\n\n\n\n\n\nCTR: sod\n484.39077\n22.00888\n.\n\n\n\n\n\n\nCTR: dos\n79.50606\n8.91662\n.\n.\n\n\n\n\n\nCTR: dod\n216.56629\n14.71619\n.\n.\n.\n\n\n\n\ncardinal: diagonal\n244.93221\n15.65031\n.\n.\n.\n.\n\n\nResidual\n\n3980.88042\n63.09422",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#parsimonious-lmm-2",
    "href": "kkl15.html#parsimonious-lmm-2",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "7.3 Parsimonious LMM (2)",
    "text": "7.3 Parsimonious LMM (2)\nThe cardinal-related CPs are quite small. Do we need them?\n\nm_prm2 = let\n  form = @formula(rt ~ 1 + CTR * cardinal * size  +\n                      (1 + CTR | Subj) + (0 + cardinal | Subj))\n  fit(MixedModel, form, dat; contrasts)\nend;\n\n\nVarCorr(m_prm2)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\nSubj\n(Intercept)\n2924.00272\n54.07405\n\n\n\n\n\n\n\nCTR: sod\n454.44827\n21.31779\n+0.60\n\n\n\n\n\n\nCTR: dos\n56.45775\n7.51384\n+0.45\n+0.19\n\n\n\n\n\nCTR: dod\n187.90094\n13.70770\n+0.37\n+0.54\n+0.30\n\n\n\n\ncardinal: diagonal\n245.17053\n15.65792\n.\n.\n.\n.\n\n\nResidual\n\n3981.74101\n63.10104",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#model-comparison-2",
    "href": "kkl15.html#model-comparison-2",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "7.4 Model comparison 2",
    "text": "7.4 Model comparison 2\n\ngof_summary = let\n  nms = [:m_zcp2, :m_prm2, :m_cpx, :m_max]\n  mods = eval.(nms)\n  lrt = MixedModels.likelihoodratiotest(m_zcp2, m_prm2, m_cpx, m_max)\n  DataFrame(;\n    name = nms,\n    dof=dof.(mods),\n    deviance=round.(deviance.(mods), digits=0),\n    AIC=round.(aic.(mods),digits=0),\n    AICc=round.(aicc.(mods),digits=0),\n    BIC=round.(bic.(mods),digits=0),\n    œá¬≤=vcat(:., round.(Int, diff(collect(lrt.lrt.deviance)))),\n    œá¬≤_dof=vcat(:., diff(collect(lrt.lrt.dof))),\n    # StatsBase.PValue includes some pretty-printing methods\n    pvalue=vcat(:., StatsBase.PValue.(collect(lrt.lrt.pval[2:end])))\n  )\nend\n\n4√ó9 DataFrame\n\n\n\nRow\nname\ndof\ndeviance\nAIC\nAICc\nBIC\nœá¬≤\nœá¬≤_dof\npvalue\n\n\n\nSymbol\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nAny\nAny\nAny\n\n\n\n\n1\nm_zcp2\n22\n599486.0\n599530.0\n599530.0\n599725.0\n.\n.\n.\n\n\n2\nm_prm2\n28\n599420.0\n599476.0\n599476.0\n599725.0\n-65\n6\n&lt;1e-11\n\n\n3\nm_cpx\n32\n599418.0\n599482.0\n599482.0\n599766.0\n-2\n4\n0.6521\n\n\n4\nm_max\n53\n599359.0\n599465.0\n599465.0\n599936.0\n-59\n21\n&lt;1e-04\n\n\n\n\n\n\nThe cardinal-related CPs could be removed w/o loss of goodness of fit. However, there is no harm in keeping them in the LMM. The data support both LMM m_prm2 and m_cpx (same as: m_prm1). We keep the slightly more complex LMM m_cpx (m_prm1).",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#residual-over-fitted-plot",
    "href": "kkl15.html#residual-over-fitted-plot",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "8.1 Residual-over-fitted plot",
    "text": "8.1 Residual-over-fitted plot\nThe slant in residuals show a lower and upper boundary of reaction times, that is we have have too few short and too few long residuals. Not ideal, but at least width of the residual band looks similar across the fitted values, that is there is no evidence for heteroskedasticity.\n\n\nCode\nCairoMakie.activate!(; type=\"png\")\nscatter(fitted(m_prm1), residuals(m_prm1); alpha=0.3)\n\n\n\n\n\n\n\nFigure¬†2: Residuals versus fitted values for model m1\n\n\n\n\nWith many observations the scatterplot is not that informative. Contour plots or heatmaps may be an alternative.\n\n\nCode\nset_aog_theme!()\ndraw(\n  data((; f=fitted(m_prm1), r=residuals(m_prm1))) *\n  mapping(\n    :f =&gt; \"Fitted values from m1\", :r =&gt; \"Residuals from m1\"\n  ) *\n  density();\n)\n\n\n\n\n\n\n\nFigure¬†3: Heatmap of residuals versus fitted values for model m1",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#q-q-plot",
    "href": "kkl15.html#q-q-plot",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "8.2 Q-Q plot",
    "text": "8.2 Q-Q plot\nThe plot of quantiles of model residuals over corresponding quantiles of the normal distribution should yield a straight line along the main diagonal.\n\n\nCode\nCairoMakie.activate!(; type=\"png\")\nqqnorm(\n  residuals(m_prm1);\n  qqline=:none,\n  axis=(;\n    xlabel=\"Standard normal quantiles\",\n    ylabel=\"Quantiles of the residuals from model m1\",\n  ),\n)\n\n\n\n\n\n\n\nFigure¬†4: Quantile-quantile plot of the residuals for model m1 versus a standard normal",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#caterpillar-plot",
    "href": "kkl15.html#caterpillar-plot",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "9.1 Caterpillar plot",
    "text": "9.1 Caterpillar plot\n\n\nCode\ncm1 = only(ranefinfo(m_prm1))\ncaterpillar!(Figure(; resolution=(800, 1200)), cm1; orderby=2)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†5: Prediction intervals of the subject random effects in model m1",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#shrinkage-plot",
    "href": "kkl15.html#shrinkage-plot",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "9.2 Shrinkage plot",
    "text": "9.2 Shrinkage plot\n\n\nCode\nshrinkageplot!(Figure(; resolution=(1000, 1200)), m_prm1)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†6: Shrinkage plots of the subject random effects in model m1L",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#generate-a-bootstrap-sample",
    "href": "kkl15.html#generate-a-bootstrap-sample",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "10.1 Generate a bootstrap sample",
    "text": "10.1 Generate a bootstrap sample\nWe generate 2500 samples for the 15 model parameters (4 fixed effect, 7 VCs, 15 CPs, and 1 residual).\n\nsamp = parametricbootstrap(MersenneTwister(1234321), 2500, m_prm1;\n                           optsum_overrides=(; ftol_rel=1e-8));\n\n\ntbl = samp.tbl\n\nTable with 48 columns and 2500 rows:\n      obj        Œ≤01      Œ≤02      Œ≤03      Œ≤04       Œ≤05      Œ≤06      ‚ãØ\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n 1  ‚îÇ 5.99121e5  318.141  26.293   13.9172  6.74045   5.9774   27.2177  ‚ãØ\n 2  ‚îÇ 5.99216e5  315.16   21.7191  13.9016  1.39841   4.28718  26.2128  ‚ãØ\n 3  ‚îÇ 5.99389e5  304.789  22.8434  14.451   1.30953   5.97823  28.3132  ‚ãØ\n 4  ‚îÇ 598759.0   304.984  25.7323  14.2535  4.46265   7.36731  25.5256  ‚ãØ\n 5  ‚îÇ 5.99201e5  305.661  20.3514  11.7943  1.94445   7.55772  22.6509  ‚ãØ\n 6  ‚îÇ 5.99825e5  303.206  20.6355  11.9631  3.00487   3.64828  14.0575  ‚ãØ\n 7  ‚îÇ 5.98849e5  308.688  26.3973  14.9334  4.31493   9.43924  28.0372  ‚ãØ\n 8  ‚îÇ 5.99044e5  308.728  22.0588  14.0064  2.05125   2.35669  21.4596  ‚ãØ\n 9  ‚îÇ 5.99525e5  300.779  22.281   10.5455  4.47888   4.40489  21.3051  ‚ãØ\n 10 ‚îÇ 5.99094e5  314.715  27.6056  12.8147  1.95067   5.65753  24.3927  ‚ãØ\n 11 ‚îÇ 599566.0   313.361  25.4106  11.587   4.60696   9.92416  28.778   ‚ãØ\n 12 ‚îÇ 5.99321e5  312.308  22.9888  15.053   0.255781  8.8578   25.8749  ‚ãØ\n 13 ‚îÇ 5.99502e5  310.985  23.8325  14.7281  1.70401   9.73295  25.5336  ‚ãØ\n 14 ‚îÇ 5.997e5    317.853  25.1802  13.9593  4.47966   8.08207  36.8839  ‚ãØ\n 15 ‚îÇ 6.00081e5  309.606  24.4858  13.2237  2.80406   3.88078  24.2754  ‚ãØ\n 16 ‚îÇ 6.00049e5  308.469  24.8433  10.4011  3.39769   6.34919  28.8349  ‚ãØ\n 17 ‚îÇ 598988.0   313.964  24.6612  12.7279  3.89632   7.20685  27.1067  ‚ãØ\n ‚ãÆ  ‚îÇ     ‚ãÆ         ‚ãÆ        ‚ãÆ        ‚ãÆ        ‚ãÆ         ‚ãÆ        ‚ãÆ     ‚ã±",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#shortest-coverage-interval",
    "href": "kkl15.html#shortest-coverage-interval",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "10.2 Shortest coverage interval",
    "text": "10.2 Shortest coverage interval\n\nconfint(samp)\n\nDictTable with 2 columns and 32 rows:\n par   lower      upper\n ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n Œ≤01 ‚îÇ 297.133    319.552\n Œ≤02 ‚îÇ 18.783     28.3183\n Œ≤03 ‚îÇ 10.2093    15.9687\n Œ≤04 ‚îÇ -1.01263   6.55024\n Œ≤05 ‚îÇ 3.25452    9.79495\n Œ≤06 ‚îÇ 14.0575    37.0171\n Œ≤07 ‚îÇ 1.8633     5.39096\n Œ≤08 ‚îÇ -0.992529  3.75391\n Œ≤09 ‚îÇ -2.88891   1.78051\n Œ≤10 ‚îÇ 3.90978    13.66\n Œ≤11 ‚îÇ -3.31052   2.46331\n Œ≤12 ‚îÇ 3.92582    11.1416\n Œ≤13 ‚îÇ -1.06987   5.61839\n Œ≤14 ‚îÇ -2.50356   1.05728\n Œ≤15 ‚îÇ -2.49599   2.2805\n Œ≤16 ‚îÇ 1.77487    6.49086\n œÅ01 ‚îÇ 0.409532   0.757823\n  ‚ãÆ  ‚îÇ     ‚ãÆ         ‚ãÆ\n\n\nWe can also visualize the shortest coverage intervals for fixed effects with the ridgeplot() command:\n\n\nCode\nridgeplot(samp; show_intercept=false)\n\n\n\n\n\n\n\nFigure¬†7: Ridge plot of fixed-effects bootstrap samples from model m1L",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#comparative-density-plots-of-bootstrapped-parameter-estimates",
    "href": "kkl15.html#comparative-density-plots-of-bootstrapped-parameter-estimates",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "10.3 Comparative density plots of bootstrapped parameter estimates",
    "text": "10.3 Comparative density plots of bootstrapped parameter estimates\n\n10.3.1 Residual\n\n\nCode\ndraw(\n  data(tbl) *\n  mapping(:œÉ =&gt; \"Residual\") *\n  density();\n  figure=(; resolution=(800, 400)),\n)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†8: Kernel density estimate from bootstrap samples of the residual standard deviation for model m_prm1\n\n\n\n\n\n\n10.3.2 Fixed effects and associated variance components (w/o GM)\nThe shortest coverage interval for the GM ranges from x to x ms and the associate variance component from .x to .x. To keep the plot range small we do not include their densities here.\n\n\nCode\nrn = renamer([\n  \"(Intercept)\" =&gt; \"GM\",\n  \"CTR: sod\" =&gt; \"spatial effect\",\n  \"CTR: dos\" =&gt; \"object effect\",\n  \"CTR: dod\" =&gt; \"attraction effect\",\n  \"(Intercept), CTR: sod\" =&gt; \"GM, spatial\",\n  \"(Intercept), CTR: dos\" =&gt; \"GM, object\",\n  \"CTR: sod, CTR: dos\" =&gt; \"spatial, object\",\n  \"(Intercept), CTR: dod\" =&gt; \"GM, attraction\",\n  \"CTR: sod, CTR: dod\" =&gt; \"spatial, attraction\",\n  \"CTR: dos, CTR: dod\" =&gt; \"object, attraction\",\n])\ndraw(\n  data(tbl) *\n  mapping(\n    [:Œ≤02, :Œ≤03, :Œ≤04] .=&gt; \"Experimental effect size [ms]\";\n    color=dims(1) =&gt;\n    renamer([\"spatial effect\", \"object effect\", \"attraction effect\"]) =&gt;\n    \"Experimental effects\",\n  ) *\n  density();\n  figure=(; resolution=(800, 350)),\n)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†9: Kernel density estimate from bootstrap samples of the fixed effects for model m_prm1\n\n\n\n\nThe densitiies correspond nicely with the shortest coverage intervals.\n\n\nCode\ndraw(\n  data(tbl) *\n  mapping(\n    [:œÉ2, :œÉ3, :œÉ4] .=&gt; \"Standard deviations [ms]\";\n    color=dims(1) =&gt;\n    renamer([\"spatial effect\", \"object effect\", \"attraction effect\"]) =&gt;\n    \"Variance components\",\n  ) *\n  density();\n  figure=(; resolution=(800, 350)),\n)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†10: Kernel density estimate from bootstrap samples of the standard deviations for model m1L (excluding Grand Mean)\n\n\n\n\nThe VC are all very nicely defined.\n\n\n10.3.3 Correlation parameters (CPs)\n\n\nCode\ndraw(\n  data(tbl) *\n  mapping(\n    [:œÅ01, :œÅ02, :œÅ03, :œÅ04, :œÅ05, :œÅ06] .=&gt; \"Correlation\";\n    color=dims(1) =&gt;\n    renamer([\"GM, spatial\", \"GM, object\", \"spatial, object\",\n    \"GM, attraction\", \"spatial, attraction\", \"object, attraction\"]) =&gt;\n    \"Correlation parameters\",\n  ) *\n  density();\n  figure=(; resolution=(800, 350)),\n)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†11: Kernel density estimate from bootstrap samples of the standard deviations for model m1L\n\n\n\n\nThree CPs stand out positively, the correlation between GM and the spatial effect, GM and attraction effect, and the correlation between spatial and attraction effects. The second CP was positive, but not significant in the first study. The third CP replicates a CP that was judged questionable in script kwdyz11.jl. The three remaining CPs are not well defined for reaction times.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "largescaledesigned.html",
    "href": "largescaledesigned.html",
    "title": "1 A large-scale designed experiment",
    "section": "",
    "text": "Load the packages to be used.\n\n\nCode\nusing AlgebraOfGraphics\nusing CairoMakie\nusing Chain\nusing DataFrameMacros\nusing DataFrames\nusing Effects\nusing MixedModels\nusing MixedModelsMakie\nusing SMLP2026: dataset\nusing StandardizedPredictors\nusing StatsBase\n\n\nThe English Lexicon Project (Balota et al., 2007) was a large-scale multicenter study to examine properties of English words. It incorporated both a lexical decision task and a word recognition task. Different groups of subjects participated in the different tasks.",
    "crumbs": [
      "Worked examples",
      "A large-scale designed experiment"
    ]
  },
  {
    "objectID": "largescaledesigned.html#trial-level-data-from-the-ldt",
    "href": "largescaledesigned.html#trial-level-data-from-the-ldt",
    "title": "1 A large-scale designed experiment",
    "section": "2.1 Trial-level data from the LDT",
    "text": "2.1 Trial-level data from the LDT\nIn the lexical decision task the study participant is shown a character string, under carefully controlled conditions, and responds according to whether they identify the string as a word or not. Two responses are recorded: whether the choice of word/non-word is correct and the time that elapsed between exposure to the string and registering a decision.\nSeveral covariates, some relating to the subject and some relating to the target, were recorded. Initially we consider only the trial-level data.\n\nldttrial = dataset(:ELP_ldt_trial)\n\nArrow.Table with 2745952 rows, 5 columns, and schema:\n :subj  String\n :seq   Int16\n :acc   Union{Missing, Bool}\n :rt    Int16\n :item  String\n\nwith metadata given by a Base.ImmutableDict{String, String} with 3 entries:\n  \"source\"    =&gt; \"https://osf.io/n63s2\"\n  \"reference\" =&gt; \"Balota et al. (2007), The English Lexicon Project, Behavior R‚Ä¶\n  \"title\"     =&gt; \"Trial-level data from Lexical Discrimination Task in the Engl‚Ä¶\n\n\nThe two response variables are acc - the accuracy of the response - and rt, the response time in milliseconds. There is one trial-level covariate, seq, the sequence number of the trial within subj. Each subject participated in two sessions on different days, with 2000 trials recorded on the first day.\nNotice the metadata with a citation and a URL for the OSF project.\nWe convert to a DataFrame and add a Boolean column s2 which is true for trials in the second session.\n\nldttrial = @transform(DataFrame(ldttrial), :s2 = :seq &gt; 2000)\ndescribe(ldttrial)\n\n6√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nType\n\n\n\n\n1\nsubj\n\nS001\n\nS816\n0\nString\n\n\n2\nseq\n1687.21\n1\n1687.0\n3374\n0\nInt16\n\n\n3\nacc\n0.85604\nfalse\n1.0\ntrue\n1370\nUnion{Missing, Bool}\n\n\n4\nrt\n846.325\n-16160\n732.0\n32061\n0\nInt16\n\n\n5\nitem\n\nAarod\n\nzuss\n0\nString\n\n\n6\ns2\n0.407128\nfalse\n0.0\ntrue\n0\nBool",
    "crumbs": [
      "Worked examples",
      "A large-scale designed experiment"
    ]
  },
  {
    "objectID": "largescaledesigned.html#sec-ldtinitialexplore",
    "href": "largescaledesigned.html#sec-ldtinitialexplore",
    "title": "1 A large-scale designed experiment",
    "section": "2.2 Initial data exploration",
    "text": "2.2 Initial data exploration\nFrom the basic summary of ldttrial we can see that there are some questionable response times ‚Äî negative values and values over 32 seconds.\nBecause of obvious outliers we will use the median response time, which is not strongly influenced by outliers, rather than the mean response time when summarizing by item or by subject.\nAlso, there are missing values of the accuracy. We should check if these are associated with particular subjects or particular items.\n\n2.2.1 Summaries by item\nTo summarize by item we group the trials by item and use combine to produce the various summary statistics. As we will create similar summaries by subject, we incorporate an ‚Äòi‚Äô in the names of these summaries (and an ‚Äòs‚Äô in the name of the summaries by subject) to be able to identify the grouping used.\n\nbyitem = @chain ldttrial begin\n  groupby(:item)\n  @combine(\n    :ni = length(:acc),               # no. of obs\n    :imiss = count(ismissing, :acc),  # no. of missing acc\n    :iacc = count(skipmissing(:acc)), # no. of accurate\n    :imedianrt = median(:rt),\n  )\n  @transform!(\n    :wrdlen = Int8(length(:item)),\n    :ipropacc = :iacc / :ni\n  )\nend\n\n80962√ó7 DataFrame80937 rows omitted\n\n\n\nRow\nitem\nni\nimiss\niacc\nimedianrt\nwrdlen\nipropacc\n\n\n\nString\nInt64\nInt64\nInt64\nFloat64\nInt8\nFloat64\n\n\n\n\n1\na\n35\n0\n26\n743.0\n1\n0.742857\n\n\n2\ne\n35\n0\n19\n824.0\n1\n0.542857\n\n\n3\naah\n34\n0\n21\n770.5\n3\n0.617647\n\n\n4\naal\n34\n0\n32\n702.5\n3\n0.941176\n\n\n5\nAaron\n33\n0\n31\n625.0\n5\n0.939394\n\n\n6\nAarod\n33\n0\n23\n810.0\n5\n0.69697\n\n\n7\naback\n34\n0\n15\n710.0\n5\n0.441176\n\n\n8\nahack\n34\n0\n34\n662.0\n5\n1.0\n\n\n9\nabacus\n34\n0\n17\n671.5\n6\n0.5\n\n\n10\nalacus\n34\n0\n29\n640.0\n6\n0.852941\n\n\n11\nabandon\n34\n0\n32\n641.0\n7\n0.941176\n\n\n12\nacandon\n34\n0\n33\n725.5\n7\n0.970588\n\n\n13\nabandoned\n34\n0\n31\n667.5\n9\n0.911765\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n80951\nzoology\n33\n0\n32\n623.0\n7\n0.969697\n\n\n80952\npoology\n33\n0\n32\n757.0\n7\n0.969697\n\n\n80953\nzoom\n35\n0\n34\n548.0\n4\n0.971429\n\n\n80954\nzool\n35\n0\n30\n633.0\n4\n0.857143\n\n\n80955\nzooming\n33\n0\n29\n617.0\n7\n0.878788\n\n\n80956\nsooming\n33\n0\n30\n721.0\n7\n0.909091\n\n\n80957\nzooms\n33\n0\n30\n598.0\n5\n0.909091\n\n\n80958\ncooms\n33\n0\n31\n660.0\n5\n0.939394\n\n\n80959\nzucchini\n34\n0\n29\n781.5\n8\n0.852941\n\n\n80960\nhucchini\n34\n0\n32\n727.5\n8\n0.941176\n\n\n80961\nZurich\n34\n0\n21\n731.5\n6\n0.617647\n\n\n80962\nZurach\n34\n0\n26\n811.0\n6\n0.764706\n\n\n\n\n\n\nIt can be seen that the items occur in word/nonword pairs and the pairs are sorted alphabetically by the word in the pair (ignoring case). We can add the word/nonword status for the items as\n\nbyitem.isword = isodd.(eachindex(byitem.item))\ndescribe(byitem)\n\n8√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nDataType\n\n\n\n\n1\nitem\n\nAarod\n\nzuss\n0\nString\n\n\n2\nni\n33.9166\n30\n34.0\n37\n0\nInt64\n\n\n3\nimiss\n0.0169215\n0\n0.0\n2\n0\nInt64\n\n\n4\niacc\n29.0194\n0\n31.0\n37\n0\nInt64\n\n\n5\nimedianrt\n753.069\n458.0\n737.5\n1691.0\n0\nFloat64\n\n\n6\nwrdlen\n7.9988\n1\n8.0\n21\n0\nInt8\n\n\n7\nipropacc\n0.855616\n0.0\n0.911765\n1.0\n0\nFloat64\n\n\n8\nisword\n0.5\nfalse\n0.5\ntrue\n0\nBool\n\n\n\n\n\n\nThis table shows that some of the items were never identified correctly. These are\n\nfilter(:iacc =&gt; iszero, byitem)\n\n9√ó8 DataFrame\n\n\n\nRow\nitem\nni\nimiss\niacc\nimedianrt\nwrdlen\nipropacc\nisword\n\n\n\nString\nInt64\nInt64\nInt64\nFloat64\nInt8\nFloat64\nBool\n\n\n\n\n1\nbaobab\n34\n0\n0\n616.5\n6\n0.0\ntrue\n\n\n2\nhaulage\n34\n0\n0\n708.5\n7\n0.0\ntrue\n\n\n3\nleitmotif\n35\n0\n0\n688.0\n9\n0.0\ntrue\n\n\n4\nmiasmal\n35\n0\n0\n774.0\n7\n0.0\ntrue\n\n\n5\npeahen\n34\n0\n0\n684.0\n6\n0.0\ntrue\n\n\n6\nplosive\n34\n0\n0\n663.0\n7\n0.0\ntrue\n\n\n7\nplugugly\n33\n0\n0\n709.0\n8\n0.0\ntrue\n\n\n8\nposhest\n34\n0\n0\n740.0\n7\n0.0\ntrue\n\n\n9\nservo\n33\n0\n0\n697.0\n5\n0.0\ntrue\n\n\n\n\n\n\nNotice that these are all words but somewhat obscure words such that none of the subjects exposed to the word identified it correctly.\nWe can incorporate characteristics like wrdlen and isword back into the original trial table with a ‚Äúleft join‚Äù. This operation joins two tables by values in a common column. It is called a left join because the left (or first) table takes precedence, in the sense that every row in the left table is present in the result. If there is no matching row in the second table then missing values are inserted for the columns from the right table in the result.\n\ndescribe(\n  leftjoin!(\n    ldttrial,\n    select(byitem, :item, :wrdlen, :isword);\n    on=:item,\n  ),\n)\n\n8√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nType\n\n\n\n\n1\nsubj\n\nS001\n\nS816\n0\nString\n\n\n2\nseq\n1687.21\n1\n1687.0\n3374\n0\nInt16\n\n\n3\nacc\n0.85604\nfalse\n1.0\ntrue\n1370\nUnion{Missing, Bool}\n\n\n4\nrt\n846.325\n-16160\n732.0\n32061\n0\nInt16\n\n\n5\nitem\n\nAarod\n\nzuss\n0\nString\n\n\n6\ns2\n0.407128\nfalse\n0.0\ntrue\n0\nBool\n\n\n7\nwrdlen\n7.99835\n1\n8.0\n21\n0\nUnion{Missing, Int8}\n\n\n8\nisword\n0.499995\nfalse\n0.0\ntrue\n0\nUnion{Missing, Bool}\n\n\n\n\n\n\nNotice that the wrdlen and isword variables in this table allow for missing values, because they are derived from the second argument, but there are no missing values for these variables. If there is no need to allow for missing values, there is a slight advantage in disallowing them in the element type, because the code to check for and handle missing values is not needed.\nThis could be done separately for each column or for the whole data frame, as in\n\ndescribe(disallowmissing!(ldttrial; error=false))\n\n8√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nType\n\n\n\n\n1\nsubj\n\nS001\n\nS816\n0\nString\n\n\n2\nseq\n1687.21\n1\n1687.0\n3374\n0\nInt16\n\n\n3\nacc\n0.85604\nfalse\n1.0\ntrue\n1370\nUnion{Missing, Bool}\n\n\n4\nrt\n846.325\n-16160\n732.0\n32061\n0\nInt16\n\n\n5\nitem\n\nAarod\n\nzuss\n0\nString\n\n\n6\ns2\n0.407128\nfalse\n0.0\ntrue\n0\nBool\n\n\n7\nwrdlen\n7.99835\n1\n8.0\n21\n0\nInt8\n\n\n8\nisword\n0.499995\nfalse\n0.0\ntrue\n0\nBool\n\n\n\n\n\n\n\n\n\n\n\n\nNamed argument ‚Äúerror‚Äù\n\n\n\n\n\nThe named argument error=false is required because there is one column, acc, that does incorporate missing values. If error=false were not given then the error thrown when trying to disallowmissing on the acc column would be propagated and the top-level call would fail.\n\n\n\nA barchart of the word length counts, Figure¬†1, shows that the majority of the items are between 3 and 14 characters.\n\n\nCode\nlet\n  wlen = 1:21\n  draw(\n    data((; wrdlen=wlen, count=counts(byitem.wrdlen, wlen))) *\n    mapping(:wrdlen =&gt; \"Length of word\", :count) *\n    visual(BarPlot),\n  )\nend\n\n\n\n\n\n\n\nFigure¬†1: Barchart of word lengths in the items used in the lexical decision task.\n\n\n\n\nTo examine trends in accuracy by word length we plot the proportion accurate versus word-length separately for words and non-words with the area of each marker proportional to the number of observations for that combination (Figure¬†2).\n\n\nCode\nlet\n  itemsummry = combine(\n    groupby(byitem, [:wrdlen, :isword]),\n    :ni =&gt; sum,\n    :imiss =&gt; sum,\n    :iacc =&gt; sum,\n  )\n  @transform!(\n    itemsummry,\n    :iacc_mean = :iacc_sum / (:ni_sum - :imiss_sum)\n  )\n  @transform!(itemsummry, :msz = sqrt((:ni_sum - :imiss_sum) / 800))\n  draw(\n    data(itemsummry) * mapping(\n      :wrdlen =&gt; \"Word length\",\n      :iacc_mean =&gt; \"Proportion accurate\";\n      color=:isword,\n      markersize=:msz,\n    );\n    figure=(; resolution=(800, 450)),\n  )\nend\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†2: Proportion of accurate trials in the LDT versus word length separately for words and non-words. The area of the marker is proportional to the number of observations represented.\n\n\n\n\nThe pattern in the range of word lengths with non-negligible counts (there are points in the plot down to word lengths of 1 and up to word lengths of 21 but these points are very small) is that the accuracy for words is nearly constant at about 84% and the accuracy for nonwords is slightly higher until lengths of 13, at which point it falls off a bit.\n\n\n2.2.2 Summaries by subject\nA summary of accuracy and median response time by subject\n\nbysubj = @chain ldttrial begin\n  groupby(:subj)\n  @combine(\n    :ns = length(:acc),               # no. of obs\n    :smiss = count(ismissing, :acc),  # no. of missing acc\n    :sacc = count(skipmissing(:acc)), # no. of accurate\n    :smedianrt = median(:rt),\n  )\n  @transform!(:spropacc = :sacc / :ns)\nend\n\n814√ó6 DataFrame789 rows omitted\n\n\n\nRow\nsubj\nns\nsmiss\nsacc\nsmedianrt\nspropacc\n\n\n\nString\nInt64\nInt64\nInt64\nFloat64\nFloat64\n\n\n\n\n1\nS001\n3374\n0\n3158\n554.0\n0.935981\n\n\n2\nS002\n3372\n1\n3031\n960.0\n0.898873\n\n\n3\nS003\n3372\n3\n3006\n813.0\n0.891459\n\n\n4\nS004\n3374\n1\n3062\n619.0\n0.907528\n\n\n5\nS005\n3374\n0\n2574\n677.0\n0.762893\n\n\n6\nS006\n3374\n0\n2927\n855.0\n0.867516\n\n\n7\nS007\n3374\n4\n2877\n918.5\n0.852697\n\n\n8\nS008\n3372\n1\n2731\n1310.0\n0.809905\n\n\n9\nS009\n3374\n13\n2669\n657.0\n0.791049\n\n\n10\nS010\n3374\n0\n2722\n757.0\n0.806758\n\n\n11\nS011\n3374\n0\n2894\n632.0\n0.857736\n\n\n12\nS012\n3374\n4\n2979\n692.0\n0.882928\n\n\n13\nS013\n3374\n2\n2980\n1114.0\n0.883225\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n803\nS805\n3374\n5\n2881\n534.0\n0.853883\n\n\n804\nS806\n3374\n1\n3097\n841.5\n0.917902\n\n\n805\nS807\n3374\n3\n2994\n704.0\n0.887374\n\n\n806\nS808\n3374\n2\n2751\n630.5\n0.815353\n\n\n807\nS809\n3372\n4\n2603\n627.0\n0.771945\n\n\n808\nS810\n3374\n1\n3242\n603.5\n0.960877\n\n\n809\nS811\n3374\n2\n2861\n827.0\n0.847955\n\n\n810\nS812\n3372\n6\n3012\n471.0\n0.893238\n\n\n811\nS813\n3372\n4\n2932\n823.0\n0.869514\n\n\n812\nS814\n3374\n1\n3070\n773.0\n0.909899\n\n\n813\nS815\n3374\n1\n3024\n602.0\n0.896266\n\n\n814\nS816\n3374\n0\n2950\n733.0\n0.874333\n\n\n\n\n\n\nshows some anomalies\n\ndescribe(bysubj)\n\n6√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nDataType\n\n\n\n\n1\nsubj\n\nS001\n\nS816\n0\nString\n\n\n2\nns\n3373.41\n3370\n3374.0\n3374\n0\nInt64\n\n\n3\nsmiss\n1.68305\n0\n1.0\n22\n0\nInt64\n\n\n4\nsacc\n2886.33\n1727\n2928.0\n3286\n0\nInt64\n\n\n5\nsmedianrt\n760.992\n205.0\n735.0\n1804.0\n0\nFloat64\n\n\n6\nspropacc\n0.855613\n0.511855\n0.868031\n0.973918\n0\nFloat64\n\n\n\n\n\n\nFirst, some subjects are accurate on only about half of their trials, which is the proportion that would be expected from random guessing. A plot of the median response time versus proportion accurate, Figure¬†3, shows that the subjects with lower accuracy are some of the fastest responders, further indicating that these subjects are sacrificing accuracy for speed.\n\n\nCode\ndraw(\n  data(bysubj) *\n  mapping(\n    :spropacc =&gt; \"Proportion accurate\",\n    :smedianrt =&gt; \"Median response time (ms)\",\n  ) *\n  (visual(Scatter) + smooth())\n)\n\n\n\n\n\n\n\nFigure¬†3: Median response time versus proportion accurate by subject in the LDT.\n\n\n\n\nAs described in Balota et al. (2007), the participants performed the trials in blocks of 250 followed by a short break. During the break they were given feedback concerning accuracy and response latency in the previous block of trials. If the accuracy was less than 80% the participant was encouraged to improve their accuracy. Similarly, if the mean response latency was greater than 1000 ms, the participant was encouraged to decrease their response time. During the trials immediate feedback was given if the response was incorrect.\nNevertheless, approximately 15% of the subjects were unable to maintain 80% accuracy on their trials\n\ncount(&lt;(0.8), bysubj.spropacc) / nrow(bysubj)\n\n0.15233415233415235\n\n\nand there is some association of faster response times with low accuracy. The majority of the subjects whose median response time is less than 500 ms. are accurate on less than 75% of their trials. Another way of characterizing the relationship is that none of the subjects with 90% accuracy or greater had a median response time less than 500 ms.\n\nminimum(@subset(bysubj, :spropacc &gt; 0.9).smedianrt)\n\n505.0\n\n\nIt is common in analyses of response latency in a lexical discrimination task to consider only the latencies on correct identifications and to trim outliers. In Balota et al. (2007) a two-stage outlier removal strategy was used; first removing responses less than 200 ms or greater than 3000 ms then removing responses more than three standard deviations from the participant‚Äôs mean response.\nAs described in Section¬†2.2.3 we will analyze these data on a speed scale (the inverse of response time) using only the first-stage outlier removal of response latencies less than 200 ms or greater than 3000 ms. On the speed scale the limits are 0.333 per second up to 5 per second.\nTo examine the effects of the fast but inaccurate responders we will fit models to the data from all the participants and to the data from the 85% of participants who maintained an overall accuracy of 80% or greater.\n\npruned = @chain ldttrial begin\n  @subset(!ismissing(:acc), 200 ‚â§ :rt ‚â§ 3000,)\n  leftjoin!(select(bysubj, :subj, :spropacc); on=:subj)\n  dropmissing!\nend\nsize(pruned)\n\n(2714311, 9)\n\n\n\ndescribe(pruned)\n\n9√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nDataType\n\n\n\n\n1\nsubj\n\nS001\n\nS816\n0\nString\n\n\n2\nseq\n1684.56\n1\n1684.0\n3374\n0\nInt16\n\n\n3\nacc\n0.859884\nfalse\n1.0\ntrue\n0\nBool\n\n\n4\nrt\n838.712\n200\n733.0\n3000\n0\nInt16\n\n\n5\nitem\n\nAarod\n\nzuss\n0\nString\n\n\n6\ns2\n0.40663\nfalse\n0.0\ntrue\n0\nBool\n\n\n7\nwrdlen\n7.99244\n1\n8.0\n21\n0\nInt8\n\n\n8\nisword\n0.500126\nfalse\n1.0\ntrue\n0\nBool\n\n\n9\nspropacc\n0.857169\n0.511855\n0.869295\n0.973918\n0\nFloat64\n\n\n\n\n\n\n\n\n2.2.3 Choice of response scale\nAs we have indicated, generally the response times are analyzed for the correct identifications only. Furthermore, unrealistically large or small response times are eliminated. For this example we only use the responses between 200 and 3000 ms.\nA density plot of the pruned response times, Figure¬†4, shows they are skewed to the right.\n\n\nCode\ndraw(\n  data(pruned) *\n  mapping(:rt =&gt; \"Response time (ms.) for correct responses\") *\n  AlgebraOfGraphics.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†4: Kernel density plot of the pruned response times (ms.) in the LDT.\n\n\n\n\nIn such cases it is common to transform the response to a scale such as the logarithm of the response time or to the speed of the response, which is the inverse of the response time.\nThe density of the response speed, in responses per second, is shown in Figure¬†5.\n\n\nCode\ndraw(\n  data(pruned) *\n  mapping(\n    :rt =&gt; (x -&gt; 1000 / x) =&gt; \"Response speed (s‚Åª¬π) for correct responses\") *\n  AlgebraOfGraphics.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†5: Kernel density plot of the pruned response speed in the LDT.\n\n\n\n\nFigure¬†4 and Figure¬†5 indicate that it may be more reasonable to establish a lower bound of 1/3 second (333 ms) on the response latency, corresponding to an upper bound of 3 per second on the response speed. However, only about one half of one percent of the correct responses have latencies in the range of 200 ms. to 333 ms.\n\ncount(\n  r -&gt; !ismissing(r.acc) && 200 &lt; r.rt &lt; 333,\n  eachrow(ldttrial),\n) / count(!ismissing, ldttrial.acc)\n\n0.005867195806137328\n\n\nso the exact position of the lower cut-off point on the response latencies is unlikely to be very important.\n\n\n\n\n\n\nUsing inline transformations vs defining new columns\n\n\n\n\n\nIf you examine the code for Figure¬†5, you will see that the conversion from rt to speed is done inline rather than creating and storing a new variable in the DataFrame.\nI prefer to keep the DataFrame simple with the integer variables (e.g.¬†:rt) if possible.\nI recommend using the StandardizedPredictors.jl capabilities to center numeric variables or convert to zscores.\n\n\n\n\n\n2.2.4 Transformation of response and the form of the model\nAs noted in Box & Cox (1964), a transformation of the response that produces a more Gaussian distribution often will also produce a simpler model structure. For example, Figure¬†6 shows the smoothed relationship between word length and response time for words and non-words separately,\n\n\nCode\ndraw(\n  data(pruned) *\n  mapping(\n    :wrdlen =&gt; \"Word length\",\n    :rt =&gt; \"Response time (ms)\";\n    :color =&gt; :isword,\n  ) * smooth()\n)\n\n\n\n\n\n\n\nFigure¬†6: Scatterplot smooths of response time versus word length in the LDT.\n\n\n\n\nand Figure¬†7 shows the similar relationships for speed\n\n\nCode\ndraw(\n  data(pruned) *\n  mapping(\n    :wrdlen =&gt; \"Word length\",\n    :rt =&gt; (x -&gt; 1000/x) =&gt; \"Speed of response (s‚Åª¬π)\";\n    :color =&gt; :isword,\n  ) * smooth()\n)\n\n\n\n\n\n\n\nFigure¬†7: Scatterplot smooths of response speed versus word length in the LDT.\n\n\n\n\nFor the most part the smoother lines in Figure¬†7 are reasonably straight. The small amount of curvature is associated with short word lengths, say less than 4 characters, of which there are comparatively few in the study.\nFigure¬†8 shows a ‚Äúviolin plot‚Äù - the empirical density of the response speed by word length separately for words and nonwords. The lines on the plot are fit by linear regression.\n\n\nCode\nlet\n  plt = data(@subset(pruned, :wrdlen &gt; 3, :wrdlen &lt; 14))\n  plt *= mapping(\n    :wrdlen =&gt; \"Word length\",\n    :rt =&gt; (x -&gt; 1000/x) =&gt; \"Speed of response (s‚Åª¬π)\",\n    color=:isword,\n    side=:isword,\n  )\n  plt *= visual(Violin)\n  draw(plt, axis=(; limits=(nothing, (0.0, 2.8))))\nend\n\n\n\n\n\n\n\nFigure¬†8: Empirical density of response speed versus word length by word/non-word status.",
    "crumbs": [
      "Worked examples",
      "A large-scale designed experiment"
    ]
  },
  {
    "objectID": "largescaledesigned.html#sec-ldtinitialmodel",
    "href": "largescaledesigned.html#sec-ldtinitialmodel",
    "title": "1 A large-scale designed experiment",
    "section": "2.3 Models with scalar random effects",
    "text": "2.3 Models with scalar random effects\nA major purpose of the English Lexicon Project is to characterize the items (words or nonwords) according to the observed accuracy of identification and to response latency, taking into account subject-to-subject variability, and to relate these to lexical characteristics of the items.\nIn Balota et al. (2007) the item response latency is characterized by the average response latency from the correct trials after outlier removal.\nMixed-effects models allow us greater flexibility and, we hope, precision in characterizing the items by controlling for subject-to-subject variability and for item characteristics such as word/nonword and item length.\nWe begin with a model that has scalar random effects for item and for subject and incorporates fixed-effects for word/nonword and for item length and for the interaction of these terms.\n\n2.3.1 Establish the contrasts\nFor the isword factor we will use an EffectsCoding contrast with the base level as false. The non-words are assigned -1 in this contrast and the words are assigned +1. The wrdlen covariate is on its original scale but centered at 8 characters.\nThus the (Intercept) coefficient is the predicted speed of response for a typical subject and typical item (without regard to word/non-word status) of 8 characters.\nSet these contrasts\n\ncontrasts = Dict(\n  :isword =&gt; EffectsCoding(; base=false),\n  :wrdlen =&gt; Center(8),\n)\n\nDict{Symbol, Any} with 2 entries:\n  :wrdlen =&gt; Center(8)\n  :isword =&gt; EffectsCoding(false, nothing)\n\n\nand fit a first model with simple, scalar, random effects for subj and item.\n\nelm01 = let\n  form = @formula(\n    1000 / rt ~ 1 + isword * wrdlen + (1 | item) + (1 | subj)\n  )\n  fit(MixedModel, form, pruned; contrasts, progress=false)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n(Intercept)\n1.3758\n0.0090\n153.69\n&lt;1e-99\n0.1185\n0.2550\n\n\nisword: true\n0.0625\n0.0005\n131.35\n&lt;1e-99\n\n\n\n\nwrdlen(centered: 8)\n-0.0436\n0.0002\n-225.38\n&lt;1e-99\n\n\n\n\nisword: true & wrdlen(centered: 8)\n-0.0056\n0.0002\n-28.83\n&lt;1e-99\n\n\n\n\nResidual\n0.3781\n\n\n\n\n\n\n\n\n\n\nThe predicted response speed by word length and word/nonword status can be summarized as\n\neffects(Dict(:isword =&gt; [false, true], :wrdlen =&gt; 4:2:12), elm01)\n\n10√ó6 DataFrame\n\n\n\nRow\nwrdlen\nisword\n1000 / rt\nerr\nlower\nupper\n\n\n\nInt64\nBool\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n4\nfalse\n1.46555\n0.00903118\n1.45652\n1.47458\n\n\n2\n6\nfalse\n1.38947\n0.00898132\n1.38049\n1.39845\n\n\n3\n8\nfalse\n1.31338\n0.00896467\n1.30442\n1.32235\n\n\n4\n10\nfalse\n1.2373\n0.00898141\n1.22832\n1.24628\n\n\n5\n12\nfalse\n1.16121\n0.00903137\n1.15218\n1.17025\n\n\n6\n4\ntrue\n1.6351\n0.00903118\n1.62607\n1.64413\n\n\n7\n6\ntrue\n1.5367\n0.00898132\n1.52772\n1.54569\n\n\n8\n8\ntrue\n1.43831\n0.00896467\n1.42934\n1.44727\n\n\n9\n10\ntrue\n1.33991\n0.00898141\n1.33092\n1.34889\n\n\n10\n12\ntrue\n1.24151\n0.00903135\n1.23248\n1.25054\n\n\n\n\n\n\nIf we restrict to only those subjects with 80% accuracy or greater the model becomes\n\nelm02 = let\n  form = @formula(\n    1000 / rt ~ 1 + isword * wrdlen + (1 | item) + (1 | subj)\n  )\n  dat = @subset(pruned, :spropacc &gt; 0.8)\n  fit(MixedModel, form, dat; contrasts, progress=false)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n(Intercept)\n1.3611\n0.0088\n153.99\n&lt;1e-99\n0.1247\n0.2318\n\n\nisword: true\n0.0656\n0.0005\n133.73\n&lt;1e-99\n\n\n\n\nwrdlen(centered: 8)\n-0.0444\n0.0002\n-222.65\n&lt;1e-99\n\n\n\n\nisword: true & wrdlen(centered: 8)\n-0.0057\n0.0002\n-28.73\n&lt;1e-99\n\n\n\n\nResidual\n0.3342\n\n\n\n\n\n\n\n\n\n\n\neffects(Dict(:isword =&gt; [false, true], :wrdlen =&gt; 4:2:12), elm02)\n\n10√ó6 DataFrame\n\n\n\nRow\nwrdlen\nisword\n1000 / rt\nerr\nlower\nupper\n\n\n\nInt64\nBool\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n4\nfalse\n1.45036\n0.00892446\n1.44144\n1.45929\n\n\n2\n6\nfalse\n1.37297\n0.0088708\n1.3641\n1.38184\n\n\n3\n8\nfalse\n1.29557\n0.00885288\n1.28672\n1.30443\n\n\n4\n10\nfalse\n1.21818\n0.0088709\n1.20931\n1.22705\n\n\n5\n12\nfalse\n1.14078\n0.00892466\n1.13186\n1.14971\n\n\n6\n4\ntrue\n1.62735\n0.00892445\n1.61842\n1.63627\n\n\n7\n6\ntrue\n1.52702\n0.0088708\n1.51815\n1.53589\n\n\n8\n8\ntrue\n1.4267\n0.00885288\n1.41784\n1.43555\n\n\n9\n10\ntrue\n1.32637\n0.00887089\n1.3175\n1.33524\n\n\n10\n12\ntrue\n1.22605\n0.00892463\n1.21712\n1.23497\n\n\n\n\n\n\nThe differences in the fixed-effects parameter estimates between a model fit to the full data set and one fit to the data from accurate responders only, are small.\nHowever, the random effects for the item, while highly correlated, are not perfectly correlated.\n\n\nCode\nCairoMakie.activate!(; type=\"png\")\ndisallowmissing!(\n  leftjoin!(\n    byitem,\n    leftjoin!(\n      rename!(DataFrame(raneftables(elm01)[:item]), [:item, :elm01]),\n      rename!(DataFrame(raneftables(elm02)[:item]), [:item, :elm02]);\n      on=:item,\n    ),\n    on=:item,\n  ),\n)\ndisallowmissing!(\n  leftjoin!(\n    bysubj,\n    leftjoin!(\n      rename!(DataFrame(raneftables(elm01)[:subj]), [:subj, :elm01]),\n      rename!(DataFrame(raneftables(elm02)[:subj]), [:subj, :elm02]);\n      on=:subj,\n    ),\n    on=:subj,\n  ); error=false,\n)\ndraw(\n  data(byitem) * mapping(\n    :elm01 =&gt; \"Conditional means of item random effects for model elm01\",\n    :elm02 =&gt; \"Conditional means of item random effects for model elm02\";\n    color=:isword,\n  ) * visual(Scatter; alpha=0.2);\n  axis=(; width=600, height=600),\n)\n\n\n\n\n\n\n\nFigure¬†9: Conditional means of scalar random effects for item in model elm01, fit to the pruned data, versus those for model elm02, fit to the pruned data with inaccurate subjects removed.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAdjust the alpha on Figure¬†9.\n\n\nFigure¬†9 is exactly of the form that would be expected in a sample from a correlated multivariate Gaussian distribution. The correlation of the two sets of conditional means is about 96%.\n\ncor(Matrix(select(byitem, :elm01, :elm02)))\n\n2√ó2 Matrix{Float64}:\n 1.0       0.958655\n 0.958655  1.0\n\n\nThese models take only a few seconds to fit on a modern laptop computer, which is quite remarkable given the size of the data set and the number of random effects.\nThe amount of time to fit more complex models will be much greater so we may want to move those fits to more powerful server computers. We can split the tasks of fitting and analyzing a model between computers by saving the optimization summary after the model fit and later creating the MixedModel object followed by restoring the optsum object.\n\nif !isfile(\"./fits/elm01.json\")\n  saveoptsum(\"./fits/elm01.json\", elm01);\nend\n\n\nelm01a = restoreoptsum!(\n  let\n    form = @formula(\n      1000 / rt ~ 1 + isword * wrdlen + (1 | item) + (1 | subj)\n    )\n    MixedModel(form, pruned; contrasts)\n  end,\n  \"./fits/elm01.json\",\n)\n\n\n‚îå Warning: optsum was saved with an older version of MixedModels.jl: consider resaving.\n‚îî @ MixedModels ~/.julia/packages/MixedModels/L0NHA/src/serialization.jl:91\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n(Intercept)\n1.3758\n0.0090\n153.69\n&lt;1e-99\n0.1185\n0.2550\n\n\nisword: true\n0.0625\n0.0005\n131.35\n&lt;1e-99\n\n\n\n\nwrdlen(centered: 8)\n-0.0436\n0.0002\n-225.38\n&lt;1e-99\n\n\n\n\nisword: true & wrdlen(centered: 8)\n-0.0056\n0.0002\n-28.83\n&lt;1e-99\n\n\n\n\nResidual\n0.3781\n\n\n\n\n\n\n\n\n\n\nOther covariates associated with the item are available as\n\nelpldtitem = DataFrame(dataset(\"ELP_ldt_item\"))\ndescribe(elpldtitem)\n\n9√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nUnion‚Ä¶\nAny\nInt64\nType\n\n\n\n\n1\nitem\n\nAarod\n\nzuss\n0\nString\n\n\n2\nOrtho_N\n1.53309\n0\n1.0\n25\n0\nInt8\n\n\n3\nBG_Sum\n13938.4\n11\n13026.0\n59803\n177\nUnion{Missing, Int32}\n\n\n4\nBG_Mean\n1921.25\n5.5\n1907.0\n6910.0\n177\nUnion{Missing, Float32}\n\n\n5\nBG_Freq_By_Pos\n2043.08\n0\n1928.0\n6985\n4\nUnion{Missing, Int16}\n\n\n6\nitemno\n40481.5\n1\n40481.5\n80962\n0\nInt32\n\n\n7\nisword\n0.5\nfalse\n0.5\ntrue\n0\nBool\n\n\n8\nwrdlen\n7.9988\n1\n8.0\n21\n0\nInt8\n\n\n9\npairno\n20241.0\n1\n20241.0\n40481\n0\nInt32\n\n\n\n\n\n\nand those associated with the subject are\n\nelpldtsubj = DataFrame(dataset(\"ELP_ldt_subj\"))\ndescribe(elpldtsubj)\n\n20√ó7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion‚Ä¶\nAny\nAny\nAny\nInt64\nType\n\n\n\n\n1\nsubj\n\nS001\n\nS816\n0\nString\n\n\n2\nuniv\n\nKansas\n\nWayne State\n0\nString\n\n\n3\nsex\n\nf\n\nm\n8\nUnion{Missing, String}\n\n\n4\nDOB\n\n1938-06-07\n\n1984-11-14\n0\nDate\n\n\n5\nMEQ\n44.4932\n19.0\n44.0\n75.0\n8\nUnion{Missing, Float32}\n\n\n6\nvision\n5.51169\n0\n6.0\n7\n1\nUnion{Missing, Int8}\n\n\n7\nhearing\n5.86101\n0\n6.0\n7\n1\nUnion{Missing, Int8}\n\n\n8\neducatn\n8.89681\n1\n12.0\n28\n0\nInt8\n\n\n9\nncorrct\n29.8505\n5\n30.0\n40\n18\nUnion{Missing, Int8}\n\n\n10\nrawscor\n31.9925\n13\n32.0\n40\n18\nUnion{Missing, Int8}\n\n\n11\nvocabAge\n17.8123\n10.3\n17.8\n21.0\n19\nUnion{Missing, Float32}\n\n\n12\nshipTime\n3.0861\n0\n3.0\n9\n1\nUnion{Missing, Int8}\n\n\n13\nreadTime\n2.50215\n0.0\n2.0\n15.0\n1\nUnion{Missing, Float32}\n\n\n14\npreshlth\n5.48708\n0\n6.0\n7\n1\nUnion{Missing, Int8}\n\n\n15\npasthlth\n4.92989\n0\n5.0\n7\n1\nUnion{Missing, Int8}\n\n\n16\nS1start\n\n2001-03-16T13:49:27\n2001-10-16T11:38:28.500\n2003-07-29T18:48:44\n0\nDateTime\n\n\n17\nS2start\n\n2001-03-19T10:00:35\n2001-10-19T14:24:19.500\n2003-07-30T13:07:45\n0\nDateTime\n\n\n18\nMEQstrt\n\n2001-03-22T18:32:00\n2001-10-23T11:26:13\n2003-07-30T14:30:49\n7\nUnion{Missing, DateTime}\n\n\n19\nfilename\n\n101DATA.LDT\n\nData998.LDT\n0\nString\n\n\n20\nfrstLang\n\nEnglish\n\nother\n8\nUnion{Missing, String}\n\n\n\n\n\n\nFor the simple model elm01 the estimated standard deviation of the random effects for subject is greater than that of the random effects for item, a common occurrence. A caterpillar plot, Figure¬†10,\n\n\nCode\nqqcaterpillar!(\n  Figure(resolution=(800, 650)),\n  ranefinfo(elm01, :subj),\n)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\n\nFigure¬†10: Conditional means and 95% prediction intervals for subject random effects in elm01.\n\n\n\n\n\nshows definite distinctions between subjects because the widths of the prediction intervals are small compared to the range of the conditional modes. Also, there is at least one outlier with a conditional mode over 1.0.\nFigure¬†11 is the corresponding caterpillar plot for model elm02 fit to the data with inaccurate responders eliminated.\n\n\nCode\nqqcaterpillar!(\n  Figure(resolution=(800, 650)),\n  ranefinfo(elm02, :subj),\n)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\n\nFigure¬†11: Conditional means and 95% prediction intervals for subject random effects in elm02.",
    "crumbs": [
      "Worked examples",
      "A large-scale designed experiment"
    ]
  },
  {
    "objectID": "largescaledesigned.html#random-effects-from-the-simple-model-related-to-covariates",
    "href": "largescaledesigned.html#random-effects-from-the-simple-model-related-to-covariates",
    "title": "1 A large-scale designed experiment",
    "section": "2.4 Random effects from the simple model related to covariates",
    "text": "2.4 Random effects from the simple model related to covariates\nThe random effects ‚Äúestimates‚Äù (technically they are ‚Äúconditional means‚Äù) from the simple model elm01 provide a measure of how much the item or subject differs from the population. (We use elm01 because the main difference between elm01 and elm02 are that some subjects were dropped before fitting elm02.)\nFor the item its length and word/non-word status have already been incorporated in the model. At this point the subjects are just being treated as a homogeneous population.\nThe random effects conditional means have been extracted and incorporated in the byitem and bysubj tables. Now add selected demographic and item-specific measures.\n\nitemextended = leftjoin(\n  byitem,\n  select(elpldtitem, 1:5);\n  on = :item,\n)\nsubjextended = leftjoin(\n  bysubj,\n  select(elpldtsubj, 1:3, :vocabAge);\n  on=:subj,\n)\n\n814√ó11 DataFrame789 rows omitted\n\n\n\nRow\nsubj\nns\nsmiss\nsacc\nsmedianrt\nspropacc\nelm01\nelm02\nuniv\nsex\nvocabAge\n\n\n\nString\nInt64\nInt64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64?\nString?\nString?\nFloat32?\n\n\n\n\n1\nS001\n3374\n0\n3158\n554.0\n0.935981\n0.411459\n0.426624\nMorehead\nm\n19.8\n\n\n2\nS002\n3372\n1\n3031\n960.0\n0.898873\n-0.30907\n-0.293732\nMorehead\nf\n17.8\n\n\n3\nS003\n3372\n3\n3006\n813.0\n0.891459\n-0.153078\n-0.139436\nMorehead\nf\n18.2\n\n\n4\nS004\n3374\n1\n3062\n619.0\n0.907528\n0.213047\n0.227539\nMorehead\nf\n18.6\n\n\n5\nS005\n3374\n0\n2574\n677.0\n0.762893\n0.0850349\nmissing\nMorehead\nf\n16.2\n\n\n6\nS006\n3374\n0\n2927\n855.0\n0.867516\n-0.207356\n-0.192651\nMorehead\nf\n17.8\n\n\n7\nS007\n3374\n4\n2877\n918.5\n0.852697\n-0.182201\n-0.166357\nMorehead\nf\n17.4\n\n\n8\nS008\n3372\n1\n2731\n1310.0\n0.809905\n-0.541434\n-0.526828\nMorehead\nm\n16.2\n\n\n9\nS009\n3374\n13\n2669\n657.0\n0.791049\n0.154926\nmissing\nMorehead\nf\n16.6\n\n\n10\nS010\n3374\n0\n2722\n757.0\n0.806758\n-0.0541104\n-0.0403266\nMorehead\nf\n17.0\n\n\n11\nS011\n3374\n0\n2894\n632.0\n0.857736\n0.217734\n0.231618\nMorehead\nf\n17.4\n\n\n12\nS012\n3374\n4\n2979\n692.0\n0.882928\n0.062351\n0.0770981\nMorehead\nm\n18.2\n\n\n13\nS013\n3374\n2\n2980\n1114.0\n0.883225\n-0.409761\n-0.3956\nMorehead\nf\n18.2\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n803\nS805\n3374\n5\n2881\n534.0\n0.853883\n0.480461\n0.495683\nWash. Univ\nm\n19.0\n\n\n804\nS806\n3374\n1\n3097\n841.5\n0.917902\n-0.1888\n-0.173376\nWash. Univ\nm\n19.8\n\n\n805\nS807\n3374\n3\n2994\n704.0\n0.887374\n0.01919\n0.0338241\nWash. Univ\nm\n17.4\n\n\n806\nS808\n3374\n2\n2751\n630.5\n0.815353\n0.199416\n0.214299\nWash. Univ\nf\n18.6\n\n\n807\nS809\n3372\n4\n2603\n627.0\n0.771945\n0.2277\nmissing\nWash. Univ\nm\n15.1\n\n\n808\nS810\n3374\n1\n3242\n603.5\n0.960877\n0.252522\n0.266822\nWash. Univ\nm\n19.8\n\n\n809\nS811\n3374\n2\n2861\n827.0\n0.847955\n-0.158097\n-0.143568\nWash. Univ\nf\n16.2\n\n\n810\nS812\n3372\n6\n3012\n471.0\n0.893238\n0.748427\n0.76354\nWash. Univ\nf\n19.8\n\n\n811\nS813\n3372\n4\n2932\n823.0\n0.869514\n-0.167166\n-0.153846\nWash. Univ\nm\n17.4\n\n\n812\nS814\n3374\n1\n3070\n773.0\n0.909899\n-0.0753662\n-0.0606956\nWash. Univ\nf\n18.6\n\n\n813\nS815\n3374\n1\n3024\n602.0\n0.896266\n0.249134\n0.2643\nWash. Univ\nf\n18.6\n\n\n814\nS816\n3374\n0\n2950\n733.0\n0.874333\n-0.0364596\n-0.0222916\nWash. Univ\nf\n17.8\n\n\n\n\n\n\nAs shown in Figure¬†12, there does not seem to be a strong relationship between vocabulary age and speed of response by subject.\n\n\nCode\ndraw(\n  data(dropmissing(select(subjextended, :elm01, :vocabAge, :sex))) *\n  mapping(\n    :vocabAge =&gt; \"Vocabulary age (yr) of subject\",\n    :elm01 =&gt; \"Random effect in model elm01\";\n    color=:sex,\n  ) * visual(Scatter; alpha=0.6)\n)\n\n\n\n\n\n\n\n\nFigure¬†12: Random effect for subject in model elm01 versus vocabulary age\n\n\n\n\n\n\n\nCode\ndraw(\n  data(dropmissing(select(subjextended, :elm01, :univ))) *\n  mapping(\n    :elm01 =&gt; \"Random effect in model elm01\";\n    color=:univ =&gt; \"University\",\n  ) * AlgebraOfGraphics.density()\n)\n\n\n\n\n\n\n\n\nFigure¬†13: Estimated density of random effects for subject in model elm01 by university\n\n\n\n\n\n\n\nCode\ndraw(\n  data(dropmissing(select(subjextended, :elm02, :univ))) *\n  mapping(\n    :elm02 =&gt; \"Random effect in model elm02 (accurate responders only)\";\n    color=:univ =&gt; \"University\",\n  ) * AlgebraOfGraphics.density()\n)\n\n\n\n\n\n\n\n\nFigure¬†14: Estimated density of random effects for subject in model elm02, fit to accurate responders only, by university\n\n\n\n\n\n\n\nCode\nCairoMakie.activate!(; type=\"png\")\ndraw(\n  data(dropmissing(select(itemextended, :elm01, :BG_Mean, :isword))) *\n  mapping(\n    :BG_Mean =&gt; \"Mean bigram frequency\",\n    :elm01 =&gt; \"Random effect in model elm01\";\n    color=:isword,\n  ) * visual(Scatter; alpha=0.2)\n)\n\n\n\n\n\n\n\nFigure¬†15: Random effect in model elm01 versus mean bigram frequency, by word/nonword status\n\n\n\n\nThis page was rendered from git revision fb99b3c\n using Quarto 1.7.33.",
    "crumbs": [
      "Worked examples",
      "A large-scale designed experiment"
    ]
  },
  {
    "objectID": "partial_within.html",
    "href": "partial_within.html",
    "title": "Partially-within subjects designs",
    "section": "",
    "text": "Begin by loading the packages to be used.\n\n\nCode\nusing AlgebraOfGraphics\nusing CairoMakie\nusing DataFrames\nusing MixedModels\nusing MixedModelsMakie\nusing MixedModelsSim\nusing ProgressMeter\nusing Random\n\nconst progress=false\n\n\n\n\nCode\nn_subj = 40\nn_item = 3\n# things are expressed as \"between\", so \"within subjects\" is \"between items\"\nitem_btwn = Dict(:frequency =&gt; [\"high\", \"medium\", \"low\"])\ndesign = simdat_crossed(MersenneTwister(42), n_subj, n_item;\n                        item_btwn = item_btwn)\ndesign = DataFrame(design)\n\n\n120√ó4 DataFrame95 rows omitted\n\n\n\nRow\nsubj\nitem\nfrequency\ndv\n\n\n\nString\nString\nString\nFloat64\n\n\n\n\n1\nS01\nI1\nhigh\n1.21028\n\n\n2\nS02\nI1\nhigh\n-0.0789986\n\n\n3\nS03\nI1\nhigh\n0.403512\n\n\n4\nS04\nI1\nhigh\n0.289951\n\n\n5\nS05\nI1\nhigh\n-0.0670422\n\n\n6\nS06\nI1\nhigh\n0.61398\n\n\n7\nS07\nI1\nhigh\n1.01694\n\n\n8\nS08\nI1\nhigh\n-0.053864\n\n\n9\nS09\nI1\nhigh\n-0.570387\n\n\n10\nS10\nI1\nhigh\n-0.857737\n\n\n11\nS11\nI1\nhigh\n-0.211452\n\n\n12\nS12\nI1\nhigh\n-0.853673\n\n\n13\nS13\nI1\nhigh\n0.439568\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n109\nS29\nI3\nlow\n-0.358987\n\n\n110\nS30\nI3\nlow\n0.274254\n\n\n111\nS31\nI3\nlow\n-1.87219\n\n\n112\nS32\nI3\nlow\n0.480454\n\n\n113\nS33\nI3\nlow\n0.778338\n\n\n114\nS34\nI3\nlow\n0.330643\n\n\n115\nS35\nI3\nlow\n-0.516103\n\n\n116\nS36\nI3\nlow\n0.156547\n\n\n117\nS37\nI3\nlow\n0.427122\n\n\n118\nS38\nI3\nlow\n-0.291824\n\n\n119\nS39\nI3\nlow\n-0.965346\n\n\n120\nS40\nI3\nlow\n-0.326028\n\n\n\n\n\n\n\n\nCode\nunique!(select(design, :item, :frequency))\n\n\n3√ó2 DataFrame\n\n\n\nRow\nitem\nfrequency\n\n\n\nString\nString\n\n\n\n\n1\nI1\nhigh\n\n\n2\nI2\nmedium\n\n\n3\nI3\nlow\n\n\n\n\n\n\n\n\nCode\nm0 = let contrasts, form\n    contrasts = Dict(:frequency =&gt; HelmertCoding(base=\"high\"))\n    form = @formula(dv ~ 1 + frequency +\n                    (1 + frequency | subj))\n    fit(MixedModel, form, design; contrasts, progress)\nend\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\n\n\n(Intercept)\n0.1366\n0.1000\n1.37\n0.1719\n0.5895\n\n\nfrequency: low\n-0.1053\n0.0964\n-1.09\n0.2747\n0.5412\n\n\nfrequency: medium\n0.0594\n0.0684\n0.87\n0.3849\n0.4008\n\n\nResidual\n0.3969\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorrmat = [ 1    0.1 -0.2\n            0.1  1    0.1\n           -0.2  0.1  1 ]\nre_subj = create_re(1.2, 1.5, 1.5; corrmat)\n\n\n3√ó3 LinearAlgebra.LowerTriangular{Float64, Matrix{Float64}}:\n  1.2    ‚ãÖ         ‚ãÖ \n  0.15  1.49248    ‚ãÖ \n -0.3   0.180907  1.45852\n\n\n\n\nCode\nŒ∏ = createŒ∏(m0; subj=re_subj)\n\n\n6-element Vector{Float64}:\n  1.2\n  0.15000000000000002\n -0.30000000000000004\n  1.49248115565993\n  0.18090680674665818\n  1.4585173044131932\n\n\n\n\nCode\nœÉ = 1;\nŒ≤ = [1.0, -3, -2];\n\n\n\n\nCode\nfit!(simulate!(m0; Œ∏, Œ≤, œÉ))\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\n\n\n(Intercept)\n0.9821\n0.2251\n4.36\n&lt;1e-04\n1.2186\n\n\nfrequency: low\n-2.6173\n0.2452\n-10.67\n&lt;1e-25\n1.2616\n\n\nfrequency: medium\n-2.4278\n0.2620\n-9.26\n&lt;1e-19\n1.5733\n\n\nResidual\n1.2755\n\n\n\n\n\n\n\n\n\n\n\nCode\nshrinkageplot(m0)\n\n\n\n\n\n\n\nCode\ncaterpillar(m0; orderby=nothing, vline_at_zero=true)\n\n\n\n\n\n\n\nCode\ndesign[!, :dv] .= response(m0)\n\n\n120-element Vector{Float64}:\n  7.137030491753391\n  4.521590905574694\n  2.6004127916457835\n  2.5124664075541574\n  8.148812898831231\n  9.85604330303867\n 10.117496608295912\n  4.124349544115095\n  5.534123249404053\n  4.352715843131017\n  ‚ãÆ\n  6.366654698612832\n -1.8372354297880817\n -0.030311878451395824\n  1.5155584157386226\n  2.490692658124921\n  2.440502107916979\n -0.29150078211204367\n  7.842126067315071\n -2.159323099414003\n\n\n\n\nCode\ndesign_partial = filter(design) do row\n    subj = parse(Int, row.subj[2:end])\n    item = parse(Int, row.item[2:end])\n    # for even-numbered subjects, we keep all conditions\n    # for odd-numbered subjects, we keep only the two \"odd\" items,\n    # i.e. the first and last conditions\n    return iseven(subj) || isodd(item)\nend\nsort!(unique!(select(design_partial, :subj, :frequency)), :subj)\n\n\n100√ó2 DataFrame75 rows omitted\n\n\n\nRow\nsubj\nfrequency\n\n\n\nString\nString\n\n\n\n\n1\nS01\nhigh\n\n\n2\nS01\nlow\n\n\n3\nS02\nhigh\n\n\n4\nS02\nmedium\n\n\n5\nS02\nlow\n\n\n6\nS03\nhigh\n\n\n7\nS03\nlow\n\n\n8\nS04\nhigh\n\n\n9\nS04\nmedium\n\n\n10\nS04\nlow\n\n\n11\nS05\nhigh\n\n\n12\nS05\nlow\n\n\n13\nS06\nhigh\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n89\nS36\nmedium\n\n\n90\nS36\nlow\n\n\n91\nS37\nhigh\n\n\n92\nS37\nlow\n\n\n93\nS38\nhigh\n\n\n94\nS38\nmedium\n\n\n95\nS38\nlow\n\n\n96\nS39\nhigh\n\n\n97\nS39\nlow\n\n\n98\nS40\nhigh\n\n\n99\nS40\nmedium\n\n\n100\nS40\nlow\n\n\n\n\n\n\n\n\nCode\nm1 = let contrasts, form\n    contrasts = Dict(:frequency =&gt; HelmertCoding(base=\"high\"))\n    form = @formula(dv ~ 1 + frequency +\n                    (1 + frequency | subj))\n    fit(MixedModel, form, design_partial; contrasts, progress)\nend\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\n\n\n(Intercept)\n0.9604\n0.2478\n3.88\n0.0001\n1.2058\n\n\nfrequency: low\n-2.6173\n0.2452\n-10.67\n&lt;1e-25\n1.3503\n\n\nfrequency: medium\n-2.4495\n0.3017\n-8.12\n&lt;1e-15\n1.6834\n\n\nResidual\n1.0787\n\n\n\n\n\n\n\n\n\n\n\nCode\nshrinkageplot(m1)\n\n\n\n\n\n\n\nCode\ncaterpillar(m1; orderby=nothing, vline_at_zero=true)\n\n\n\n\n\nThis page was rendered from git revision fb99b3c\n using Quarto 1.7.33.\n\n\n\n Back to top",
    "crumbs": [
      "Worked examples",
      "Partially-within subjects designs"
    ]
  },
  {
    "objectID": "profiling.html",
    "href": "profiling.html",
    "title": "Confidence intervals from profiled objective",
    "section": "",
    "text": "Statistical methods that are based on probability models can be used to provide us with a ‚Äúbest guess‚Äù of the value of parameters, such as the effect of a particular experimental treatment, in the form of a parameter estimate. In addition, the probability model can be used to assess the uncertainty in the estimate.\nOften the information about the uncertainty is reduced to a single number, a p-value for a test of a null hypothesis, such as the effect being zero, versus the alternative of a non-zero effect. But quoting a single number from a model fit to experimental data, which may have required considerable effort and expense to obtain, will often mean discarding a considerable amount of the information in the data. In the days when computing was expensive and labor-intensive this may have been unavoidable. However, modern computing hardware and software systems provide us with the opportunity of much more intensive evaluation of the uncertainty. At a minimum, instead of focussing solely on the question of whether a coefficient could reasonably be zero, we can formulate confidence intervals on individual parameter estimates or confidence regions on groups of parameters.\nWe have seen the used of a parametric bootstrap to create a sample from the distribution of the estimators of the parameters, and how such samples can be used to create coverage intervals. The bootstrap is based on simulating response vectors from the model that has been fit to the observed data and refitting the same model to these simulated responses.\nIn this section we explore another approach based on refitting the model, keeping the same responses but holding one of the parameters fixed at a specified value.\n\n\nLoad the packages to be used\n\n\nCode\nusing AlgebraOfGraphics\nusing CairoMakie\nusing MixedModels\nusing MixedModelsMakie\nusing Random\nusing SMLP2026: dataset\n\nconst progress=false\n\n\nLoad the data and define the contrasts so that the coefficients for each of the experimental variables, load, spkr and prec, are positive.\n\ncontrasts = Dict( # base levels so estimates for speed are positive\n  :load =&gt; EffectsCoding(; base=\"yes\"),\n  :prec =&gt; EffectsCoding(; base=\"break\"),\n  :spkr =&gt; EffectsCoding(; base=\"old\"),\n)\nkb07 = Table(dataset(:kb07))\n\nTable with 7 columns and 1789 rows:\n      subj  item  spkr  prec      load  rt_trunc  rt_raw\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n 1  ‚îÇ S030  I01   new   break     yes   2267      2267\n 2  ‚îÇ S030  I02   old   maintain  no    3856      3856\n 3  ‚îÇ S030  I03   old   break     no    1567      1567\n 4  ‚îÇ S030  I04   new   maintain  no    1732      1732\n 5  ‚îÇ S030  I05   new   break     no    2660      2660\n 6  ‚îÇ S030  I06   old   maintain  yes   2763      2763\n 7  ‚îÇ S030  I07   old   break     yes   3528      3528\n 8  ‚îÇ S030  I08   new   maintain  yes   1741      1741\n 9  ‚îÇ S030  I09   new   break     yes   3692      3692\n 10 ‚îÇ S030  I10   old   maintain  no    1949      1949\n 11 ‚îÇ S030  I11   old   break     no    2189      2189\n 12 ‚îÇ S030  I12   new   maintain  no    2207      2207\n 13 ‚îÇ S030  I13   new   break     no    2078      2078\n 14 ‚îÇ S030  I14   old   maintain  yes   1901      1901\n 15 ‚îÇ S030  I15   old   break     yes   4015      4015\n 16 ‚îÇ S030  I16   new   maintain  yes   1880      1880\n 17 ‚îÇ S030  I17   new   break     yes   1444      1444\n ‚ãÆ  ‚îÇ  ‚ãÆ     ‚ãÆ     ‚ãÆ       ‚ãÆ       ‚ãÆ       ‚ãÆ        ‚ãÆ\n\n\nNow we fit and profile a model. The response is defined as 1000 / rt_raw where rt_raw is measured in milliseconds. Thus the response being modeled is the speed measured in responses per second.\n\npr01 = let f = @formula 1000 / rt_raw ~\n    1 + load + spkr + prec + (1 + prec | item) + (1 | subj)\n  profile(fit(MixedModel, f, kb07; contrasts, progress))\nend\nprintln(pr01.m) # model is a property of the profile object\n\nLinear mixed model fit by maximum likelihood\n :(1000 / rt_raw) ~ 1 + load + spkr + prec + (1 + prec | item) + (1 | subj)\n   logLik   -2 logLik     AIC       AICc        BIC    \n   846.2869 -1692.5738 -1674.5738 -1674.4726 -1625.1691\n\nVariance components:\n             Column      Variance  Std.Dev.   Corr.\nitem     (Intercept)     0.0061053 0.0781364\n         prec: maintain  0.0020476 0.0452502 -0.21\nsubj     (Intercept)     0.0054186 0.0736111\nResidual                 0.0194484 0.1394577\n Number of obs: 1789; levels of grouping factors: 32, 56\n\n  Fixed-effects parameters:\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                    Coef.  Std. Error      z  Pr(&gt;|z|)\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n(Intercept)     0.531523   0.0172749   30.77    &lt;1e-99\nload: no        0.0212959  0.00329731   6.46    &lt;1e-09\nspkr: new       0.011218   0.00329732   3.40    0.0007\nprec: maintain  0.0698293  0.00865212   8.07    &lt;1e-15\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\nEvaluation of pr01 is similar to other model fits in these notes except that the call to fit is wrapped in a call to profile. Because the object returned from profile includes the original model fit as its m property, it is not necessary to save the original model fit separately.\n\n\n\nThe information from the profile is encapsulated in a table.\n\npr01.tbl\n\nTable with 15 columns and 249 rows:\n      p  Œ∂          Œ≤1        Œ≤2         Œ≤3         Œ≤4         œÉ         ‚ãØ\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n 1  ‚îÇ œÉ  -4.11622   0.531525  0.021298   0.0112154  0.0698319  0.130088  ‚ãØ\n 2  ‚îÇ œÉ  -3.59104   0.531525  0.0212977  0.0112157  0.0698316  0.131224  ‚ãØ\n 3  ‚îÇ œÉ  -3.06896   0.531524  0.0212975  0.011216   0.0698313  0.13237   ‚ãØ\n 4  ‚îÇ œÉ  -2.54995   0.531524  0.0212972  0.0112164  0.069831   0.133526  ‚ãØ\n 5  ‚îÇ œÉ  -2.03398   0.531524  0.021297   0.0112167  0.0698307  0.134692  ‚ãØ\n 6  ‚îÇ œÉ  -1.52103   0.531524  0.0212967  0.011217   0.0698303  0.135868  ‚ãØ\n 7  ‚îÇ œÉ  -1.01106   0.531523  0.0212964  0.0112173  0.06983    0.137054  ‚ãØ\n 8  ‚îÇ œÉ  -0.504062  0.531523  0.0212961  0.0112177  0.0698297  0.138251  ‚ãØ\n 9  ‚îÇ œÉ  0.0        0.531523  0.0212959  0.011218   0.0698293  0.139458  ‚ãØ\n 10 ‚îÇ œÉ  0.501152   0.531523  0.0212956  0.0112184  0.069829   0.140675  ‚ãØ\n 11 ‚îÇ œÉ  0.999416   0.531522  0.0212953  0.0112187  0.0698286  0.141904  ‚ãØ\n 12 ‚îÇ œÉ  1.49482    0.531522  0.021295   0.0112191  0.0698282  0.143143  ‚ãØ\n 13 ‚îÇ œÉ  1.98738    0.531522  0.0212947  0.0112195  0.0698279  0.144392  ‚ãØ\n 14 ‚îÇ œÉ  2.47713    0.531521  0.0212944  0.0112199  0.0698275  0.145653  ‚ãØ\n 15 ‚îÇ œÉ  2.9641     0.531521  0.021294   0.0112202  0.0698271  0.146925  ‚ãØ\n 16 ‚îÇ œÉ  3.44829    0.531521  0.0212937  0.0112206  0.0698267  0.148208  ‚ãØ\n 17 ‚îÇ œÉ  3.92975    0.53152   0.0212934  0.011221   0.0698263  0.149502  ‚ãØ\n ‚ãÆ  ‚îÇ ‚ãÆ      ‚ãÆ         ‚ãÆ          ‚ãÆ          ‚ãÆ          ‚ãÆ         ‚ãÆ      ‚ã±\n\n\nEach row of the table summarizes a fit of the original model to the original data but with one of the parameters held fixed. For the first 18 rows of the table, the parameter being held fixed is \\(\\sigma\\), as shown in the p column. In the next set of rows the parameter being held fixed will be \\(\\beta_1\\), the intercept.\nThere are blocks of rows for the fixed-effects (\\(\\boldsymbol{\\beta}\\)) parameters, the variance components (on the scale of a standard deviation), and the \\(\\boldsymbol{\\theta}\\) parameters that generate the covariance factor \\(\\boldsymbol{\\Lambda}_{\\boldsymbol{\\theta}}\\). (At present the correlation parameters are not profiled - we may add them later but that computation is rather awkward.)\n\nshow(unique(pr01.tbl.p))\n\n[:œÉ, :Œ≤1, :Œ≤2, :Œ≤3, :Œ≤4, :Œ∏1, :Œ∏2, :Œ∏3, :Œ∏4, :œÉ1, :œÉ2, :œÉ3]\n\n\nTo reiterate, the first row contains the parameter estimates for this model fit to the original response values with the constraint that \\(\\sigma=0.130088\\), instead of the global estimate \\(\\hat{\\sigma}=0.139458\\) in the row for which \\(\\zeta=0.0\\).\nThe global estimates are included in every block at the row for which \\(\\zeta=0.0\\).\n\nfilter(r -&gt; iszero(r.Œ∂), pr01.tbl)\n\nTable with 15 columns and 12 rows:\n      p   Œ∂    Œ≤1        Œ≤2         Œ≤3        Œ≤4         œÉ         œÉ1         ‚ãØ\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n 1  ‚îÇ œÉ   0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ‚ãØ\n 2  ‚îÇ Œ≤1  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ‚ãØ\n 3  ‚îÇ Œ≤2  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ‚ãØ\n 4  ‚îÇ Œ≤3  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ‚ãØ\n 5  ‚îÇ Œ≤4  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ‚ãØ\n 6  ‚îÇ Œ∏1  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ‚ãØ\n 7  ‚îÇ Œ∏2  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ‚ãØ\n 8  ‚îÇ Œ∏3  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ‚ãØ\n 9  ‚îÇ Œ∏4  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ‚ãØ\n 10 ‚îÇ œÉ1  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ‚ãØ\n 11 ‚îÇ œÉ2  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ‚ãØ\n 12 ‚îÇ œÉ3  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ‚ãØ\n\n\nThe \\(\\zeta\\) column in this table is a measure of the quality of the fit from the parameters in each row, relative to the global parameter estimates, as measured by the change in the objective (negative twice the log-likelihood).\nThe minimum value for the objective is that at the global parameter estimates. The change in the objective when we constrain one parameter to a particular value has approximately a \\(\\chi^2\\) distribution on 1 degree of freedom, which is the square of a standard normal distribution, \\(\\mathcal{Z}^2\\). We can convert this change in the quality of the fit to the scale of the standard normal distribution by taking the signed square root, which is the square root of the change in the objective with the sign of \\(\\psi-\\hat{\\psi}\\) where \\(\\psi\\) represents the parameter being profiled. This is the value labelled \\(\\zeta\\) in the table.\nTo review:\n\nEach row in the table is the result of re-fitting the original model with the parameter in the p column held fixed at a particular value, as shown in the column for that parameter.\nThe \\(\\zeta\\) column is the signed square root of the change in the objective from the global parameter estimates.\nThus in the block of rows where \\(\\sigma\\) is held fixed, the \\(\\zeta\\) values in rows for which \\(\\sigma&lt;\\hat\\sigma\\) are negative and those for which \\(\\sigma &gt; \\hat\\sigma\\) have positive values of \\(\\zeta\\).\nRows in which \\(\\zeta=0.0\\) are the global parameter estimates.\n\n\n\n\nFigure¬†1 shows, for each of the fixed effects parameters, \\(\\zeta\\) versus the parameter value.\n\n\nCode\nzetaplot!(Figure(; resolution=(1200, 350)), pr01; ptyp='Œ≤')\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†1: Œ∂ versus the value of the coefficient for the fixed-effects parameters in a model of response speed for the kb07 data.\n\n\n\n\nThe lines on these panels are read like normal probability plots, i.e.¬†QQ plots against a standard normal distribution. Those on the \\(\\beta_2\\) and \\(\\beta_3\\) panels are, to the resolution of the plot, straight lines which indicates that the estimators of those parameters are normally distributed over the region of interest.\nThe points in the \\(\\beta_1\\) and \\(\\beta_4\\) panels are slightly over-dispersed relative to the straight line, which means that the estimators of these parameters are distributed like a T-distribution with a moderate number of degrees of freedom.\nThe profile-\\(\\zeta\\) function can be used to generate confidence intervals on the parameters\n\nconfint(pr01)\n\nDictTable with 3 columns and 8 rows:\n par   estimate   lower       upper\n ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n Œ≤1  ‚îÇ 0.531523   0.497103    0.565942\n Œ≤2  ‚îÇ 0.0212959  0.0148295   0.0277621\n Œ≤3  ‚îÇ 0.011218   0.00475174  0.0176844\n Œ≤4  ‚îÇ 0.0698293  0.0523046   0.0873562\n œÉ   ‚îÇ 0.139458   0.13486     0.144322\n œÉ1  ‚îÇ 0.0781364  0.0612443   0.103257\n œÉ2  ‚îÇ 0.0452502  0.0338521   0.0618819\n œÉ3  ‚îÇ 0.0736111  0.0600844   0.0916852\n\n\nas shown in Figure¬†2, which shows the absolute value of \\(\\zeta\\), which is simply the square root of the difference in the objective, versus the parameter being profiled.\n\n\nCode\nzetaplot!(Figure(; resolution=(1200, 330)), pr01; ptyp='Œ≤', absv=true)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†2: Absolute value of Œ∂ versus value of the coefficient for the fixed-effects parameters in a model of response speed for the kb07 data. The horizontal lines are confidence intervals with nominal 50%, 80%, 90%, 95% and 99% confidence.\n\n\n\n\nThe 95% confidence intervals are the second horizontal lines from the top in each panel, at 1.96 on the vertical scale.\n\n\nCode\nzetaplot!(Figure(; resolution=(1200, 330)), pr01; ptyp='œÉ', absv=true)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†3: Absolute value of Œ∂ versus value of the coefficient for the variance component parameters in a model of response speed for the kb07 data. The horizontal lines are confidence intervals with nominal 50%, 80%, 90%, 95% and 99% confidence.\n\n\n\n\nFigure¬†3 shows similar confidence intervals on the parameters representing standard deviations as does Figure¬†4 for the \\(\\theta\\) parameters.\n\n\nCode\nzetaplot!(Figure(; resolution=(1200, 330)), pr01; ptyp='Œ∏', absv=true)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†4: Absolute value of Œ∂ versus parameter value for the Œ∏ parameters in a model of response speed for the kb07 data. The horizontal lines are confidence intervals with nominal 50%, 80%, 90%, 95% and 99% confidence.",
    "crumbs": [
      "Bootstrap and profiling",
      "Confidence intervals from profiled objective"
    ]
  },
  {
    "objectID": "profiling.html#profiling-a-model-for-the-kb07-data",
    "href": "profiling.html#profiling-a-model-for-the-kb07-data",
    "title": "Confidence intervals from profiled objective",
    "section": "",
    "text": "Load the packages to be used\n\n\nCode\nusing AlgebraOfGraphics\nusing CairoMakie\nusing MixedModels\nusing MixedModelsMakie\nusing Random\nusing SMLP2026: dataset\n\nconst progress=false\n\n\nLoad the data and define the contrasts so that the coefficients for each of the experimental variables, load, spkr and prec, are positive.\n\ncontrasts = Dict( # base levels so estimates for speed are positive\n  :load =&gt; EffectsCoding(; base=\"yes\"),\n  :prec =&gt; EffectsCoding(; base=\"break\"),\n  :spkr =&gt; EffectsCoding(; base=\"old\"),\n)\nkb07 = Table(dataset(:kb07))\n\nTable with 7 columns and 1789 rows:\n      subj  item  spkr  prec      load  rt_trunc  rt_raw\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n 1  ‚îÇ S030  I01   new   break     yes   2267      2267\n 2  ‚îÇ S030  I02   old   maintain  no    3856      3856\n 3  ‚îÇ S030  I03   old   break     no    1567      1567\n 4  ‚îÇ S030  I04   new   maintain  no    1732      1732\n 5  ‚îÇ S030  I05   new   break     no    2660      2660\n 6  ‚îÇ S030  I06   old   maintain  yes   2763      2763\n 7  ‚îÇ S030  I07   old   break     yes   3528      3528\n 8  ‚îÇ S030  I08   new   maintain  yes   1741      1741\n 9  ‚îÇ S030  I09   new   break     yes   3692      3692\n 10 ‚îÇ S030  I10   old   maintain  no    1949      1949\n 11 ‚îÇ S030  I11   old   break     no    2189      2189\n 12 ‚îÇ S030  I12   new   maintain  no    2207      2207\n 13 ‚îÇ S030  I13   new   break     no    2078      2078\n 14 ‚îÇ S030  I14   old   maintain  yes   1901      1901\n 15 ‚îÇ S030  I15   old   break     yes   4015      4015\n 16 ‚îÇ S030  I16   new   maintain  yes   1880      1880\n 17 ‚îÇ S030  I17   new   break     yes   1444      1444\n ‚ãÆ  ‚îÇ  ‚ãÆ     ‚ãÆ     ‚ãÆ       ‚ãÆ       ‚ãÆ       ‚ãÆ        ‚ãÆ\n\n\nNow we fit and profile a model. The response is defined as 1000 / rt_raw where rt_raw is measured in milliseconds. Thus the response being modeled is the speed measured in responses per second.\n\npr01 = let f = @formula 1000 / rt_raw ~\n    1 + load + spkr + prec + (1 + prec | item) + (1 | subj)\n  profile(fit(MixedModel, f, kb07; contrasts, progress))\nend\nprintln(pr01.m) # model is a property of the profile object\n\nLinear mixed model fit by maximum likelihood\n :(1000 / rt_raw) ~ 1 + load + spkr + prec + (1 + prec | item) + (1 | subj)\n   logLik   -2 logLik     AIC       AICc        BIC    \n   846.2869 -1692.5738 -1674.5738 -1674.4726 -1625.1691\n\nVariance components:\n             Column      Variance  Std.Dev.   Corr.\nitem     (Intercept)     0.0061053 0.0781364\n         prec: maintain  0.0020476 0.0452502 -0.21\nsubj     (Intercept)     0.0054186 0.0736111\nResidual                 0.0194484 0.1394577\n Number of obs: 1789; levels of grouping factors: 32, 56\n\n  Fixed-effects parameters:\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                    Coef.  Std. Error      z  Pr(&gt;|z|)\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n(Intercept)     0.531523   0.0172749   30.77    &lt;1e-99\nload: no        0.0212959  0.00329731   6.46    &lt;1e-09\nspkr: new       0.011218   0.00329732   3.40    0.0007\nprec: maintain  0.0698293  0.00865212   8.07    &lt;1e-15\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\nEvaluation of pr01 is similar to other model fits in these notes except that the call to fit is wrapped in a call to profile. Because the object returned from profile includes the original model fit as its m property, it is not necessary to save the original model fit separately.",
    "crumbs": [
      "Bootstrap and profiling",
      "Confidence intervals from profiled objective"
    ]
  },
  {
    "objectID": "profiling.html#fixing-values-of-parameters",
    "href": "profiling.html#fixing-values-of-parameters",
    "title": "Confidence intervals from profiled objective",
    "section": "",
    "text": "The information from the profile is encapsulated in a table.\n\npr01.tbl\n\nTable with 15 columns and 249 rows:\n      p  Œ∂          Œ≤1        Œ≤2         Œ≤3         Œ≤4         œÉ         ‚ãØ\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n 1  ‚îÇ œÉ  -4.11622   0.531525  0.021298   0.0112154  0.0698319  0.130088  ‚ãØ\n 2  ‚îÇ œÉ  -3.59104   0.531525  0.0212977  0.0112157  0.0698316  0.131224  ‚ãØ\n 3  ‚îÇ œÉ  -3.06896   0.531524  0.0212975  0.011216   0.0698313  0.13237   ‚ãØ\n 4  ‚îÇ œÉ  -2.54995   0.531524  0.0212972  0.0112164  0.069831   0.133526  ‚ãØ\n 5  ‚îÇ œÉ  -2.03398   0.531524  0.021297   0.0112167  0.0698307  0.134692  ‚ãØ\n 6  ‚îÇ œÉ  -1.52103   0.531524  0.0212967  0.011217   0.0698303  0.135868  ‚ãØ\n 7  ‚îÇ œÉ  -1.01106   0.531523  0.0212964  0.0112173  0.06983    0.137054  ‚ãØ\n 8  ‚îÇ œÉ  -0.504062  0.531523  0.0212961  0.0112177  0.0698297  0.138251  ‚ãØ\n 9  ‚îÇ œÉ  0.0        0.531523  0.0212959  0.011218   0.0698293  0.139458  ‚ãØ\n 10 ‚îÇ œÉ  0.501152   0.531523  0.0212956  0.0112184  0.069829   0.140675  ‚ãØ\n 11 ‚îÇ œÉ  0.999416   0.531522  0.0212953  0.0112187  0.0698286  0.141904  ‚ãØ\n 12 ‚îÇ œÉ  1.49482    0.531522  0.021295   0.0112191  0.0698282  0.143143  ‚ãØ\n 13 ‚îÇ œÉ  1.98738    0.531522  0.0212947  0.0112195  0.0698279  0.144392  ‚ãØ\n 14 ‚îÇ œÉ  2.47713    0.531521  0.0212944  0.0112199  0.0698275  0.145653  ‚ãØ\n 15 ‚îÇ œÉ  2.9641     0.531521  0.021294   0.0112202  0.0698271  0.146925  ‚ãØ\n 16 ‚îÇ œÉ  3.44829    0.531521  0.0212937  0.0112206  0.0698267  0.148208  ‚ãØ\n 17 ‚îÇ œÉ  3.92975    0.53152   0.0212934  0.011221   0.0698263  0.149502  ‚ãØ\n ‚ãÆ  ‚îÇ ‚ãÆ      ‚ãÆ         ‚ãÆ          ‚ãÆ          ‚ãÆ          ‚ãÆ         ‚ãÆ      ‚ã±\n\n\nEach row of the table summarizes a fit of the original model to the original data but with one of the parameters held fixed. For the first 18 rows of the table, the parameter being held fixed is \\(\\sigma\\), as shown in the p column. In the next set of rows the parameter being held fixed will be \\(\\beta_1\\), the intercept.\nThere are blocks of rows for the fixed-effects (\\(\\boldsymbol{\\beta}\\)) parameters, the variance components (on the scale of a standard deviation), and the \\(\\boldsymbol{\\theta}\\) parameters that generate the covariance factor \\(\\boldsymbol{\\Lambda}_{\\boldsymbol{\\theta}}\\). (At present the correlation parameters are not profiled - we may add them later but that computation is rather awkward.)\n\nshow(unique(pr01.tbl.p))\n\n[:œÉ, :Œ≤1, :Œ≤2, :Œ≤3, :Œ≤4, :Œ∏1, :Œ∏2, :Œ∏3, :Œ∏4, :œÉ1, :œÉ2, :œÉ3]\n\n\nTo reiterate, the first row contains the parameter estimates for this model fit to the original response values with the constraint that \\(\\sigma=0.130088\\), instead of the global estimate \\(\\hat{\\sigma}=0.139458\\) in the row for which \\(\\zeta=0.0\\).\nThe global estimates are included in every block at the row for which \\(\\zeta=0.0\\).\n\nfilter(r -&gt; iszero(r.Œ∂), pr01.tbl)\n\nTable with 15 columns and 12 rows:\n      p   Œ∂    Œ≤1        Œ≤2         Œ≤3        Œ≤4         œÉ         œÉ1         ‚ãØ\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n 1  ‚îÇ œÉ   0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ‚ãØ\n 2  ‚îÇ Œ≤1  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ‚ãØ\n 3  ‚îÇ Œ≤2  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ‚ãØ\n 4  ‚îÇ Œ≤3  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ‚ãØ\n 5  ‚îÇ Œ≤4  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ‚ãØ\n 6  ‚îÇ Œ∏1  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ‚ãØ\n 7  ‚îÇ Œ∏2  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ‚ãØ\n 8  ‚îÇ Œ∏3  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ‚ãØ\n 9  ‚îÇ Œ∏4  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ‚ãØ\n 10 ‚îÇ œÉ1  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ‚ãØ\n 11 ‚îÇ œÉ2  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ‚ãØ\n 12 ‚îÇ œÉ3  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ‚ãØ\n\n\nThe \\(\\zeta\\) column in this table is a measure of the quality of the fit from the parameters in each row, relative to the global parameter estimates, as measured by the change in the objective (negative twice the log-likelihood).\nThe minimum value for the objective is that at the global parameter estimates. The change in the objective when we constrain one parameter to a particular value has approximately a \\(\\chi^2\\) distribution on 1 degree of freedom, which is the square of a standard normal distribution, \\(\\mathcal{Z}^2\\). We can convert this change in the quality of the fit to the scale of the standard normal distribution by taking the signed square root, which is the square root of the change in the objective with the sign of \\(\\psi-\\hat{\\psi}\\) where \\(\\psi\\) represents the parameter being profiled. This is the value labelled \\(\\zeta\\) in the table.\nTo review:\n\nEach row in the table is the result of re-fitting the original model with the parameter in the p column held fixed at a particular value, as shown in the column for that parameter.\nThe \\(\\zeta\\) column is the signed square root of the change in the objective from the global parameter estimates.\nThus in the block of rows where \\(\\sigma\\) is held fixed, the \\(\\zeta\\) values in rows for which \\(\\sigma&lt;\\hat\\sigma\\) are negative and those for which \\(\\sigma &gt; \\hat\\sigma\\) have positive values of \\(\\zeta\\).\nRows in which \\(\\zeta=0.0\\) are the global parameter estimates.",
    "crumbs": [
      "Bootstrap and profiling",
      "Confidence intervals from profiled objective"
    ]
  },
  {
    "objectID": "profiling.html#profile-zeta-plots",
    "href": "profiling.html#profile-zeta-plots",
    "title": "Confidence intervals from profiled objective",
    "section": "",
    "text": "Figure¬†1 shows, for each of the fixed effects parameters, \\(\\zeta\\) versus the parameter value.\n\n\nCode\nzetaplot!(Figure(; resolution=(1200, 350)), pr01; ptyp='Œ≤')\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†1: Œ∂ versus the value of the coefficient for the fixed-effects parameters in a model of response speed for the kb07 data.\n\n\n\n\nThe lines on these panels are read like normal probability plots, i.e.¬†QQ plots against a standard normal distribution. Those on the \\(\\beta_2\\) and \\(\\beta_3\\) panels are, to the resolution of the plot, straight lines which indicates that the estimators of those parameters are normally distributed over the region of interest.\nThe points in the \\(\\beta_1\\) and \\(\\beta_4\\) panels are slightly over-dispersed relative to the straight line, which means that the estimators of these parameters are distributed like a T-distribution with a moderate number of degrees of freedom.\nThe profile-\\(\\zeta\\) function can be used to generate confidence intervals on the parameters\n\nconfint(pr01)\n\nDictTable with 3 columns and 8 rows:\n par   estimate   lower       upper\n ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n Œ≤1  ‚îÇ 0.531523   0.497103    0.565942\n Œ≤2  ‚îÇ 0.0212959  0.0148295   0.0277621\n Œ≤3  ‚îÇ 0.011218   0.00475174  0.0176844\n Œ≤4  ‚îÇ 0.0698293  0.0523046   0.0873562\n œÉ   ‚îÇ 0.139458   0.13486     0.144322\n œÉ1  ‚îÇ 0.0781364  0.0612443   0.103257\n œÉ2  ‚îÇ 0.0452502  0.0338521   0.0618819\n œÉ3  ‚îÇ 0.0736111  0.0600844   0.0916852\n\n\nas shown in Figure¬†2, which shows the absolute value of \\(\\zeta\\), which is simply the square root of the difference in the objective, versus the parameter being profiled.\n\n\nCode\nzetaplot!(Figure(; resolution=(1200, 330)), pr01; ptyp='Œ≤', absv=true)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†2: Absolute value of Œ∂ versus value of the coefficient for the fixed-effects parameters in a model of response speed for the kb07 data. The horizontal lines are confidence intervals with nominal 50%, 80%, 90%, 95% and 99% confidence.\n\n\n\n\nThe 95% confidence intervals are the second horizontal lines from the top in each panel, at 1.96 on the vertical scale.\n\n\nCode\nzetaplot!(Figure(; resolution=(1200, 330)), pr01; ptyp='œÉ', absv=true)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†3: Absolute value of Œ∂ versus value of the coefficient for the variance component parameters in a model of response speed for the kb07 data. The horizontal lines are confidence intervals with nominal 50%, 80%, 90%, 95% and 99% confidence.\n\n\n\n\nFigure¬†3 shows similar confidence intervals on the parameters representing standard deviations as does Figure¬†4 for the \\(\\theta\\) parameters.\n\n\nCode\nzetaplot!(Figure(; resolution=(1200, 330)), pr01; ptyp='Œ∏', absv=true)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†4: Absolute value of Œ∂ versus parameter value for the Œ∏ parameters in a model of response speed for the kb07 data. The horizontal lines are confidence intervals with nominal 50%, 80%, 90%, 95% and 99% confidence.",
    "crumbs": [
      "Bootstrap and profiling",
      "Confidence intervals from profiled objective"
    ]
  },
  {
    "objectID": "shrinkageplot.html",
    "href": "shrinkageplot.html",
    "title": "More on shrinkage plots",
    "section": "",
    "text": "I have stated that the likelihood criterion used to fit linear mixed-effects can be considered as balancing fidelity to the data (i.e.¬†fits the observed data well) versus model complexity.\nThis is similar to some of the criterion used in Machine Learning (ML), except that the criterion for LMMs has a rigorous mathematical basis.\nIn the shrinkage plot we consider the values of the random-effects coefficients for the fitted values of the model versus those from a model in which there is no penalty for model complexity.\nIf there is strong subject-to-subject variation then the model fit will tend to values of the random effects similar to those without a penalty on complexity.\nIf the random effects term is not contributing much (i.e.¬†it is ‚Äúinert‚Äù) then the random effects will be shrunk considerably towards zero in some directions.\nCode\nusing CairoMakie\nusing DataFrames\nusing LinearAlgebra\nusing MixedModels\nusing MixedModelsMakie\nusing Random\nusing ProgressMeter\n\nconst progress = false\n\n\nfalse\nLoad the kb07 data set (don‚Äôt tell Reinhold that I used these data).\nkb07 = MixedModels.dataset(:kb07)\n\nArrow.Table with 1789 rows, 7 columns, and schema:\n :subj      String\n :item      String\n :spkr      String\n :prec      String\n :load      String\n :rt_trunc  Int16\n :rt_raw    Int16\ncontrasts = Dict(\n  :spkr =&gt; HelmertCoding(),\n  :prec =&gt; HelmertCoding(),\n  :load =&gt; HelmertCoding(),\n)\nm1 = let\n  form = @formula(\n    rt_trunc ~\n      1 +\n      spkr * prec * load +\n      (1 + spkr + prec + load | subj) +\n      (1 + spkr + prec + load | item)\n  )\n  fit(MixedModel, form, kb07; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\nœÉ_item\n\n\n(Intercept)\n2181.6728\n77.3025\n28.22\n&lt;1e-99\n301.7795\n362.1969\n\n\nspkr: old\n67.7486\n18.2889\n3.70\n0.0002\n42.9237\n40.6934\n\n\nprec: maintain\n-333.9211\n47.1525\n-7.08\n&lt;1e-11\n61.9966\n246.8936\n\n\nload: yes\n78.7703\n19.5367\n4.03\n&lt;1e-04\n65.1301\n42.3692\n\n\nspkr: old & prec: maintain\n-21.9656\n15.8062\n-1.39\n0.1646\n\n\n\n\nspkr: old & load: yes\n18.3842\n15.8062\n1.16\n0.2448\n\n\n\n\nprec: maintain & load: yes\n4.5338\n15.8062\n0.29\n0.7742\n\n\n\n\nspkr: old & prec: maintain & load: yes\n23.6074\n15.8062\n1.49\n0.1353\n\n\n\n\nResidual\n668.5033\nVarCorr(m1)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nsubj\n(Intercept)\n91070.8712\n301.7795\n\n\n\n\n\n\nspkr: old\n1842.4411\n42.9237\n+0.78\n\n\n\n\n\nprec: maintain\n3843.5766\n61.9966\n-0.59\n+0.03\n\n\n\n\nload: yes\n4241.9308\n65.1301\n+0.36\n+0.83\n+0.53\n\n\nitem\n(Intercept)\n131186.5676\n362.1969\n\n\n\n\n\n\nspkr: old\n1655.9495\n40.6934\n+0.44\n\n\n\n\n\nprec: maintain\n60956.4434\n246.8936\n-0.69\n+0.35\n\n\n\n\nload: yes\n1795.1488\n42.3692\n+0.32\n+0.16\n-0.14\n\n\nResidual\n\n446896.6953\n668.5033\nissingular(m1)\n\ntrue\nprint(m1)\n\nLinear mixed model fit by maximum likelihood\n rt_trunc ~ 1 + spkr + prec + load + spkr & prec + spkr & load + prec & load + spkr & prec & load + (1 + spkr + prec + load | subj) + (1 + spkr + prec + load | item)\n    logLik   -2 logLik      AIC         AICc        BIC     \n -14318.5614  28637.1227  28695.1227  28696.1119  28854.3157\n\nVariance components:\n             Column       Variance  Std.Dev.   Corr.\nsubj     (Intercept)      91070.8712 301.7795\n         spkr: old         1842.4411  42.9237 +0.78\n         prec: maintain    3843.5766  61.9966 -0.59 +0.03\n         load: yes         4241.9308  65.1301 +0.36 +0.83 +0.53\nitem     (Intercept)     131186.5676 362.1969\n         spkr: old         1655.9495  40.6934 +0.44\n         prec: maintain   60956.4434 246.8936 -0.69 +0.35\n         load: yes         1795.1488  42.3692 +0.32 +0.16 -0.14\nResidual                 446896.6953 668.5033\n Number of obs: 1789; levels of grouping factors: 56, 32\n\n  Fixed-effects parameters:\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                                            Coef.  Std. Error      z  Pr(&gt;|z|)\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n(Intercept)                             2181.67       77.3025  28.22    &lt;1e-99\nspkr: old                                 67.7486     18.2889   3.70    0.0002\nprec: maintain                          -333.921      47.1525  -7.08    &lt;1e-11\nload: yes                                 78.7703     19.5367   4.03    &lt;1e-04\nspkr: old & prec: maintain               -21.9656     15.8062  -1.39    0.1646\nspkr: old & load: yes                     18.3842     15.8062   1.16    0.2448\nprec: maintain & load: yes                 4.5338     15.8062   0.29    0.7742\nspkr: old & prec: maintain & load: yes    23.6074     15.8062   1.49    0.1353\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ",
    "crumbs": [
      "Visualizations and diagnostics",
      "More on shrinkage plots"
    ]
  },
  {
    "objectID": "shrinkageplot.html#expressing-the-covariance-of-random-effects",
    "href": "shrinkageplot.html#expressing-the-covariance-of-random-effects",
    "title": "More on shrinkage plots",
    "section": "1 Expressing the covariance of random effects",
    "text": "1 Expressing the covariance of random effects\nEarlier today we mentioned that the parameters being optimized are from a ‚Äúmatrix square root‚Äù of the covariance matrix for the random effects. There is one such lower triangular matrix for each grouping factor.\n\nl1 = first(m1.Œª)   # Cholesky factor of relative covariance for subj\n\n4√ó4 LowerTriangular{Float64, Matrix{Float64}}:\n  0.451426    ‚ãÖ          ‚ãÖ          ‚ãÖ \n  0.0502539  0.0399661   ‚ãÖ          ‚ãÖ \n -0.0550269  0.0729273  0.0159447   ‚ãÖ \n  0.0351603  0.0857539  0.0300331  0.0\n\n\nNotice the zero on the diagonal. A triangular matrix with zeros on the diagonal is singular.\n\nl2 = last(m1.Œª)    # this one is also singular\n\n4√ó4 LowerTriangular{Float64, Matrix{Float64}}:\n  0.541803    ‚ãÖ           ‚ãÖ          ‚ãÖ \n  0.0268569  0.0546274    ‚ãÖ          ‚ãÖ \n -0.253088   0.267988    0.0229877   ‚ãÖ \n  0.0200493  0.00126987  0.0600951  0.00138194\n\n\nTo regenerate the covariance matrix we need to know that the covariance is not the square of l1, it is l1 * l1' (so that the result is symmetric) and multiplied by œÉÃÇ¬≤\n\nŒ£‚ÇÅ = varest(m1) .* (l1 * l1')\n\n4√ó4 Matrix{Float64}:\n  91070.9   10138.3     -11101.2     7093.26\n  10138.3    1842.44        66.7243  2321.27\n -11101.2      66.7243    3843.58    2144.17\n   7093.26   2321.27      2144.17    4241.93\n\n\n\ndiag(Œ£‚ÇÅ)  # compare to the variance column in the VarCorr output\n\n4-element Vector{Float64}:\n 91070.87115964481\n  1842.441054170415\n  3843.5765510736305\n  4241.930807293007\n\n\n\nsqrt.(diag(Œ£‚ÇÅ))\n\n4-element Vector{Float64}:\n 301.7795075210456\n  42.92366543260739\n  61.99658499525301\n  65.13010676555818",
    "crumbs": [
      "Visualizations and diagnostics",
      "More on shrinkage plots"
    ]
  },
  {
    "objectID": "shrinkageplot.html#shrinkage-plots",
    "href": "shrinkageplot.html#shrinkage-plots",
    "title": "More on shrinkage plots",
    "section": "2 Shrinkage plots",
    "text": "2 Shrinkage plots\n\n\nCode\nshrinkageplot(m1)\n\n\n\n\n\n\n\nFigure¬†1: Shrinkage plot of model m1\n\n\n\n\nThe upper left panel shows the perfect negative correlation for those two components of the random effects.\n\nshrinkageplot(m1, :item)\n\n\n\n\n\nX1 = Int.(m1.X')\n\n8√ó1789 Matrix{Int64}:\n  1   1   1   1   1  1   1   1   1   1  ‚Ä¶   1   1   1   1   1   1   1  1   1\n -1   1   1  -1  -1  1   1  -1  -1   1      1  -1  -1   1   1  -1  -1  1   1\n -1   1  -1   1  -1  1  -1   1  -1   1     -1   1  -1   1  -1   1  -1  1  -1\n  1  -1  -1  -1  -1  1   1   1   1  -1      1   1   1  -1  -1  -1  -1  1   1\n  1   1  -1  -1   1  1  -1  -1   1   1     -1  -1   1   1  -1  -1   1  1  -1\n -1  -1  -1   1   1  1   1  -1  -1  -1  ‚Ä¶   1  -1  -1  -1  -1   1   1  1   1\n -1  -1   1  -1   1  1  -1   1  -1  -1     -1   1  -1  -1   1  -1   1  1  -1\n  1  -1   1   1  -1  1  -1  -1   1  -1     -1  -1   1  -1   1   1  -1  1  -1\n\n\n\nX1 * X1'\n\n8√ó8 Matrix{Int64}:\n 1789    -1    -1     3    -3     1     1     3\n   -1  1789    -3     1    -1     3     3     1\n   -1    -3  1789     1    -1     3     3     1\n    3     1     1  1789     3    -1    -1    -3\n   -3    -1    -1     3  1789     1     1     3\n    1     3     3    -1     1  1789    -3    -1\n    1     3     3    -1     1    -3  1789    -1\n    3     1     1    -3     3    -1    -1  1789",
    "crumbs": [
      "Visualizations and diagnostics",
      "More on shrinkage plots"
    ]
  },
  {
    "objectID": "shrinkageplot.html#how-to-interpret-a-shrinkage-plot",
    "href": "shrinkageplot.html#how-to-interpret-a-shrinkage-plot",
    "title": "More on shrinkage plots",
    "section": "3 How to interpret a shrinkage plot",
    "text": "3 How to interpret a shrinkage plot\n\nExtreme shrinkage (shrunk to a line or to a point) is easy to interpret ‚Äì the term is not providing benefit and can be removed.\nWhen the range of the blue dots (shrunk values) is comparable to those of the red dots (unshrunk) it indicates that the term after shrinkage is about as strong as without shrinkage.\nBy itself, this doesn‚Äôt mean that the term is important. In some ways you need to get a feeling for the absolute magnitude of the random effects in addition to the relative magnitude.\nSmall magnitude and small relative magnitude indicate you can drop that term",
    "crumbs": [
      "Visualizations and diagnostics",
      "More on shrinkage plots"
    ]
  },
  {
    "objectID": "shrinkageplot.html#conclusions-from-these-plots",
    "href": "shrinkageplot.html#conclusions-from-these-plots",
    "title": "More on shrinkage plots",
    "section": "4 Conclusions from these plots",
    "text": "4 Conclusions from these plots\n\nOnly the intercept for the subj appears to be contributing explanatory power\nFor the item both the intercept and the spkr appear to be contributing\n\n\nm2 = let\n  form = @formula(\n    rt_trunc ~\n      1 + prec * spkr * load + (1 | subj) + (1 + prec | item)\n  )\n  fit(MixedModel, form, kb07; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n(Intercept)\n2181.7582\n77.4710\n28.16\n&lt;1e-99\n364.7293\n298.1107\n\n\nprec: maintain\n-333.8582\n47.4631\n-7.03\n&lt;1e-11\n252.6694\n\n\n\nspkr: old\n67.8114\n16.0526\n4.22\n&lt;1e-04\n\n\n\n\nload: yes\n78.6849\n16.0525\n4.90\n&lt;1e-06\n\n\n\n\nprec: maintain & spkr: old\n-21.8802\n16.0525\n-1.36\n0.1729\n\n\n\n\nprec: maintain & load: yes\n4.4710\n16.0526\n0.28\n0.7806\n\n\n\n\nspkr: old & load: yes\n18.3214\n16.0526\n1.14\n0.2537\n\n\n\n\nprec: maintain & spkr: old & load: yes\n23.5219\n16.0525\n1.47\n0.1428\n\n\n\n\nResidual\n678.9319\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m2)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\nitem\n(Intercept)\n133027.439\n364.729\n\n\n\n\nprec: maintain\n63841.834\n252.669\n-0.70\n\n\nsubj\n(Intercept)\n88870.013\n298.111\n\n\n\nResidual\n\n460948.573\n678.932\n\n\n\n\n\n\n\n\nCode\nshrinkageplot(m2)\n\n\n\n\n\n\n\nFigure¬†2: Shrinkage plot of model m2\n\n\n\n\n\nm3 = let\n  form = @formula(\n    rt_trunc ~\n      1 + prec + spkr + load + (1 | subj) + (1 + prec | item)\n  )\n  fit(MixedModel, form, kb07; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n(Intercept)\n2181.8526\n77.4680\n28.16\n&lt;1e-99\n364.7121\n298.0257\n\n\nprec: maintain\n-333.7906\n47.4476\n-7.03\n&lt;1e-11\n252.5236\n\n\n\nspkr: old\n67.8790\n16.0785\n4.22\n&lt;1e-04\n\n\n\n\nload: yes\n78.5904\n16.0785\n4.89\n&lt;1e-05\n\n\n\n\nResidual\n680.0318\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m3)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\nitem\n(Intercept)\n133014.901\n364.712\n\n\n\n\nprec: maintain\n63768.182\n252.524\n-0.70\n\n\nsubj\n(Intercept)\n88819.305\n298.026\n\n\n\nResidual\n\n462443.261\n680.032\n\n\n\n\n\n\n\nrng = Random.seed!(1234321);\n\n\nm3btstrp = parametricbootstrap(rng, 2000, m3);\n\n\nDataFrame(shortestcovint(m3btstrp))\n\n9√ó5 DataFrame\n\n\n\nRow\ntype\ngroup\nnames\nlower\nupper\n\n\n\nString\nString?\nString?\nFloat64\nFloat64\n\n\n\n\n1\nŒ≤\nmissing\n(Intercept)\n2022.91\n2334.0\n\n\n2\nŒ≤\nmissing\nprec: maintain\n-430.239\n-239.802\n\n\n3\nŒ≤\nmissing\nspkr: old\n34.0592\n96.72\n\n\n4\nŒ≤\nmissing\nload: yes\n46.5349\n109.526\n\n\n5\nœÉ\nitem\n(Intercept)\n270.065\n451.957\n\n\n6\nœÉ\nitem\nprec: maintain\n181.74\n325.125\n\n\n7\nœÅ\nitem\n(Intercept), prec: maintain\n-0.907255\n-0.490079\n\n\n8\nœÉ\nsubj\n(Intercept)\n233.835\n364.504\n\n\n9\nœÉ\nresidual\nmissing\n657.341\n702.655\n\n\n\n\n\n\n\nridgeplot(m3btstrp)\n\n\n\n\n\n\nFigure¬†3: Ridge plot of the fixed-effects coefficients from the bootstrap sample\n\n\n\n\n\nridgeplot(m3btstrp; show_intercept=false)\n\n\n\n\n\n\nFigure¬†4: Ridge plot of the fixed-effects coefficients from the bootstrap sample (with the intercept)\n\n\n\n\n\nm4 = let\n  form = @formula(\n    rt_trunc ~\n      1 + prec + spkr + load + (1 + prec | item) + (1 | subj)\n  )\n  fit(MixedModel, form, kb07; contrasts, progress)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n(Intercept)\n2181.8526\n77.4680\n28.16\n&lt;1e-99\n364.7121\n298.0257\n\n\nprec: maintain\n-333.7906\n47.4476\n-7.03\n&lt;1e-11\n252.5236\n\n\n\nspkr: old\n67.8790\n16.0785\n4.22\n&lt;1e-04\n\n\n\n\nload: yes\n78.5904\n16.0785\n4.89\n&lt;1e-05\n\n\n\n\nResidual\n680.0318\n\n\n\n\n\n\n\n\n\n\n\nm4bstrp = parametricbootstrap(rng, 2000, m4);\n\n\nridgeplot(m4bstrp; show_intercept=false)\n\n\n\n\n\nDataFrame(shortestcovint(m4bstrp))\n\n9√ó5 DataFrame\n\n\n\nRow\ntype\ngroup\nnames\nlower\nupper\n\n\n\nString\nString?\nString?\nFloat64\nFloat64\n\n\n\n\n1\nŒ≤\nmissing\n(Intercept)\n2034.15\n2335.45\n\n\n2\nŒ≤\nmissing\nprec: maintain\n-427.769\n-248.337\n\n\n3\nŒ≤\nmissing\nspkr: old\n35.6938\n97.8081\n\n\n4\nŒ≤\nmissing\nload: yes\n45.3095\n107.368\n\n\n5\nœÉ\nitem\n(Intercept)\n260.498\n451.649\n\n\n6\nœÉ\nitem\nprec: maintain\n179.477\n315.415\n\n\n7\nœÅ\nitem\n(Intercept), prec: maintain\n-0.901827\n-0.471356\n\n\n8\nœÉ\nsubj\n(Intercept)\n236.684\n360.363\n\n\n9\nœÉ\nresidual\nmissing\n657.178\n702.739\n\n\n\n\n\n\n\nVarCorr(m4)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\nitem\n(Intercept)\n133014.901\n364.712\n\n\n\n\nprec: maintain\n63768.182\n252.524\n-0.70\n\n\nsubj\n(Intercept)\n88819.305\n298.026\n\n\n\nResidual\n\n462443.261\n680.032\n\n\n\n\n\n\n\n\nCode\nlet mods = [m1, m2, m4]\n  DataFrame(;\n    geomdof=(sum ‚àò leverage).(mods),\n    npar=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n3√ó6 DataFrame\n\n\n\nRow\ngeomdof\nnpar\ndeviance\nAIC\nBIC\nAICc\n\n\n\nFloat64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n131.555\n29\n28637.1\n28695.1\n28854.3\n28696.1\n\n\n2\n107.543\n13\n28658.5\n28684.5\n28755.8\n28684.7\n\n\n3\n103.478\n9\n28663.9\n28681.9\n28731.3\n28682.0\n\n\n\n\n\n\n\nscatter(fitted(m4), residuals(m4))\n\n\n\n\n\n\nFigure¬†5: Residuals versus fitted values for model m4\n\n\n\n\nThis page was rendered from git revision fb99b3c\n using Quarto 1.7.33.",
    "crumbs": [
      "Visualizations and diagnostics",
      "More on shrinkage plots"
    ]
  },
  {
    "objectID": "sleepstudy.html",
    "href": "sleepstudy.html",
    "title": "Analysis of the sleepstudy data",
    "section": "",
    "text": "The sleepstudy data are from a study of the effects of sleep deprivation on response time reported in Balkin et al. (2000) and in Belenky et al. (2003). Eighteen subjects were allowed only 3 hours of time to sleep each night for 9 successive nights. Their reaction time was measured each day, starting the day before the first night of sleep deprivation, when the subjects were on their regular sleep schedule.\n\n\n\n\n\n\nNote\n\n\n\nThis description is inaccurate. In fact the first two days were acclimatization, the third was a baseline and sleep deprivation was only enforced after day 2. To allow for comparison with earlier analyses of these data we retain the old data description for this notebook only.\n\n\n\n1 Loading the data\nFirst attach the MixedModels package and other packages for plotting. The CairoMakie package allows the Makie graphics system (Danisch & Krumbiegel, 2021) to generate high quality static images. Activate that package with the SVG (Scalable Vector Graphics) backend.\n\n\nCode\nusing AlgebraOfGraphics\nusing CairoMakie       # graphics back-end\nusing DataFrames\nusing KernelDensity    # density estimation\nusing MixedModels\nusing MixedModelsMakie # diagnostic plots\nusing Random           # random number generators\nusing RCall            # call R from Julia\nusing MixedModelsMakie: simplelinreg\nusing SMLP2026: dataset\n\n\nThe sleepstudy data are one of the datasets available with the MixedModels package. It is re-exported by the SMLP2026 package‚Äôs dataset function.\n\nsleepstudy = DataFrame(dataset(\"sleepstudy\"))\n\n180√ó3 DataFrame155 rows omitted\n\n\n\nRow\nsubj\ndays\nreaction\n\n\n\nString\nInt8\nFloat64\n\n\n\n\n1\nS308\n0\n249.56\n\n\n2\nS308\n1\n258.705\n\n\n3\nS308\n2\n250.801\n\n\n4\nS308\n3\n321.44\n\n\n5\nS308\n4\n356.852\n\n\n6\nS308\n5\n414.69\n\n\n7\nS308\n6\n382.204\n\n\n8\nS308\n7\n290.149\n\n\n9\nS308\n8\n430.585\n\n\n10\nS308\n9\n466.353\n\n\n11\nS309\n0\n222.734\n\n\n12\nS309\n1\n205.266\n\n\n13\nS309\n2\n202.978\n\n\n‚ãÆ\n‚ãÆ\n‚ãÆ\n‚ãÆ\n\n\n169\nS371\n8\n350.781\n\n\n170\nS371\n9\n369.469\n\n\n171\nS372\n0\n269.412\n\n\n172\nS372\n1\n273.474\n\n\n173\nS372\n2\n297.597\n\n\n174\nS372\n3\n310.632\n\n\n175\nS372\n4\n287.173\n\n\n176\nS372\n5\n329.608\n\n\n177\nS372\n6\n334.482\n\n\n178\nS372\n7\n343.22\n\n\n179\nS372\n8\n369.142\n\n\n180\nS372\n9\n364.124\n\n\n\n\n\n\nFigure¬†1 displays the data in a multi-panel plot created with the lattice package in R (Sarkar, 2008), using RCall.jl.\n\n\nCode\nlet f = Figure(; size=(700, 400))\n  yrange = maximum(sleepstudy.reaction) - minimum(sleepstudy.reaction)\n  xrange = maximum(sleepstudy.days) - minimum(sleepstudy.days)\n  \n  reg = combine(groupby(sleepstudy, :subj), \n                [:days, :reaction] =&gt; NamedTuple{(:intercept, :slope)} ‚àò simplelinreg =&gt; AsTable)\n  sort!(reg, :intercept)\n\n  # order of grid positions to plot the facets in\n  gridpos = Dict{String, NTuple{2,Int}}()\n  for (i, subj) in enumerate(reg.subj)\n    gridpos[subj] = fldmod1(i, 9)\n  end\n  gridpos\n\n   axes = Axis[]\n\n  # set up all the axes and plot the simple regression lines\n  for row in eachrow(reg)\n    pos = gridpos[row.subj]\n    ax = Axis(f[pos...]; title=row.subj, \n              autolimitaspect=xrange/yrange)\n    if pos[1] == 1\n      hidexdecorations!(ax; grid=false, ticks=false)\n    end\n    if pos[2] != 1\n      hideydecorations!(ax; grid=false, ticks=true)\n    end\n    push!(axes, ax)\n    ablines!(ax, row.intercept, row.slope)\n  end\n\n  # scatter plot in each facet\n  for (grouping, gdf) in pairs(groupby(sleepstudy, :subj))\n    pos = gridpos[grouping.subj]\n    scatter!(f[pos...], gdf.days, gdf.reaction)\n  end\n  Label(f[end+1, :], \"Days of sleep deprivation\"; \n        tellwidth=false, tellheight=true)\n  Label(f[:, 0], \"Average reaction time (ms)\"; \n        tellwidth=true, tellheight=false, rotation=pi/2)\n  \n  linkaxes!(axes...)\n\n  # tweak the layout a little\n  rowgap!(f.layout, 0)\n  colgap!(f.layout, 3)\n  colsize!(f.layout, 0, 25)\n  rowsize!(f.layout, 1, 100)\n  rowsize!(f.layout, 2, 100)\n  rowsize!(f.layout, 3, 25)\n  f\nend\n\n\n\n\n\n\n\nFigure¬†1: Average response time versus days of sleep deprivation by subject\n\n\n\n\nEach panel shows the data from one subject and a line fit by least squares to that subject‚Äôs data. Starting at the lower left panel and proceeding across rows, the panels are ordered by increasing intercept of the least squares line.\nThere are some deviations from linearity within the panels but the deviations are neither substantial nor systematic.\n\n\n2 Fitting an initial model\n\ncontrasts = Dict{Symbol,Any}(:subj =&gt; Grouping())\nm1 = let f = @formula(reaction ~ 1 + days + (1 + days | subj))\n  fit(MixedModel, f, sleepstudy; contrasts)\nend\n\n\nMinimizing 2    Time: 0:00:00 (79.32 ms/it)\n   objective: 1792.8615963994716\n\n\nMinimizing 72    Time: 0:00:00 ( 4.97 ms/it)\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\n\n\n(Intercept)\n251.4051\n6.6323\n37.91\n&lt;1e-99\n23.7807\n\n\ndays\n10.4673\n1.5022\n6.97\n&lt;1e-11\n5.7169\n\n\nResidual\n25.5918\n\n\n\n\n\n\n\n\n\nThis model includes fixed effects for the intercept, representing the typical reaction time at the beginning of the experiment with zero days of sleep deprivation, and the slope w.r.t. days of sleep deprivation. The parameter estimates are about 250 ms. typical reaction time without deprivation and a typical increase of 10.5 ms. per day of sleep deprivation.\nThe random effects represent shifts from the typical behavior for each subject. The shift in the intercept has a standard deviation of about 24 ms. which would suggest a range of about 200 ms. to 300 ms. in the intercepts. Similarly within-subject slopes would be expected to have a range of about 0 ms./day up to 20 ms./day.\nThe random effects for the slope and for the intercept are allowed to be correlated within subject. The estimated correlation, 0.08, is small. This estimate is not shown in the default display above but is shown in the output from VarCorr (variance components and correlations).\n\nVarCorr(m1)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\nsubj\n(Intercept)\n565.52071\n23.78068\n\n\n\n\ndays\n32.68242\n5.71685\n+0.08\n\n\nResidual\n\n654.94015\n25.59180\n\n\n\n\n\n\nTechnically, the random effects for each subject are unobserved random variables and are not ‚Äúparameters‚Äù in the model per se. Hence we do not report standard errors or confidence intervals for these deviations. However, we can produce prediction intervals on the random effects for each subject. Because the experimental design is balanced, these intervals will have the same width for all subjects.\nA plot of the prediction intervals versus the level of the grouping factor (subj, in this case) is sometimes called a caterpillar plot because it can look like a fuzzy caterpillar if there are many levels of the grouping factor. By default, the levels of the grouping factor are sorted by increasing value of the first random effect.\n\n\nCode\ncaterpillar(m1; vline_at_zero=true)\n\n\n\n\n\n\n\nFigure¬†2: Prediction intervals on random effects for model m1\n\n\n\n\nFigure¬†2 reinforces the conclusion that there is little correlation between the random effect for intercept and the random effect for slope.\n\n\n3 A model with uncorrelated random effects\nThe zerocorr function applied to a random-effects term creates uncorrelated vector-valued per-subject random effects.\n\nm2 = let f = @formula reaction ~ 1 + days + zerocorr(1 + days | subj)\n  fit(MixedModel, f, sleepstudy; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\n\n\n(Intercept)\n251.4051\n6.7077\n37.48\n&lt;1e-99\n24.1714\n\n\ndays\n10.4673\n1.5193\n6.89\n&lt;1e-11\n5.7994\n\n\nResidual\n25.5561\n\n\n\n\n\n\n\n\n\nAgain, the default display doesn‚Äôt show that there is no correlation parameter to be estimated in this model, but the VarCorr display does.\n\nVarCorr(m2)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\nsubj\n(Intercept)\n584.2547\n24.1714\n\n\n\n\ndays\n33.6330\n5.7994\n.\n\n\nResidual\n\n653.1157\n25.5561\n\n\n\n\n\n\nThis model has a slightly lower log-likelihood than does m1 and one fewer parameter than m1. A likelihood-ratio test can be used to compare these nested models.\n\nMixedModels.likelihoodratiotest(m2, m1)\n\n\n\n\n\nmodel-dof\n-2 logLik\nœá¬≤\nœá¬≤-dof\nP(&gt;œá¬≤)\n\n\nreaction ~ 1 + days + zerocorr(1 + days | subj)\n5\n-1752\n\n\n\n\n\nreaction ~ 1 + days + (1 + days | subj)\n6\n-1752\n0\n1\n0.8004\n\n\n\n\n\nAlternatively, the AIC or BIC values can be compared.\n\n\nCode\nlet mods = [m2, m1]\n  Table(;\n    model=[:m2, :m1],\n    pars=dof.(mods),\n    geomdof=(sum ‚àò leverage).(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\nTable with 6 columns and 2 rows:\n     model  pars  geomdof  AIC      BIC      AICc\n   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n 1 ‚îÇ m2     5     29.0449  1762.0   1777.97  1762.35\n 2 ‚îÇ m1     6     28.6117  1763.94  1783.1   1764.42\n\n\nThe goodness of fit measures: AIC, BIC, and AICc, are all on a ‚Äúsmaller is better‚Äù scale and, hence, they all prefer m2.\nThe pars column, which is the same as the model-dof column in the likelihood ratio test output, is simply a count of the number of parameters to be estimated when fitting the model. For example, in m2 there are two fixed-effects parameters and three variance components (including the residual variance).\nAn alternative, more geometrically inspired definition of ‚Äúdegrees of freedom‚Äù, is the sum of the leverage values, called geomdof in this table.\nInterestingly, the model with fewer parameters, m2, has a greater sum of the leverage values than the model with more parameters, m1. We‚Äôre not sure what to make of that.\nIn both cases the sum of the leverage values is toward the upper end of the range of possible values, which is the rank of the fixed-effects model matrix (2) up to the rank of the fixed-effects plus the random effects model matrix (2 + 36 = 38).\n\n\n\n\n\n\nNote\n\n\n\nI think that the upper bound may be 36, not 38, because the two columns of X lie in the column span of Z\n\n\nThis comparison does show, however, that a simple count of the parameters in a mixed-effects model can underestimate, sometimes drastically, the model complexity. This is because a single variance component or multiple components can add many dimensions to the linear predictor.\n\n\n4 Some diagnostic plots\nIn mixed-effects models the linear predictor expression incorporates fixed-effects parameters, which summarize trends for the population or certain well-defined subpopulations, and random effects which represent deviations associated with the experimental units or observational units - individual subjects, in this case. The random effects are modeled as unobserved random variables.\nThe conditional means of these random variables, sometimes called the BLUPs or Best Linear Unbiased Predictors, are not simply the least squares estimates. They are attenuated or shrunk towards zero to reflect the fact that the individuals are assumed to come from a population. A shrinkage plot, Figure¬†3, shows the BLUPs from the model fit compared to the values without any shrinkage. If the BLUPs are similar to the unshrunk values then the more complicated model accounting for individual differences is supported. If the BLUPs are strongly shrunk towards zero then the additional complexity in the model to account for individual differences is not providing sufficient increase in fidelity to the data to warrant inclusion.\n\n\nCode\nshrinkageplot!(Figure(; resolution=(500, 500)), m1)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†3: Shrinkage plot of means of the random effects in model m1\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis plot could be drawn as shrinkageplot(m1). The reason for explicitly creating a Figure to be modified by shrinkageplot! is to control the resolution.\n\n\nThis plot shows an intermediate pattern. The random effects are somewhat shrunk toward the origin, a model simplification trend, but not completely shrunk - indicating that fidelity to the data is enhanced with these additional coefficients in the linear predictor.\nIf the shrinkage were primarily in one direction - for example, if the arrows from the unshrunk values to the shrunk values were mostly in the vertical direction - then we would get an indication that we could drop the random effect for slope and revert to a simpler model. This is not the case here.\nAs would be expected, the unshrunk values that are further from the origin tend to be shrunk more toward the origin. That is, the arrows that originate furthest from the origin are longer. However, that is not always the case. The arrow in the upper right corner, from S337, is relatively short. Examination of the panel for S337 in the data plot shows a strong linear trend, even though both the intercept and the slope are unusually large. The neighboring panels in the data plot, S330 and S331, have more variability around the least squares line and are subject to a greater amount of shrinkage in the model. (They correspond to the two arrows on the right hand side of the figure around -5 on the vertical scale.)\n\n\n5 Assessing variability by bootstrapping\nThe speed of fitting linear mixed-effects models using MixedModels.jl allows for using simulation-based approaches to inference instead of relying on approximate standard errors. A parametric bootstrap sample for model m is a collection of models of the same form as m fit to data values simulated from m. That is, we pretend that m and its parameter values are the true parameter values, simulate data from these values, and estimate parameters from the simulated data.\nSimulating and fitting a substantial number of model fits, 5000 in this case, takes only a few seconds, following which we extract a data frame of the parameter estimates and plot densities of some of these estimates.\n\nrng = Random.seed!(42)    # initialize a random number generator\nm1bstp = parametricbootstrap(rng, 5000, m1)\ntbl = m1bstp.tbl\n\nTable with 10 columns and 5000 rows:\n      obj      Œ≤1       Œ≤2       œÉ        œÉ1       œÉ2       œÅ1          ‚ãØ\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n 1  ‚îÇ 1717.29  260.712  9.84971  23.4092  15.3319  6.40281  -0.0259384  ‚ãØ\n 2  ‚îÇ 1744.06  262.253  12.3008  25.7047  16.3186  5.54689  0.552582    ‚ãØ\n 3  ‚îÇ 1714.16  253.149  12.8789  22.2753  25.4791  6.14442  0.0691521   ‚ãØ\n 4  ‚îÇ 1711.54  263.376  11.5798  23.3129  18.8019  4.65565  0.103406    ‚ãØ\n 5  ‚îÇ 1741.66  248.429  9.39446  25.4354  20.1417  5.27365  -0.163646   ‚ãØ\n 6  ‚îÇ 1754.81  256.794  8.02398  26.5087  10.6782  7.14165  0.335477    ‚ãØ\n 7  ‚îÇ 1777.73  253.388  8.83558  27.8623  17.8327  7.17389  0.00377749  ‚ãØ\n 8  ‚îÇ 1768.59  254.441  11.4479  27.4034  16.2486  6.67044  0.725364    ‚ãØ\n 9  ‚îÇ 1753.56  244.906  11.3423  25.6047  25.3602  5.98663  -0.171842   ‚ãØ\n 10 ‚îÇ 1722.61  257.088  9.18396  23.3386  24.9285  5.18008  0.181129    ‚ãØ\n 11 ‚îÇ 1738.16  251.262  11.6568  25.7822  17.6658  4.0725   0.258007    ‚ãØ\n 12 ‚îÇ 1747.76  258.302  12.8015  26.1085  19.24    5.06066  0.879685    ‚ãØ\n 13 ‚îÇ 1745.91  254.57   11.8062  24.8863  24.2512  6.14643  0.0126749   ‚ãØ\n 14 ‚îÇ 1738.8   251.179  10.3226  24.2672  23.7198  6.32641  0.368574    ‚ãØ\n 15 ‚îÇ 1724.76  238.603  11.5045  25.23    19.0264  3.64045  -0.346598   ‚ãØ\n 16 ‚îÇ 1777.7   254.133  8.26396  26.9845  26.3717  7.82834  -0.288783   ‚ãØ\n 17 ‚îÇ 1748.33  251.571  9.5294   26.2927  21.9613  4.31323  -0.150127   ‚ãØ\n ‚ãÆ  ‚îÇ    ‚ãÆ        ‚ãÆ        ‚ãÆ        ‚ãÆ        ‚ãÆ        ‚ãÆ         ‚ãÆ       ‚ã±\n\n\nAn empirical density plot of the estimates for the fixed-effects coefficients, Figure¬†4, shows the normal distribution, ‚Äúbell-curve‚Äù, shape as we might expect.\n\n\nCode\nbegin\n  f1 = Figure(; resolution=(1000, 400))\n  CairoMakie.density!(\n    Axis(f1[1, 1]; xlabel=\"Intercept [ms]\"), tbl.Œ≤1\n  )\n  CairoMakie.density!(\n    Axis(f1[1, 2]; xlabel=\"Coefficient of days [ms/day]\"),\n    tbl.Œ≤2\n  )\n  f1\nend\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†4: Empirical density plots of bootstrap replications of fixed-effects parameter estimates\n\n\n\n\nIt is also possible to create interval estimates of the parameters from the bootstrap replicates. We define the 1-Œ± shortestcovint to be the shortest interval that contains a proportion 1-Œ± (defaults to 95%) of the bootstrap estimates of the parameter.\n\nTable(shortestcovint(m1bstp))\n\nTable with 5 columns and 6 rows:\n     type  group     names              lower      upper\n   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n 1 ‚îÇ Œ≤     missing   (Intercept)        238.982    264.803\n 2 ‚îÇ Œ≤     missing   days               7.42345    13.1628\n 3 ‚îÇ œÉ     subj      (Intercept)        10.1286    33.3852\n 4 ‚îÇ œÉ     subj      days               2.95892    7.68717\n 5 ‚îÇ œÅ     subj      (Intercept), days  -0.404422  1.0\n 6 ‚îÇ œÉ     residual  missing            22.8401    28.626\n\n\nThe intervals look reasonable except that the upper end point of the interval for œÅ1, the correlation coefficient, is 1.0 . It turns out that the estimates of œÅ have a great deal of variability.\nBecause there are several values on the boundary (œÅ = 1.0) and a pulse like this is not handled well by a density plot, we plot this sample as a histogram, Figure¬†5.\n\n\nCode\nhist(\n  tbl.œÅ1;\n  bins=40,\n  axis=(; xlabel=\"Estimated correlation of the random effects\"),\n  figure=(; resolution=(500, 500)),\n)\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†5: Histogram of bootstrap replications of the within-subject correlation parameter\n\n\n\n\nFinally, density plots for the variance components (but on the scale of the standard deviation), Figure¬†6, show reasonable symmetry.\n\n\nCode\nbegin\n  f2 = Figure(; resolution=(1000, 300))\n  CairoMakie.density!(\n    Axis(f2[1, 1]; xlabel=\"Residual œÉ\"),\n    tbl.œÉ,\n  )\n  CairoMakie.density!(\n    Axis(f2[1, 2]; xlabel=\"subj-Intercept œÉ\"),\n    tbl.œÉ1,\n  )\n  CairoMakie.density!(\n    Axis(f2[1, 3]; xlabel=\"subj-slope œÉ\"),\n    tbl.œÉ2,\n  )\n  f2\nend\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†6: Empirical density plots of bootstrap replicates of standard deviation estimates\n\n\n\n\nThe estimates of the coefficients, Œ≤‚ÇÅ and Œ≤‚ÇÇ, are not highly correlated as shown in a scatterplot of the bootstrap estimates, Figure¬†7 .\n\nvcov(m1; corr=true)  # correlation estimate from the model\n\n2√ó2 Matrix{Float64}:\n  1.0       -0.137557\n -0.137557   1.0\n\n\n\n\nCode\nlet\n  scatter(\n    tbl.Œ≤1, tbl.Œ≤2,\n    color=(:blue, 0.20),\n    axis=(; xlabel=\"Intercept\", ylabel=\"Coefficient of days\"),\n    figure=(; resolution=(500, 500)),\n  )\n  contour!(kde((tbl.Œ≤1, tbl.Œ≤2)))\n  current_figure()\nend\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n‚îî @ Makie ~/.julia/packages/Makie/FUAHr/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nFigure¬†7: Scatter-plot of bootstrap replicates of fixed-effects estimates with contours\n\n\n\n\n\n\n6 References\n\n\nBalkin, T., Thome, D., Sing, H., Thomas, M., Redmond, D., Wesensten, N., Williams, J., Hall, S., & Belenky, G. (2000). Effects of sleep schedules on commercial motor vehicle driver performance (DOT-MC-00-133). Federal Motor Carrier Safety Administration. https://doi.org/10.21949/1503015.\n\n\nBelenky, G., Wesensten, N. J., Thorne, D. R., Thomas, M. L., Sing, H. C., Redmond, D. P., Russo, M. B., & Balkin, T. J. (2003). Patterns of performance degradation and restoration during sleep restriction and subsequent recovery: A sleep dose-response study. Journal of Sleep Research, 12(1), 1‚Äì12. https://doi.org/10.1046/j.1365-2869.2003.00337.x\n\n\nDanisch, S., & Krumbiegel, J. (2021). Makie.jl: Flexible high-performance data visualization for julia. Journal of Open Source Software, 6(65), 3349. https://doi.org/10.21105/joss.03349\n\n\nSarkar, D. (2008). Lattice: Mutivariate data visualization with r. Springer-Verlag GmbH. https://www.ebook.de/de/product/11429038/deepayan_sarkar_lattice.html\n\n\nThis page was rendered from git revision fb99b3c\n using Quarto 1.7.33.\n\n\n\n\n Back to top",
    "crumbs": [
      "Worked examples",
      "Analysis of the sleepstudy data"
    ]
  },
  {
    "objectID": "transformation.html",
    "href": "transformation.html",
    "title": "Transformations of the predictors and the response",
    "section": "",
    "text": "When dealing with categorical variables, the choice of contrast coding impacts the interpretation of the coefficients of the fitted model but does not impact the predictions made by the model nor its general goodness of it. If we apply linear transformations to our predictors, then we see a similar pattern for continuous variables.\nFor example, in a model with age (in years) as a predictor, the untransformed variable yields a model where the intercept corresponds to age = 0, i.e.¬†a newborn. For a typical experiment with young adult participants, this presents a few challenges in interpretation:\n\nnewborns are widely outside the range of the observed data, so it seems problematic prima facie to interpret the estimated results for a value so far outside the range of the observed data\nwe know that newborns and young adults are widely different and that the effect of age across childhood on most psychological and biological phenomena is not linear. For example, children do not grow at a constant rate from birth until adulthood.\n\nBeyond centering a variable so that the center reflects an interpretable hypothesis, we may also want to scale a variable to move towards more easily interpretable units. For example, it is common to express things in terms of standard deviations instead of raw units ‚Äì combined with centering, this yields \\(z\\)-scoring .\nIn addition to placing some variables on a more interpretable scale, \\(z\\)-scoring can be used across all continuous predictors to place them all on a single, common scale. The advantage to shared scale across all continuous predictors is that the magnitude of coefficient estimates are directly comparable. The disadvantage is that the natural units are lost, especially when the natural units are directly interpretable (e.g.¬†milliseconds, grams, etc.).\n\n\n\n\n\n\nNonlinear transformations\n\n\n\n\n\nThere are also other possible nonlinear transformation, such as the logarithm or various polynomials, but we will leave this alone. Nonlinear transformation change the predictions of the model (in addition to changing the interpretation of the associated coefficients) and should be appropriately motivated by the data and theory.\n\n\n\nIn other words, from an interpretability standpoint, many continuous variables require just as much attention to their ‚Äúcoding‚Äù as categorical variables do.\n\n\n\n\n\n\nScaling can also help numerical aspects of model fitting\n\n\n\n\n\nFrom a practical perspective, linear transformations of the predicots may also make model fitting easier. In an abstract mathematical sense, the scale of the variables does not matter, but computers and hence our software exist in a less idealized realm. In an intuitive sense, we can think of rounding error ‚Äì if we are dealing with quantities on widely different scales, then the quantities on the larger scale will tend to dominate the quantities on the smaller scale. This is why many guides on how to deal with convergence issues suggest scaling your variables.\n\n\n\nIn Julia, the package StandardizedPredictors.jl takes advantage of this parallel between linear transformations and contrast coding and allows you to specify centering, scaling and \\(z\\)-transformations as part of the contrast specification.\nWe‚Äôll also be using the Effects.jl package to demonstrate that these transformation do not change the model predictions.\n\nusing DataFrames\nusing Effects\nusing MixedModels\nusing StandardizedPredictors\nusing SMLP2026: dataset\n\n\nslp = fit(MixedModel, \n          @formula(reaction ~ 1 + days + (1 + days |subj)),\n          dataset(:sleepstudy))\n\n\nMinimizing 2    Time: 0:00:00 (89.41 ms/it)\n   objective: 1792.8615963994716\n\n\nMinimizing 72    Time: 0:00:00 ( 5.02 ms/it)\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\n\n\n(Intercept)\n251.4051\n6.6323\n37.91\n&lt;1e-99\n23.7807\n\n\ndays\n10.4673\n1.5022\n6.97\n&lt;1e-11\n5.7169\n\n\nResidual\n25.5918\n\n\n\n\n\n\n\n\n\n\ndays_centered = fit(MixedModel, \n                    @formula(reaction ~ 1 + days + (1 + days |subj)),\n                    dataset(:sleepstudy);\n                    contrasts=Dict(:days =&gt; Center()))\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\n\n\n(Intercept)\n298.5079\n8.7950\n33.94\n&lt;1e-99\n36.4259\n\n\ndays(centered: 4.5)\n10.4673\n1.5022\n6.97\n&lt;1e-11\n5.7168\n\n\nResidual\n25.5918\n\n\n\n\n\n\n\n\n\nIf we look at the log-likelihood, AIC, BIC, etc. of these two models, we see that they are the same:\n\nmods = [slp, days_centered]\nDataFrame(; model=[\"original\", \"centered\"], \n          AIC=aic.(mods),\n          AICc=aicc.(mods),\n          BIC=bic.(mods), \n          logLik=loglikelihood.(mods))\n\n2√ó5 DataFrame\n\n\n\nRow\nmodel\nAIC\nAICc\nBIC\nlogLik\n\n\n\nString\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\noriginal\n1763.94\n1764.42\n1783.1\n-875.97\n\n\n2\ncentered\n1763.94\n1764.42\n1783.1\n-875.97\n\n\n\n\n\n\nWe can also see that models make identical predictions. The Effects package will compute predictions and estimated errors at a predefined grid. For more complicated models, we can also use the package to compute ‚Äútypical‚Äù values, such as the mean, median or mode, for variables that we wish to ignore. We don‚Äôt need to worry about that right now, since we only have one non-intercept predictor.\n\n# a fully crossed grid is computed from the elements of `design`.\n# this is similar to how `expand.grid` works in R.\ndesign = Dict(:days =&gt; [1, 4, 9])\neffects(design, slp; level=0.95)\n\n3√ó5 DataFrame\n\n\n\nRow\ndays\nreaction\nerr\nlower\nupper\n\n\n\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n1\n261.872\n6.59569\n248.945\n274.8\n\n\n2\n4\n293.274\n8.3145\n276.978\n309.57\n\n\n3\n9\n345.611\n14.2166\n317.747\n373.475\n\n\n\n\n\n\n\neffects(design, days_centered; level=0.95)\n\n3√ó5 DataFrame\n\n\n\nRow\ndays\nreaction\nerr\nlower\nupper\n\n\n\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n1\n261.872\n6.5957\n248.945\n274.8\n\n\n2\n4\n293.274\n8.31455\n276.978\n309.57\n\n\n3\n9\n345.611\n14.2167\n317.746\n373.475\n\n\n\n\n\n\nIf this sounds like effects or emmeans in R, that‚Äôs because there is a large overlap.",
    "crumbs": [
      "Contrast coding and transformations",
      "Transformations of the predictors and the response"
    ]
  },
  {
    "objectID": "transformation.html#predictors",
    "href": "transformation.html#predictors",
    "title": "Transformations of the predictors and the response",
    "section": "",
    "text": "When dealing with categorical variables, the choice of contrast coding impacts the interpretation of the coefficients of the fitted model but does not impact the predictions made by the model nor its general goodness of it. If we apply linear transformations to our predictors, then we see a similar pattern for continuous variables.\nFor example, in a model with age (in years) as a predictor, the untransformed variable yields a model where the intercept corresponds to age = 0, i.e.¬†a newborn. For a typical experiment with young adult participants, this presents a few challenges in interpretation:\n\nnewborns are widely outside the range of the observed data, so it seems problematic prima facie to interpret the estimated results for a value so far outside the range of the observed data\nwe know that newborns and young adults are widely different and that the effect of age across childhood on most psychological and biological phenomena is not linear. For example, children do not grow at a constant rate from birth until adulthood.\n\nBeyond centering a variable so that the center reflects an interpretable hypothesis, we may also want to scale a variable to move towards more easily interpretable units. For example, it is common to express things in terms of standard deviations instead of raw units ‚Äì combined with centering, this yields \\(z\\)-scoring .\nIn addition to placing some variables on a more interpretable scale, \\(z\\)-scoring can be used across all continuous predictors to place them all on a single, common scale. The advantage to shared scale across all continuous predictors is that the magnitude of coefficient estimates are directly comparable. The disadvantage is that the natural units are lost, especially when the natural units are directly interpretable (e.g.¬†milliseconds, grams, etc.).\n\n\n\n\n\n\nNonlinear transformations\n\n\n\n\n\nThere are also other possible nonlinear transformation, such as the logarithm or various polynomials, but we will leave this alone. Nonlinear transformation change the predictions of the model (in addition to changing the interpretation of the associated coefficients) and should be appropriately motivated by the data and theory.\n\n\n\nIn other words, from an interpretability standpoint, many continuous variables require just as much attention to their ‚Äúcoding‚Äù as categorical variables do.\n\n\n\n\n\n\nScaling can also help numerical aspects of model fitting\n\n\n\n\n\nFrom a practical perspective, linear transformations of the predicots may also make model fitting easier. In an abstract mathematical sense, the scale of the variables does not matter, but computers and hence our software exist in a less idealized realm. In an intuitive sense, we can think of rounding error ‚Äì if we are dealing with quantities on widely different scales, then the quantities on the larger scale will tend to dominate the quantities on the smaller scale. This is why many guides on how to deal with convergence issues suggest scaling your variables.\n\n\n\nIn Julia, the package StandardizedPredictors.jl takes advantage of this parallel between linear transformations and contrast coding and allows you to specify centering, scaling and \\(z\\)-transformations as part of the contrast specification.\nWe‚Äôll also be using the Effects.jl package to demonstrate that these transformation do not change the model predictions.\n\nusing DataFrames\nusing Effects\nusing MixedModels\nusing StandardizedPredictors\nusing SMLP2026: dataset\n\n\nslp = fit(MixedModel, \n          @formula(reaction ~ 1 + days + (1 + days |subj)),\n          dataset(:sleepstudy))\n\n\nMinimizing 2    Time: 0:00:00 (89.41 ms/it)\n   objective: 1792.8615963994716\n\n\nMinimizing 72    Time: 0:00:00 ( 5.02 ms/it)\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\n\n\n(Intercept)\n251.4051\n6.6323\n37.91\n&lt;1e-99\n23.7807\n\n\ndays\n10.4673\n1.5022\n6.97\n&lt;1e-11\n5.7169\n\n\nResidual\n25.5918\n\n\n\n\n\n\n\n\n\n\ndays_centered = fit(MixedModel, \n                    @formula(reaction ~ 1 + days + (1 + days |subj)),\n                    dataset(:sleepstudy);\n                    contrasts=Dict(:days =&gt; Center()))\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\n\n\n(Intercept)\n298.5079\n8.7950\n33.94\n&lt;1e-99\n36.4259\n\n\ndays(centered: 4.5)\n10.4673\n1.5022\n6.97\n&lt;1e-11\n5.7168\n\n\nResidual\n25.5918\n\n\n\n\n\n\n\n\n\nIf we look at the log-likelihood, AIC, BIC, etc. of these two models, we see that they are the same:\n\nmods = [slp, days_centered]\nDataFrame(; model=[\"original\", \"centered\"], \n          AIC=aic.(mods),\n          AICc=aicc.(mods),\n          BIC=bic.(mods), \n          logLik=loglikelihood.(mods))\n\n2√ó5 DataFrame\n\n\n\nRow\nmodel\nAIC\nAICc\nBIC\nlogLik\n\n\n\nString\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\noriginal\n1763.94\n1764.42\n1783.1\n-875.97\n\n\n2\ncentered\n1763.94\n1764.42\n1783.1\n-875.97\n\n\n\n\n\n\nWe can also see that models make identical predictions. The Effects package will compute predictions and estimated errors at a predefined grid. For more complicated models, we can also use the package to compute ‚Äútypical‚Äù values, such as the mean, median or mode, for variables that we wish to ignore. We don‚Äôt need to worry about that right now, since we only have one non-intercept predictor.\n\n# a fully crossed grid is computed from the elements of `design`.\n# this is similar to how `expand.grid` works in R.\ndesign = Dict(:days =&gt; [1, 4, 9])\neffects(design, slp; level=0.95)\n\n3√ó5 DataFrame\n\n\n\nRow\ndays\nreaction\nerr\nlower\nupper\n\n\n\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n1\n261.872\n6.59569\n248.945\n274.8\n\n\n2\n4\n293.274\n8.3145\n276.978\n309.57\n\n\n3\n9\n345.611\n14.2166\n317.747\n373.475\n\n\n\n\n\n\n\neffects(design, days_centered; level=0.95)\n\n3√ó5 DataFrame\n\n\n\nRow\ndays\nreaction\nerr\nlower\nupper\n\n\n\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n1\n261.872\n6.5957\n248.945\n274.8\n\n\n2\n4\n293.274\n8.31455\n276.978\n309.57\n\n\n3\n9\n345.611\n14.2167\n317.746\n373.475\n\n\n\n\n\n\nIf this sounds like effects or emmeans in R, that‚Äôs because there is a large overlap.",
    "crumbs": [
      "Contrast coding and transformations",
      "Transformations of the predictors and the response"
    ]
  },
  {
    "objectID": "transformation.html#response",
    "href": "transformation.html#response",
    "title": "Transformations of the predictors and the response",
    "section": "2 Response",
    "text": "2 Response\nIn addition to transforming the predictors, we can also consider transforming the response (dependent variable). There are many different common possibilities ‚Äì the log, the inverse/reciprocal, or even the square root ‚Äì and it can be difficult to choose an appropriate one. For non-negative response (e.g., reaction time in many experiences), Box & Cox (1964) figured out a generalization that subsumes all of these possibilities:\n\\[\n\\begin{cases}\n\\frac{y^{\\lambda} - 1}{\\lambda} &\\quad \\lambda \\neq 0 \\\\\n\\log y &\\quad \\lambda = 0\n\\end{cases}\n\\]\nOur task is thus finding the appropriate \\(\\lambda\\) such that the conditional distribution is as normal as possible. In other words, we need to find \\(\\lambda\\) that results in the residuals are as normal as possible. I‚Äôve emphasized conditional distribution and residuals because that‚Äôs where the normality assumption actually lies in the linear (mixed) model. The assumption is not that the response y, i.e.¬†the uncondidtional distribution, is normally distributed, but rather that the residuals are normally distributed. In other words, we can only check the quality of a given \\(\\lambda\\) by fitting a model to the transformed response. Fortunately, BoxCox.jl makes this easy.\nThe fit function takes two arguments: - the transformation to be fit (i.e.¬†BoxCoxTransformation) - the model fit to the original data\n\nusing BoxCox\nbc = fit(BoxCoxTransformation, slp)\n\nBox-Cox transformation\n\nestimated Œª: -1.0738\nresultant transformation:\n\n y^-1.1 - 1\n------------\n    -1.1\n\n\n\nFor large models, fitting the BoxCoxTransformation can take a while because a mixed model must be repeatedly fit after each intermediate transformation.\n\nAlthough we receive a single ‚Äúbest‚Äù value (approximately -1.0747) from the fitting process, it is worthwhile to look at the profile likelihood plot for the transformation:\n\n# we need a plotting backend loaded before we can use plotting functionality\n# from BoxCox.jl\nusing CairoMakie\nboxcoxplot(bc; conf_level=0.95)\n\n\nMinimizing 2    Time: 0:00:00 (60.14 ms/it)\n   objective: -279.62305332787685\n\n\nMinimizing 46    Time: 0:00:00 ( 2.63 ms/it)\n\n\n\n\n\n\n\nHere we see that -1 is nearly as good. Moreover, time\\(^{-1}\\) has a natural interpretation as speed. In other words, we can model reaction speed instead of reaction time. Then instead of seeing whether participants take longer to respond with each passing day, we can see whether their speed increases or decreases. In both cases, we‚Äôre looking at whether they respond faster or slower and even the terminology fast and slow suggests that speed is easily interpretable.\nIf we recall the definition of the Box-Cox transformation from above: \\[\n\\begin{cases}\n\\frac{y^{\\lambda} - 1}{\\lambda} &\\quad \\lambda \\neq 0 \\\\\n\\log y &\\quad \\lambda = 0\n\\end{cases}\n\\]\nthen we see that there is a normalizing denominator that flips the sign when \\lambda &lt; 0. If we use the full Box-Cox formula, then the sign of the effect in our transformed and untransformed model remains the same. While useful at times, speed has a natural interpretation and so we instead use the power relation, which is the actual key component, without normalization.\nBecause reaction is stored in milliseconds, we use 1000 / reaction instead of 1 / reaction so that our speed units are responses per second.\n\nmodel_bc = fit(MixedModel,\n               @formula(1000 / reaction ~ 1 + days + (1 + days | subj)),\n                dataset(:sleepstudy))\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\n\n\n(Intercept)\n3.9658\n0.1056\n37.55\n&lt;1e-99\n0.4190\n\n\ndays\n-0.1110\n0.0151\n-7.37\n&lt;1e-12\n0.0566\n\n\nResidual\n0.2698\n\n\n\n\n\n\n\n\n\nFor our original model on the untransformed scale, the intercept was approximately 250, which means that the average response time was about 250 milliseconds. For the model on the speed scale, we have an intercept about approximately 4, which means that the average response speed is about 4 responses per second, which implies that the the average response time is 250 milliseconds. In other words, our new results are compatible with our previous estimates.\nThis example also makes something else clear: much like transformations of the predictors, transforming the response changes the hypothesis being tested. While it is relatively easy to re-formulate hypothesis about reaction time into hypotheses about speed, it can be harder to re-formulate other hypotheses. For example, a log transformation of the response changes the hypotheses on the original scale from additive effects to multiplicative effects. As a very simple example, consider two observations y1 = 100 and y2 = 1000. On the original scale, there y2 = 10 * y1. But on the \\(\\log_{10}\\) scale, log10(y2) = 1 + log10(y1). In other words: I recommend keeping interpretability of the model in mind before blindly chasing perfectly fulfilling all model assumptions.\nThere are two other little tricks that BoxCox.jl has to offer. First, the fitted transformation will work just like a function:\n\nbc(1000)\n\n0.9307537052448639\n\n\n\nbc.(response(slp))\n\n180-element Vector{Float64}:\n 0.9288294177544062\n 0.9289235675939608\n 0.9288426079448279\n 0.929420513604405\n 0.9296214300314919\n 0.929873433959407\n 0.9297416256989369\n 0.9292004915075488\n 0.9299304268657653\n 0.930043978567257\n ‚ãÆ\n 0.9290618632092912\n 0.9292572164229206\n 0.929349710215327\n 0.929176972972555\n 0.9294708292520951\n 0.929499642377606\n 0.9295491739285928\n 0.9296818349144634\n 0.9296576814179915\n\n\nSecond, the decades since the publication of Box & Cox (1964) have seen many proposed extensions to handle that that may not be strictly positive. One such proposal from Yeo & Johnson (2000) is also implemented in BoxCox.jl. The definition of the transformation is:\n\\[\n\\begin{cases} ((y_+1)^\\lambda-1)/\\lambda                      & \\text{if }\\lambda \\neq 0, y \\geq 0 \\\\\n               \\log(y_i + 1)                                   & \\text{if }\\lambda =     0, y \\geq 0 \\\\\n               -((-y_ + 1)^{(2-\\lambda)} - 1) / (2 - \\lambda) &  \\text{if }\\lambda \\neq 2, y &lt;     0 \\\\\n               -\\log(-y_ + 1)                                 &  \\text{if }\\lambda =     2, y &lt;     0\n\\end{cases}\n\\]\nand we can fit it in BoxCox.jl with\n\nyj = fit(YeoJohnsonTransformation, slp)\n\nYeo-Johnson transformation\n\nestimated Œª: -1.0753\np-value: &lt;1e-09\n\nresultant transformation:\n\nFor y ‚â• 0,\n\n (y + 1)^-1.1 - 1\n------------------\n       -1.1\n\n\nFor y &lt; 0:\n\n -((-y + 1)^(2 - -1.1) - 1)\n----------------------------\n         (2 - -1.1)\n\n\n\nf = boxcoxplot(yj; conf_level=0.95)\nf[0, :] = Label(f, \"Yeo-Johnson\"; tellwidth=false)\nf\n\n\n\n\n\n\n\nThis page was rendered from git revision fb99b3c\n using Quarto 1.7.33.",
    "crumbs": [
      "Contrast coding and transformations",
      "Transformations of the predictors and the response"
    ]
  }
]